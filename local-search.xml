<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>深度学习炼丹踩坑</title>
    <link href="/2022/07/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%82%BC%E4%B8%B9%E8%B8%A9%E5%9D%91/"/>
    <url>/2022/07/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%82%BC%E4%B8%B9%E8%B8%A9%E5%9D%91/</url>
    
    <content type="html"><![CDATA[<h2 id="如题"><a href="#如题" class="headerlink" title="如题"></a>如题</h2><span id="more"></span><h4 id="GPU显存"><a href="#GPU显存" class="headerlink" title="GPU显存"></a>GPU显存</h4><p>使用GPU进行炼丹的时候，发现了些ghost进程，如图中所示<br><img src="/img/article/DL1.jpg"><br>这些不仅进程占用了大量的GPU显存，而且使用 ps 查询该进程的时候竟然都查不到。<br>好在互联网上查到了解决的办法</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">sudo fuser -v <span class="hljs-regexp">/dev/</span>nvidia*<br></code></pre></td></tr></table></figure><p>使用该命令查询出所有在GPU显卡上的进程，然后kill掉已经不存在的进程即可释放显存。</p><h4 id="learning-rate"><a href="#learning-rate" class="headerlink" title="learning rate"></a>learning rate</h4><p>在网上找了张图<br><img src="/img/article/DL2.jpg"><br>LR在优化算法中更新网络权重的幅度大小，同时也是深度学习中需要调的第一大参数了吧。一般学习率从0.1或0.01开始尝试，如果太大loss会很震荡甚至直接NA，太小了收敛就会很慢。学习率一般要随着训练进行衰减。衰减系数设0.1，0.3，0.5均可，衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后自动进行衰减。当然了也可以一个学习率走到底（不衰减）。</p><h4 id="batch-size"><a href="#batch-size" class="headerlink" title="batch size"></a>batch size</h4><p>深度学习中一直都有一些参数有着玄学般的存在，比如 random seed = 42，batch size 设置为 2 的倍数，或者 8 的倍数等等。但是最近有大佬对 batch size 这个玄学问题做了实验，结果呢就是也不用非要按 2 的倍数这样去设置。既然玄学被打破那就随心好了，在GPU显存允许的范围内可以尽量设置大点，这样可以节约不少时间。当然了也不能盲目的使用一个大的batch size，如果模型效果不好，该改还是要改的，毕竟玄学终究是玄学。</p><h4 id="optimizer"><a href="#optimizer" class="headerlink" title="optimizer"></a>optimizer</h4><p>目前的优化器有 Adagrad, Adadelta, RMSprop, Adam等，这么多该怎么选呢，整体来讲，Adam是最好的选择，也就是像我这样的小白直接无脑 Adam 就好了。大佬请随意。</p><p>当然了还有一些其他的参数，比如 weight decay, epoch, drop out 等等，epoch 主要就是训练的轮数，只要没过拟合，loss没收敛，那epoch就只能往大了改。如果过拟合了就可以增加 weight decay, drop out的参数，或者使用其他的正则化或者减少过拟合的方法。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>neo4j中节点及关系的查询、删除等</title>
    <link href="/2022/03/01/neo4j%E4%B8%AD%E9%87%8D%E5%A4%8D%E8%8A%82%E7%82%B9%E5%8F%8A%E5%85%B3%E7%B3%BB%E7%9A%84%E6%9F%A5%E6%89%BE%E3%80%81%E5%88%A0%E9%99%A4/"/>
    <url>/2022/03/01/neo4j%E4%B8%AD%E9%87%8D%E5%A4%8D%E8%8A%82%E7%82%B9%E5%8F%8A%E5%85%B3%E7%B3%BB%E7%9A%84%E6%9F%A5%E6%89%BE%E3%80%81%E5%88%A0%E9%99%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="Neo4j-查询、删除"><a href="#Neo4j-查询、删除" class="headerlink" title="Neo4j 查询、删除"></a>Neo4j 查询、删除</h2><span id="more"></span><figure class="highlight hsp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs hsp">MATCH (n)-[r]-(m) <span class="hljs-keyword">RETURN</span> * <span class="hljs-keyword">LIMIT</span> <span class="hljs-number">100</span>  等价于 MATCH p=()--() <span class="hljs-keyword">RETURN</span> p <span class="hljs-keyword">LIMIT</span> <span class="hljs-number">100</span><br></code></pre></td></tr></table></figure><p>n和m代表节点，r代表relationship关系<br>(n)-[r]-(m)代表一种模式，即n节点和m节点由r关系联系起来<br>MATCH (n)-[r]-(m) RETURN的意思是查询并返回所有与(n)-[r]-(m)模式匹配的节点和关系。<br>* LIMIT 100是限制条件，意思是仅需返回前100个匹配到的结果（节点和关系）。</p><h5 id="查询重复节点"><a href="#查询重复节点" class="headerlink" title="查询重复节点"></a>查询重复节点</h5><p>可以分为以下步骤解决：</p><ul><li><p> 1、先查看下某个标签下的节点总数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">match</span> (n:PERSON) <span class="hljs-keyword">return</span> <span class="hljs-built_in">count</span>(n)<br></code></pre></td></tr></table></figure></li><li><p> 2、比较总数和去重后总数，可判断是否存在相同name的节点</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">match</span> (n:PERSON) <span class="hljs-keyword">return</span> <span class="hljs-built_in">count</span>(<span class="hljs-keyword">distinct</span> n.name)<br></code></pre></td></tr></table></figure></li></ul><p>若执行步骤1和步骤2，得到的数量相同，则证明没有重复的节点</p><ul><li><p> 3、若重复节点较少，可通过设置id条件进行删除</p></li><li><p> 4、若重复节点较多，可用apoc来进行操作(需要安装apoc插件，地址：<a href="https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases">https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases</a>)</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">MATCH (n:Tag) <br><span class="hljs-keyword">WITH</span> n.name <span class="hljs-keyword">AS</span> <span class="hljs-type">name</span>, <br>    COLLECT(n) <span class="hljs-keyword">AS</span> nodelist, <br>    COUNT(*) <span class="hljs-keyword">AS</span> count <br><span class="hljs-keyword">WHERE</span> count &gt; <span class="hljs-number">1</span> <br><span class="hljs-keyword">CALL</span> apoc.refactor.mergeNodes(nodelist) YIELD node <br><span class="hljs-keyword">RETURN</span> node<br></code></pre></td></tr></table></figure></li></ul><h5 id="查询重复关系"><a href="#查询重复关系" class="headerlink" title="查询重复关系"></a>查询重复关系</h5><ul><li><p>查询</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">MATCH (<span class="hljs-keyword">a</span>)-[r]-(b) <br><span class="hljs-keyword">with</span> count(type(r)) <span class="hljs-keyword">as</span> <span class="hljs-built_in">num</span>,<span class="hljs-keyword">a</span>,b <br>where <span class="hljs-built_in">num</span> &gt;= <span class="hljs-number">2</span> <span class="hljs-literal">return</span> <span class="hljs-built_in">num</span>,<span class="hljs-keyword">a</span>,b LIMIT <span class="hljs-number">25</span><br></code></pre></td></tr></table></figure></li><li><p>删除</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-selector-tag">MATCH</span> (a)<span class="hljs-selector-tag">-</span><span class="hljs-selector-attr">[r]</span><span class="hljs-selector-tag">-</span>(b) <br><span class="hljs-selector-tag">WITH</span> <span class="hljs-selector-tag">a</span>, <span class="hljs-selector-tag">b</span>, <span class="hljs-selector-tag">TAIL</span> (COLLECT (r)) <span class="hljs-selector-tag">as</span> <span class="hljs-selector-tag">rr</span> <br><span class="hljs-selector-tag">WHERE</span> <span class="hljs-selector-tag">size</span>(rr)&gt;<span class="hljs-selector-tag">0</span> <br><span class="hljs-selector-tag">FOREACH</span> (r IN rr | DELETE r)<br></code></pre></td></tr></table></figure></li><li><p>修改节点的 label ，可以先新加 label ，再删除旧的的label</p><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs oxygene">match (n:<span class="hljs-keyword">old</span>) <span class="hljs-keyword">set</span> n:<span class="hljs-keyword">NEW</span> <span class="hljs-keyword">remove</span> n:<span class="hljs-keyword">old</span><br></code></pre></td></tr></table></figure></li><li><p>修改关系的 label</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-selector-tag">MATCH</span> (<span class="hljs-attribute">n</span>:drug)<span class="hljs-selector-tag">-</span><span class="hljs-selector-attr">[r:indication]</span><span class="hljs-selector-tag">-</span>(<span class="hljs-attribute">m</span>:disease)<br><span class="hljs-selector-tag">CREATE</span> (n)<span class="hljs-selector-tag">-</span><span class="hljs-selector-attr">[r2:drug_disease]</span><span class="hljs-selector-tag">-</span>&gt;(m)<br><span class="hljs-selector-tag">DELETE</span> <span class="hljs-selector-tag">r</span><br></code></pre></td></tr></table></figure></li><li><p>删除某个节点及对应的关系</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">MATCH (n:Disease)-[r]-() <span class="hljs-keyword">where</span> n.name=&quot;&#123;f&#125;&quot; <span class="hljs-keyword">DELETE</span> n,r<br></code></pre></td></tr></table></figure></li><li><p>节点添加属性</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">MATCH (n &#123; <span class="hljs-type">name</span>: <span class="hljs-string">&#x27;Andres&#x27;</span> &#125;) <span class="hljs-keyword">SET</span> n.surname = <span class="hljs-string">&#x27;Taylor&#x27;</span> <span class="hljs-keyword">RETURN</span> n.name, n.surname<br></code></pre></td></tr></table></figure></li><li><p>关系添加属性</p><figure class="highlight sas"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sas">MATCH p=()-[r:`尾部`]-&gt;() <span class="hljs-meta">SET</span> r=&#123;<span class="hljs-meta">title</span>:<span class="hljs-string">&quot;尾部&quot;</span>&#125; <span class="hljs-meta">RETURN</span> p<br></code></pre></td></tr></table></figure></li></ul><h5 id="查询某节点深度关系"><a href="#查询某节点深度关系" class="headerlink" title="查询某节点深度关系"></a>查询某节点深度关系</h5><p>变长路径的模式</p><ul><li><p>(a)-[*2]-&gt;(b) : 表示路径长度为2， 起始节点是a，终止节点是b；</p></li><li><p>(a)-[*3..5]-&gt;(b) : 表示路径长度最小为2，最大为5， 起始节点是a，终止节点是b；</p></li><li><p>(a)-[*..5]-&gt;(b) : 表示路径长度最大为5， 起始节点是a，终止节点是b；</p></li><li><p>(a)-[*3..]-&gt;(b) : 表示路径长度最小为3， 起始节点是a，终止节点是b；</p></li><li><p>(a)-[*]-&gt;(b) : 表示不限制路径长度， 起始节点是a，终止节点是b；</p></li></ul><ul><li><p>所有深度</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">match(n:TargetGene&#123;name:<span class="hljs-string">&#x27;TOP2A&#x27;</span>&#125;)-[r*]-(m) <span class="hljs-built_in">return</span> n,r,m <span class="hljs-built_in">limit</span> 100  <span class="hljs-comment">## 某节点的所有关系</span><br>match p=()--() <span class="hljs-built_in">return</span> p <span class="hljs-built_in">limit</span> 25   <span class="hljs-comment">## 所有节点所有关系</span><br></code></pre></td></tr></table></figure></li><li><p>一级深度（某节点关联的一级所有节点及关系）</p><figure class="highlight rsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs rsl"><span class="hljs-built_in">match</span> p=(n:TargetGene&#123;name:<span class="hljs-string">&#x27;TOP2A&#x27;</span>&#125;) -[]-() <span class="hljs-keyword">return</span> p    <span class="hljs-meta">## [] 前后的 - 代表关系的方向，如果没有箭头表示所有方向(中括号[]可省略)</span><br><span class="hljs-built_in">match</span>(n:TargetGene&#123;name:<span class="hljs-string">&#x27;TOP2A&#x27;</span>&#125;)-[r]-(m) <span class="hljs-keyword">return</span> n,r,m  <span class="hljs-meta">## 与上条命令效果相同</span><br><br><span class="hljs-built_in">match</span> p=(n:TargetGene&#123;name:<span class="hljs-string">&#x27;TOP2A&#x27;</span>&#125;) -[]-&gt;() <span class="hljs-keyword">return</span> p   <span class="hljs-meta">##  表示TOP2A指向外部的所有节点及关系</span><br><span class="hljs-built_in">match</span> p=(n:TargetGene&#123;name:<span class="hljs-string">&#x27;TOP2A&#x27;</span>&#125;) &lt;-[]-() <span class="hljs-keyword">return</span> p   <span class="hljs-meta">##  表示指向TOP2A的所有节点及关系</span><br></code></pre></td></tr></table></figure></li><li><p>指定深度</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">match</span><span class="hljs-params">(n:TargetGene&#123;name:<span class="hljs-string">&#x27;TOP2A&#x27;</span>&#125;)</span></span>-<span class="hljs-selector-attr">[r*..3]</span>-(m) return n,r,m limit <span class="hljs-number">100</span><br></code></pre></td></tr></table></figure></li></ul><h5 id="查询某两个节点是否有关系"><a href="#查询某两个节点是否有关系" class="headerlink" title="查询某两个节点是否有关系"></a>查询某两个节点是否有关系</h5><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">match (:节点标签<span class="hljs-number">1</span>&#123;<span class="hljs-type">name</span>:<span class="hljs-string">&#x27;xx&#x27;</span>&#125;)-[r]-(:节点标签<span class="hljs-number">2</span>&#123;<span class="hljs-type">name</span>:<span class="hljs-string">&#x27;xx&#x27;</span>&#125;) <span class="hljs-keyword">return</span> r,<span class="hljs-keyword">type</span>(r)<br></code></pre></td></tr></table></figure><ul><li><p>查询深度关系（r*3表示3级）</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">match p=(n:TargetGene&#123;<span class="hljs-type">name</span>:<span class="hljs-string">&#x27;MIR7-1&#x27;</span>&#125;)-[r*<span class="hljs-number">3</span>]-(m:Disease&#123;<span class="hljs-type">name</span>:<span class="hljs-string">&#x27;breast cancer&#x27;</span>&#125;) <span class="hljs-keyword">return</span> p<br></code></pre></td></tr></table></figure></li><li><p>使用where进行查询</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs applescript">match p=(n:TargetGene)-[r*<span class="hljs-number">3</span>]-(m:Disease) <span class="hljs-keyword">where</span> n.<span class="hljs-built_in">name</span>=<span class="hljs-string">&quot;MIR7-1&quot;</span> <span class="hljs-keyword">and</span> m.<span class="hljs-built_in">name</span>=<span class="hljs-string">&quot;breast cancer&quot;</span> <span class="hljs-literal">return</span> p<br></code></pre></td></tr></table></figure></li><li><p>使用正则忽略大小写  <del>“(?i)strings”  其语法为 : =</del> “regexp”  区分大小写的模糊匹配; =~”(?i)regexp” 不区分大小写的模糊匹配</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs applescript">match p=(n:TargetGene)-[r*<span class="hljs-number">3</span>]-(m:Disease) <span class="hljs-keyword">where</span> n.<span class="hljs-built_in">name</span>=~<span class="hljs-string">&quot;(?i)mir7-1&quot;</span> <span class="hljs-keyword">and</span> m.<span class="hljs-built_in">name</span>=<span class="hljs-string">&quot;breast cancer&quot;</span> <span class="hljs-literal">return</span> p<br></code></pre></td></tr></table></figure></li><li><p>最短路径查询<br>方法一：</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs lisp">match p=shortestPath((<span class="hljs-name">n</span><span class="hljs-symbol">:TargetGene</span>&#123;name:<span class="hljs-string">&quot;MIR7-1&quot;</span>&#125;)-[r*]-(<span class="hljs-name">m</span><span class="hljs-symbol">:Disease</span>&#123;name:<span class="hljs-string">&quot;breast cancer&quot;</span>&#125;))  return p<br></code></pre></td></tr></table></figure><p>方法二：<br>加入了忽略大小写</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs applescript">match (n:TargetGene),(m:Disease) <span class="hljs-keyword">where</span> n.<span class="hljs-built_in">name</span>=~<span class="hljs-string">&quot;(?i)mir7-1&quot;</span> <span class="hljs-keyword">and</span> m.<span class="hljs-built_in">name</span>=<span class="hljs-string">&quot;breast cancer&quot;</span><br><span class="hljs-keyword">with</span> n,m match p=shortestpath((n)-[r*]-(m)) <span class="hljs-literal">return</span> p;<br></code></pre></td></tr></table></figure></li><li><p>查询多个药物与某个疾病的最短路径</p><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs coq"><span class="hljs-keyword">match</span> (m:Drug), (n:Disease) <span class="hljs-keyword">where</span> m.name=~&#x27;(?i)DOXYLAMINE|<span class="hljs-type">FESOTERODINE</span>|<span class="hljs-type">TROPICAMIDE</span>|<span class="hljs-type">Dexmedetomidine</span>&#x27; and n.name=~&#x27;(?i)Systemic sclerosis&#x27; <span class="hljs-built_in">with</span> n,m <span class="hljs-keyword">match</span> p=shortestpath((n)-[r*]-(m)) <span class="hljs-keyword">return</span> p<br></code></pre></td></tr></table></figure></li></ul><h5 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h5><ul><li><p>通过 name 属性  删除这一个节点</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs excel"><span class="hljs-built_in">MATCH</span> (<span class="hljs-symbol">n:TE</span>ST1&#123;<span class="hljs-built_in">na</span><span class="hljs-symbol">me:</span>&#x27;temp&#x27;&#125;) delete <span class="hljs-built_in">n</span><br></code></pre></td></tr></table></figure></li><li><p>通过 id 属性 删除这一个节点</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">MATCH (<span class="hljs-built_in">r</span>) <span class="hljs-built_in">WHERE</span> id(<span class="hljs-built_in">r</span>) = <span class="hljs-number">492</span> DELETE <span class="hljs-built_in">r</span><br></code></pre></td></tr></table></figure></li><li><p>删除一个节点及其所有的关系</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">MATCH (<span class="hljs-built_in">r</span>) <span class="hljs-built_in">WHERE</span> id(<span class="hljs-built_in">r</span>) = <span class="hljs-number">493</span> DETACH DELETE <span class="hljs-built_in">r</span><br></code></pre></td></tr></table></figure></li><li><p>删除所有节点和所有的关系</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">MATCH (<span class="hljs-built_in">r</span>) DETACH DELETE <span class="hljs-built_in">r</span><br></code></pre></td></tr></table></figure></li><li><p>删除一个标签中所有的节点</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-selector-tag">MATCH</span> (<span class="hljs-attribute">r</span>:Loc) <span class="hljs-selector-tag">DETACH</span> <span class="hljs-selector-tag">DELETE</span> <span class="hljs-selector-tag">r</span><br></code></pre></td></tr></table></figure></li><li><p>删除标签</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clean">:schema    ### 查询标签所对应的索引<br></code></pre></td></tr></table></figure></li></ul><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs n1ql"><span class="hljs-keyword">drop</span> <span class="hljs-keyword">index</span> 索引名字<br></code></pre></td></tr></table></figure><p>参考：<a href="https://python.iitter.com/other/191720.html">https://python.iitter.com/other/191720.html</a></p>]]></content>
    
    
    <categories>
      
      <category>Neo4j</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Neo4j</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cpp学习笔记03</title>
    <link href="/2022/03/01/cpp%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B003/"/>
    <url>/2022/03/01/cpp%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B003/</url>
    
    <content type="html"><![CDATA[<h2 id="如题"><a href="#如题" class="headerlink" title="如题"></a>如题</h2><span id="more"></span><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span><span class="hljs-meta-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span><span class="hljs-meta-string">&lt;string&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><br><span class="hljs-comment">//圆周长</span><br><span class="hljs-keyword">const</span> <span class="hljs-keyword">double</span> pi = <span class="hljs-number">3.14</span>;<br><br><br><span class="hljs-comment">// struct 默认权限为 公共 public</span><br><span class="hljs-comment">// class 默认权限为 私有 private</span><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">C1</span></span><br><span class="hljs-class">&#123;</span><br>    <span class="hljs-keyword">int</span> m1; <span class="hljs-comment">// 默认私有</span><br>&#125;;<br><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">C2</span></span><br><span class="hljs-class">&#123;</span><br>    <span class="hljs-keyword">int</span> m2; <span class="hljs-comment">// 默认公共 </span><br>&#125;;<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Circle</span></span><br><span class="hljs-class">&#123;</span><br>    <span class="hljs-comment">//访问权限：公共权限public、保护权限protected、私有权限private</span><br>    <span class="hljs-comment">//公共权限</span><br><span class="hljs-keyword">public</span>:   <span class="hljs-comment">//类内外都可访问</span><br>    <span class="hljs-comment">//类中属性和行为统一称为成员</span><br>    <span class="hljs-comment">// 属性  成员属性 成员变量</span><br>    <span class="hljs-keyword">int</span> r;<br><br>    <span class="hljs-comment">//行为  成员行为 成员函数</span><br>    <span class="hljs-function"><span class="hljs-keyword">double</span> <span class="hljs-title">calculate</span><span class="hljs-params">()</span></span><br><span class="hljs-function">    </span>&#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * pi * r;<br>    &#125;<br><br><span class="hljs-keyword">protected</span>:  <span class="hljs-comment">//类内可访问，类外不可    子类可访问</span><br>    <span class="hljs-keyword">int</span> r1;<br><br><br><span class="hljs-keyword">private</span>:   <span class="hljs-comment">// 类内可访问，类外不可   子类不可访问</span><br>    <span class="hljs-keyword">int</span> r2;<br>    <br>&#125;;<br><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">int</span> argc, <span class="hljs-keyword">char</span> <span class="hljs-keyword">const</span> *argv[])</span></span><br><span class="hljs-function"></span>&#123;<br>    Circle c1;<br>    c1.r = <span class="hljs-number">10</span>;<br><br>    cout &lt;&lt; c1.<span class="hljs-built_in">calculate</span>() &lt;&lt; endl;<br><br>    <br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>CPP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CPP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cpp学习笔记02</title>
    <link href="/2022/03/01/cpp%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B002/"/>
    <url>/2022/03/01/cpp%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B002/</url>
    
    <content type="html"><![CDATA[<h2 id="如题"><a href="#如题" class="headerlink" title="如题"></a>如题</h2><span id="more"></span><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span><span class="hljs-meta-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span><span class="hljs-meta-string">&lt;string&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><br><span class="hljs-comment">//值传递, 形参不会修饰实参</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">swap1</span><span class="hljs-params">(<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">int</span> tmp = a;<br>    a = b;<br>    b = tmp;<br><br>&#125;<br><br><span class="hljs-comment">//地址传递，形参会修饰实参</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">swap2</span><span class="hljs-params">(<span class="hljs-keyword">int</span> *a, <span class="hljs-keyword">int</span> *b)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">int</span> tmp = *a;<br>    *a = *b;<br>    *b = tmp;<br>&#125;<br><br><span class="hljs-comment">//常量引用，用来修饰形参，防止误操作</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">showValue</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> &amp;val)</span></span><br><span class="hljs-function"></span>&#123;<br>    cout &lt;&lt; val &lt;&lt; endl; <span class="hljs-comment">// val的值无法修改</span><br>&#125;<br><br><br><span class="hljs-comment">//引用的本质是指针常量</span><br><span class="hljs-comment">//引用传递，形参会修饰实参</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">swap3</span><span class="hljs-params">(<span class="hljs-keyword">int</span> &amp;a, <span class="hljs-keyword">int</span> &amp;b)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">int</span> tmp = a;<br>    a = b;<br>    b = tmp;<br>&#125;<br><br><span class="hljs-comment">// 不要返回局部变量的引用</span><br><span class="hljs-comment">// int&amp; test1()</span><br><span class="hljs-comment">// &#123;</span><br><span class="hljs-comment">//  int a = 10; // 局部变量存放在栈区</span><br><span class="hljs-comment">//  // return a; // 返回引用会带来隐患</span><br><span class="hljs-comment">// &#125;</span><br><br><span class="hljs-comment">// 函数的调用可以作为左值</span><br><span class="hljs-function"><span class="hljs-keyword">int</span>&amp; <span class="hljs-title">test2</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">static</span> <span class="hljs-keyword">int</span> a = <span class="hljs-number">10</span>; <span class="hljs-comment">// 静态变量存放在全局区</span><br>    <span class="hljs-keyword">return</span> a; <br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">int</span> argc, <span class="hljs-keyword">char</span> <span class="hljs-keyword">const</span> *argv[])</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-comment">//引用的基本使用</span><br>    <span class="hljs-comment">//作用：给变量起别名</span><br>    <span class="hljs-comment">//语法：数据类型 &amp;别名 = 原名</span><br>    <span class="hljs-keyword">int</span> a = <span class="hljs-number">10</span>;<br>    <span class="hljs-comment">// 创建引用</span><br>    <span class="hljs-keyword">int</span> &amp;b = a;   <span class="hljs-comment">// 引用初始化(必须)，初始化后不可改变，b的值也是10</span><br><br>    <span class="hljs-keyword">int</span> &amp;aa = <span class="hljs-built_in">test2</span>();<br>    <span class="hljs-built_in">test2</span>() = <span class="hljs-number">100</span>;       <span class="hljs-comment">// 函数的调用可以作为左值</span><br>    cout &lt;&lt; aa &lt;&lt; endl;<br><br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-meta">#include&lt;iostream&gt;</span><br><span class="hljs-meta">#include&lt;string&gt;</span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> <span class="hljs-title">std</span>;<br><br><span class="hljs-comment">//声明和实现只能有一个有默认参数,即如果函数声明有默认参数，函数实现就不能有默认参数，反之亦然</span><br><span class="hljs-comment">//函数声明</span><br><span class="hljs-function"><span class="hljs-built_in">int</span> <span class="hljs-title">func1</span>(<span class="hljs-params"><span class="hljs-built_in">int</span> a, <span class="hljs-built_in">int</span> b=<span class="hljs-number">10</span></span>)</span>;<br><br><span class="hljs-comment">//函数实现</span><br><span class="hljs-function"><span class="hljs-built_in">int</span> <span class="hljs-title">func1</span>(<span class="hljs-params"><span class="hljs-built_in">int</span> a, <span class="hljs-built_in">int</span> b</span>)</span><br>&#123;<br>    <span class="hljs-keyword">return</span> a + b<br>&#125;<br><br><span class="hljs-comment">// 占位参数</span><br><span class="hljs-function"><span class="hljs-built_in">int</span> <span class="hljs-title">func2</span>(<span class="hljs-params"><span class="hljs-built_in">int</span> a, <span class="hljs-built_in">int</span></span>)  <span class="hljs-comment">// 第二个 int 为占位符，占位符也可有默认值, 如 int func2(int a, int = 10);</span></span><br>&#123;<br>    <span class="hljs-keyword">return</span> a<br>&#125;<br><br><span class="hljs-comment">//函数重载</span><br><span class="hljs-comment">//同一个作用域下</span><br><span class="hljs-comment">//函数名称相同</span><br><span class="hljs-comment">//函数参数类型不同，或者个数不同，或者顺序不同</span><br><span class="hljs-comment">// 函数的返回值不可以做为函数重载的条件</span><br><br><span class="hljs-comment">//引用做为重载条件</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">func3</span>(<span class="hljs-params"><span class="hljs-built_in">int</span> &amp;a</span>)</span><br>&#123;<br>    cout &lt;&lt; <span class="hljs-string">&#x27;&#x27;</span> &lt;&lt; endl;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">func3</span>(<span class="hljs-params"><span class="hljs-keyword">const</span> <span class="hljs-built_in">int</span> &amp;a</span>)</span><br>&#123;<br>    cout &lt;&lt; <span class="hljs-string">&#x27;const&#x27;</span> &lt;&lt; endl;<br>&#125;<br><br><br><span class="hljs-function"><span class="hljs-built_in">int</span> <span class="hljs-title">main</span>(<span class="hljs-params"><span class="hljs-built_in">int</span> argc, <span class="hljs-built_in">char</span> <span class="hljs-keyword">const</span> *argv[]</span>)</span><br>&#123;<br>    func3(<span class="hljs-number">10</span>); <span class="hljs-comment">// 运行 func3(const int &amp;a), 因为 int &amp;a = 10 不合法</span><br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>CPP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CPP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cpp学习笔记01</title>
    <link href="/2022/03/01/cpp%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001/"/>
    <url>/2022/03/01/cpp%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001/</url>
    
    <content type="html"><![CDATA[<h2 id="如题"><a href="#如题" class="headerlink" title="如题"></a>如题</h2><span id="more"></span><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span><span class="hljs-meta-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span><span class="hljs-meta-string">&lt;string&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><br><span class="hljs-comment">// 函数分文件</span><br><span class="hljs-comment">// 1、创建 .h 后缀名的头文件</span><br><span class="hljs-comment">// 2、创建 .cpp 后缀名的源文件</span><br><span class="hljs-comment">// 3、在头文件中写函数的声明</span><br><span class="hljs-comment">// 4、在源文件中写函数定义</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;swap.h&quot;</span></span><br><br><span class="hljs-comment">//常量定义方式</span><br><span class="hljs-comment">//1、 #define 宏常量</span><br><span class="hljs-comment">//2、 const 修饰的变量</span><br><br><span class="hljs-comment">//1、 #define 定义常量</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> Day 7</span><br><br><span class="hljs-comment">// struct 自定义数据结构关键字，结构体指针通过 -&gt; 获取属性数据</span><br><br><span class="hljs-comment">//栈区数据(局部变量、形参等)由编译器管理，堆区数据(new关键字开辟内存存放数据，可使用delete删除)由程序员管理</span><br><br><span class="hljs-comment">// 使用new创建数据：new返回的是 该数据类型的指针</span><br><span class="hljs-keyword">int</span> *p = <span class="hljs-keyword">new</span> <span class="hljs-built_in"><span class="hljs-keyword">int</span></span>(<span class="hljs-number">10</span>);<br><br><span class="hljs-comment">// 函数声明：可以提前告诉编译器函数的存在, 声明可以多次，定义只有一次</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">max</span><span class="hljs-params">(<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b)</span></span>;<br><br><span class="hljs-comment">// 函数：值传递不修改实参，地址传递修改实参</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">swap1</span><span class="hljs-params">(<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b)</span></span>; <span class="hljs-comment">// 值传递</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">swap2</span><span class="hljs-params">(<span class="hljs-keyword">int</span> *p1, <span class="hljs-keyword">int</span> *p2)</span></span>; <span class="hljs-comment">// 地址传递 swap2(&amp;a, &amp;b)</span><br><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">bubbleSort</span><span class="hljs-params">(<span class="hljs-keyword">int</span> arr[], <span class="hljs-keyword">int</span> len)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; len - <span class="hljs-number">1</span>; i++)<br>    &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; len -i <span class="hljs-number">-1</span>; j++)<br>        &#123;<br>            <span class="hljs-keyword">if</span> (arr[j] &gt; arr[j+<span class="hljs-number">1</span>])<br>            &#123;<br>                <span class="hljs-keyword">int</span> tmp = arr[j];<br>                arr[j] = arr[j+<span class="hljs-number">1</span>];<br>                arr[j+<span class="hljs-number">1</span>] = tmp;<br>            &#125;<br>                <br>        &#125;<br>    &#125;<br>&#125;<br><br><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">int</span> argc, <span class="hljs-keyword">char</span> <span class="hljs-keyword">const</span> *argv[])</span></span><br><span class="hljs-function"></span>&#123;   <br>    <span class="hljs-comment">//2、 const 修饰的变量</span><br>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> month = <span class="hljs-number">12</span>;  <span class="hljs-comment">// const 修饰的变量也是常量</span><br>    cout &lt;&lt; Day &lt;&lt; month &lt;&lt; endl;<br><br>    <span class="hljs-comment">// int a = 10;</span><br>    <span class="hljs-comment">// sizeof 计算内存大小</span><br>    <span class="hljs-comment">// cout &lt;&lt; sizeof(a) &lt;&lt; endl;</span><br><br>    <span class="hljs-comment">// 默认情况小数显示6位有效数字</span><br>    <span class="hljs-keyword">float</span> f = <span class="hljs-number">3.14f</span>; <span class="hljs-comment">// 默认双精度，数字后加f为单精度</span><br><br>    <span class="hljs-comment">//字符型变量用于显示单个字符</span><br>    <span class="hljs-keyword">char</span> c = <span class="hljs-string">&#x27;a&#x27;</span>; <span class="hljs-comment">// 只能单引号</span><br>    cout &lt;&lt; (<span class="hljs-keyword">int</span>)c &lt;&lt; endl; <span class="hljs-comment">// (int) 转换 c 为 ascii 值</span><br><br>    cout &lt;&lt; <span class="hljs-string">&quot;aa\tbb&quot;</span> &lt;&lt; endl;<br><br>    <span class="hljs-comment">// c 风格字符串  chat 字符串名 []</span><br>    <span class="hljs-keyword">char</span> str1[] = <span class="hljs-string">&quot;hello&quot;</span>;<br>    <span class="hljs-comment">// c++ 风格字符串</span><br>    string str2 = <span class="hljs-string">&quot;hello&quot;</span>;<br>    cout &lt;&lt; str1 &lt;&lt; <span class="hljs-string">&quot;\t&quot;</span> &lt;&lt; str2 &lt;&lt;endl;<br><br>    <span class="hljs-comment">// 两个整数相除结果还是整数（向下取整）</span><br>    cout &lt;&lt; <span class="hljs-number">10</span> / <span class="hljs-number">20</span> &lt;&lt; endl;<br>    <span class="hljs-comment">// % 取膜运算（取余数）</span><br><br>    <span class="hljs-comment">// 三目运算符</span><br>    <span class="hljs-comment">// 表达式1 ？表达式2 ：表达式3  1真 执行2 ,否则 执行3</span><br>    <span class="hljs-comment">// c++ 中 三目运算符返回的是变量，可以继续赋值</span><br>    <span class="hljs-keyword">int</span> a = <span class="hljs-number">10</span>; <span class="hljs-keyword">int</span> b = <span class="hljs-number">20</span>;<br>    (a &gt; b ? a : b) = <span class="hljs-number">100</span>; <span class="hljs-comment">// 对 b 重新赋值 100</span><br><br>    <span class="hljs-comment">// goto 语句</span><br>    <span class="hljs-comment">// 如果goto的标记存在，则直接跳转到标记处</span><br>    <span class="hljs-keyword">goto</span> FLAG;<br>    cout &lt;&lt; <span class="hljs-string">&quot;跳过了&quot;</span> &lt;&lt; endl; <span class="hljs-comment">// 该语句不执行</span><br>    FLAG:<br>    cout &lt;&lt; <span class="hljs-string">&quot;执行&quot;</span> &lt;&lt; endl;<br><br>    <span class="hljs-comment">// 数据类型 数组名[数组长度];</span><br>    <span class="hljs-keyword">int</span> arr[<span class="hljs-number">2</span>] = &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>&#125;;  <span class="hljs-comment">// 数组名 arr 是常量，不可进行赋值操作，如 arr = 10</span><br><br>    <span class="hljs-comment">// 二维数组</span><br>    <span class="hljs-comment">// 1、数据类型 数组名[行数][列数];</span><br>    <span class="hljs-comment">// 2、数据类型 数组名[行数][列数] = &#123;&#123;数据1，数据2&#125;, &#123;数据3，数据4&#125;&#125;;</span><br>    <span class="hljs-comment">// 3、数据类型 数组名[行数][列数] = &#123;数据1，数据2, 数据3，数据4&#125;;</span><br>    <span class="hljs-comment">// 4、数据类型 数组名[][列数] = &#123;数据1，数据2, 数据3，数据4&#125;;</span><br><br>    <span class="hljs-comment">// 指针定义：数据类型 * 指针变量名;</span><br>    <span class="hljs-comment">// 让指针记录变量的地址</span><br>    <span class="hljs-keyword">int</span> g = <span class="hljs-number">10</span>;<br>    <span class="hljs-keyword">int</span> *p = &amp;g; <span class="hljs-comment">// 等同于下面两行代码</span><br>    <span class="hljs-comment">// int *p;</span><br>    <span class="hljs-comment">// p = &amp;g;</span><br><br>    <span class="hljs-comment">// 指针前加 * 号代表 解引用</span><br>    cout &lt;&lt; *p &lt;&lt; endl;<br>    *p = <span class="hljs-number">100</span>; <span class="hljs-comment">// 可以进行赋值修改变量的值</span><br>    cout &lt;&lt; g &lt;&lt; endl;<br><br>    <span class="hljs-comment">// 空指针：指针变量指向内存中编号为0的空间</span><br>    <span class="hljs-comment">// 用途：初始化变量</span><br>    <span class="hljs-comment">// 注：空指针不可以进行访问(0-255内存编号为系统占用)</span><br>    <span class="hljs-keyword">int</span> *p1 = <span class="hljs-literal">NULL</span>;<br><br>    <span class="hljs-comment">// 野指针：指向非法的内存空间</span><br><br>    <span class="hljs-comment">// const修饰指针 </span><br>    <span class="hljs-comment">// 常量指针:指针的指向可以修改，指针指的值不可以修改</span><br>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> *p2 = &amp;a;  <span class="hljs-comment">// *n = 20 错误， n = &amp;b; 正确</span><br><br>    <span class="hljs-comment">// 指针常量：指针的指向不可以修改，指针指的值可以修改</span><br>    <span class="hljs-keyword">int</span> * <span class="hljs-keyword">const</span> p3 = &amp;a;<br><br>    <span class="hljs-comment">// const修饰指针，又修饰常量： 指针的指向不可以修改，指针指的值不可以修改</span><br>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> * <span class="hljs-keyword">const</span> p4 = &amp;a;<br><br>    <span class="hljs-comment">// 指针和数组</span><br>    <span class="hljs-keyword">int</span> arr2[<span class="hljs-number">2</span>] = &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>&#125;;<br>    <span class="hljs-keyword">int</span> *p5 = arr2; <span class="hljs-comment">// arr2 数组首地址</span><br><br>    cout &lt;&lt; *p5 &lt;&lt; endl; <span class="hljs-comment">//可获取arr第一个值</span><br>    p5++;<br>    cout &lt;&lt; *p5 &lt;&lt; endl; <span class="hljs-comment">//可获取arr第2个值</span><br>    <br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">max</span><span class="hljs-params">(<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b)</span></span>&#123;<br>    <span class="hljs-keyword">return</span> a &gt; b ? a : b;<br>&#125;<br><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>CPP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CPP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图数据库neo4j-Echarts可视化</title>
    <link href="/2022/02/23/%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93neo4j-Echarts%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <url>/2022/02/23/%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93neo4j-Echarts%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="Neo4j-Flask-Echarts"><a href="#Neo4j-Flask-Echarts" class="headerlink" title="Neo4j Flask Echarts"></a>Neo4j Flask Echarts</h2><span id="more"></span><h5 id="安装-Neo4j"><a href="#安装-Neo4j" class="headerlink" title="安装 Neo4j"></a>安装 Neo4j</h5><p>Neo4j依赖Java环境，所以需要先安装JDK， JDK下载地址如下：</p><ul><li><a href="https://www.oracle.com/java/technologies/downloads/">https://www.oracle.com/java/technologies/downloads/</a><br>最新版的Neo4j需要java11版本以上，所以按需求安装对应的版本即可。</li></ul><p>Neo4j下载地址如下：</p><ul><li><a href="https://neo4j.com/download-center/#community">https://neo4j.com/download-center/#community</a></li></ul><p>社区版是免费的，所以选择社区版安装最新版即可。<br>另：Neo4j 插件 apoc 安装地址如下（插件根据需求安装）：</p><ul><li><a href="https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases">https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases</a></li></ul><p>插件这里有个小坑，就是版本之间的依赖，如果Neo4j下载的是最新版，插件也最好选择最新版进行安装。<br>下载完后直接放在 neo4j-community-4.4.4/plugins 目录下，并且需要在配置文件里进行设置。</p><h5 id="启动-Neo4j"><a href="#启动-Neo4j" class="headerlink" title="启动 Neo4j"></a>启动 Neo4j</h5><p>如果是桌面端直接运行程序即可，如果是server端配置好环境后运行 neo4j start 即可启动，停止运行命令：neo4j stop<br>启动后可以在浏览器输入 <a href="http://localhost:7474/">http://localhost:7474/</a> 进入数据库，初始用户名和密码均是：neo4j，第一次进入需要修改密码，自行修改即可。<br>另外neo4j自带的数据库为 system 和 neo4j。如果想创建一个新的数据库需要在配置文件里修改，配置文件地址为：</p><ul><li>neo4j-community-4.4.4/conf/neo4j.conf</li></ul><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">修改默认数据库<br><span class="hljs-meta">#The name of the default database</span><br><span class="hljs-meta">#dbms.default_database=neo4j</span><br>dbms.default_database=test<br><br>修改插件配置<br># A comma separated list <span class="hljs-keyword">of</span> <span class="hljs-keyword">procedures</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">user</span> defined <span class="hljs-keyword">functions</span> that are allowed<br># <span class="hljs-keyword">full</span> <span class="hljs-keyword">access</span> <span class="hljs-keyword">to</span> the <span class="hljs-keyword">database</span> through unsupported/insecure <span class="hljs-type">internal</span> APIs.<br><span class="hljs-meta">#dbms.security.procedures.unrestricted=my.extensions.example,my.procedures.*</span><br>dbms.<span class="hljs-keyword">security</span>.<span class="hljs-keyword">procedures</span>.unrestricted=apoc.*<br><br>修改免密登陆（测试需求）<br># Whether requests <span class="hljs-keyword">to</span> Neo4j are authenticated.<br># <span class="hljs-keyword">To</span> <span class="hljs-keyword">disable</span> authentication, uncomment this <span class="hljs-type">line</span><br>dbms.<span class="hljs-keyword">security</span>.auth_enabled=<span class="hljs-keyword">false</span><br></code></pre></td></tr></table></figure><p>其它配置有需求再进行修改。</p><h5 id="Neo4j-节点和关系创建"><a href="#Neo4j-节点和关系创建" class="headerlink" title="Neo4j 节点和关系创建"></a>Neo4j 节点和关系创建</h5><ul><li>参考教程：<a href="https://www.w3cschool.cn/neo4j/">https://www.w3cschool.cn/neo4j/</a></li><li>python方法</li></ul><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-comment">## pip install py2neo </span><br>from py2neo <span class="hljs-built_in">import</span> *<br><br><span class="hljs-comment"># 连接neo4j数据库，输入地址、用户名、密码</span><br><span class="hljs-attr">graph</span> = Graph(<span class="hljs-string">&quot;bolt://localhost:7687&quot;</span>, <span class="hljs-attr">auth=(&#x27;neo4j&#x27;,&#x27;neo4j&#x27;))</span><br>graph.delete_all() <span class="hljs-comment">#清除neo4j中原有的结点等所有信息</span><br><br><span class="hljs-comment"># 随机生成100个节点</span><br><span class="hljs-attr">N</span> = <span class="hljs-number">100</span><br><span class="hljs-attr">node_ls</span> = []<br>for i <span class="hljs-keyword">in</span> range(N):<br><span class="hljs-attr">random_name</span> = <span class="hljs-string">&quot;P&quot;</span>+str(round(random.random()*N*<span class="hljs-number">2</span>))<br><span class="hljs-attr">random_age</span> = round(random.random()*<span class="hljs-number">15</span>)<br><span class="hljs-attr">node</span> = Node(<span class="hljs-string">&quot;Person&quot;</span>, <span class="hljs-attr">name=random_name,</span> <span class="hljs-attr">age=random_age)</span><br>node_ls.append(node)<br><br><span class="hljs-attr">subgraph</span> = Subgraph(node_ls, [])    <span class="hljs-comment">## [] 代表关系为空</span><br><span class="hljs-attr">tx</span> = graph.begin() <br>tx.create(subgraph)<br>graph.commit(tx)<br></code></pre></td></tr></table></figure><p>详情可参考：<a href="https://mp.weixin.qq.com/s/1DnMHOx6jPi0j0GhZwNPqw">https://mp.weixin.qq.com/s/1DnMHOx6jPi0j0GhZwNPqw</a></p><h5 id="Echarts可视化"><a href="#Echarts可视化" class="headerlink" title="Echarts可视化"></a>Echarts可视化</h5><p>neo4j本身有自己的可视化系统，比如browser、Bloom等，但是blowser面向开发者，Bloom又是收费的，所以只能自己动手了。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment">## 主要就是 option的设置</span><br><span class="hljs-string">var</span> <span class="hljs-string">categories</span> <span class="hljs-string">=</span> [&#123;<span class="hljs-string">name:&quot;DrugName&quot;</span>&#125;,&#123;<span class="hljs-string">name:&quot;TargetGene&quot;</span>&#125;,&#123;<span class="hljs-string">name:&quot;DiseaseLabel&quot;</span>&#125;]<span class="hljs-string">;</span><br><br><span class="hljs-string">option</span> <span class="hljs-string">=</span> &#123;<br>    <span class="hljs-string">//</span> <span class="hljs-string">color:</span>[<span class="hljs-string">&#x27;#5470c6&#x27;</span>, <span class="hljs-string">&#x27;#91cc75&#x27;</span>, <span class="hljs-string">&#x27;#fac858&#x27;</span>, <span class="hljs-string">&#x27;#ee6666&#x27;</span>, <span class="hljs-string">&#x27;#73c0de&#x27;</span>, <span class="hljs-string">&#x27;#3ba272&#x27;</span>, <span class="hljs-string">&#x27;#fc8452&#x27;</span>, <span class="hljs-string">&#x27;#9a60b4&#x27;</span>, <span class="hljs-string">&#x27;#ea7ccc&#x27;</span>],<br>    <span class="hljs-attr">tooltip:</span> &#123;<br>     <span class="hljs-attr">trigger:</span> <span class="hljs-string">&#x27;item&#x27;</span>,<br>     <span class="hljs-attr">triggerOn:</span> <span class="hljs-string">&#x27;mousemove&#x27;</span>,<br>         <span class="hljs-string">enterable:true</span>,<span class="hljs-string">//鼠标是否可进入提示框浮层中</span><br>          <span class="hljs-string">formatter:formatterHover</span>,<span class="hljs-string">//修改鼠标悬停显示的内容</span><br>    &#125;,<br>    <span class="hljs-string">color:</span>[<span class="hljs-string">&quot;#c12e34&quot;</span>, <span class="hljs-string">&quot;#e6b600&quot;</span>, <span class="hljs-string">&quot;#0098d9&quot;</span>,],<br>    <span class="hljs-string">textStyle:</span>&#123;<br>        <span class="hljs-attr">fontFamily:</span> <span class="hljs-string">&quot;sans-serif&quot;</span>,<br>        <span class="hljs-attr">fontSize:</span> <span class="hljs-number">12</span>,<br>        <span class="hljs-attr">fontStyle:</span> <span class="hljs-string">&quot;normal&quot;</span>,<br>        <span class="hljs-attr">fontWeight:</span> <span class="hljs-string">&quot;normal&quot;</span>,<br>    &#125;,<br>    <span class="hljs-attr">legend:</span> &#123;<br>        <span class="hljs-attr">data:</span> [<span class="hljs-string">&#x27;DrugName&#x27;</span>, <span class="hljs-string">&#x27;TargetGene&#x27;</span>, <span class="hljs-string">&#x27;DiseaseLabel&#x27;</span>] <span class="hljs-string">//此处的数据必须和关系网类别中name相对应</span>  <br>    &#125;,<br>    <span class="hljs-attr">series:</span> [&#123;<br>        <span class="hljs-attr">type:</span> <span class="hljs-string">&#x27;graph&#x27;</span>,<br>        <span class="hljs-attr">layout:</span> <span class="hljs-string">&#x27;force&#x27;</span>,     <span class="hljs-comment"># 力导图</span><br>        <span class="hljs-attr">animation:</span> <span class="hljs-literal">false</span>,<br>        <span class="hljs-attr">symbolSize:</span> <span class="hljs-number">25</span>,<br>        <span class="hljs-attr">label:</span> &#123;<br>            <span class="hljs-attr">normal:</span> &#123;<br>                <span class="hljs-attr">show:</span> <span class="hljs-literal">false</span>,<br>                <span class="hljs-attr">position:</span> <span class="hljs-string">&#x27;right&#x27;</span>,<br>                <span class="hljs-attr">textStyle:</span> &#123;<br>                    <span class="hljs-attr">fontSize:</span> <span class="hljs-number">10</span><br>                &#125;,<br>            &#125;,<br>        &#125;,<br>        <span class="hljs-attr">labelLayout:</span> &#123;<br>            <span class="hljs-attr">hideOverlap:</span> <span class="hljs-literal">true</span><br>        &#125;,<br>        <span class="hljs-attr">draggable:</span> <span class="hljs-literal">true</span>,<br>        <span class="hljs-attr">roam:</span> <span class="hljs-literal">true</span>,<br>        <span class="hljs-attr">focusNodeAdjacency:</span> <span class="hljs-literal">true</span>,<br>        <span class="hljs-attr">force:</span> &#123;<br>            <span class="hljs-attr">edgeLength:</span> <span class="hljs-number">80</span>, <span class="hljs-string">//连线的长度</span>  <br>            <span class="hljs-attr">repulsion:</span> <span class="hljs-number">100</span>, <span class="hljs-string">//子节点之间的间距</span>  <br>            <span class="hljs-string">//</span> <span class="hljs-attr">gravity:</span> <span class="hljs-number">0.2</span><br>        &#125;,<br>        <span class="hljs-attr">categories:</span> <span class="hljs-string">categories</span>,<br>        <span class="hljs-attr">data:</span> <span class="hljs-string">data.nodes</span>,<br>        <span class="hljs-attr">edges:</span> <span class="hljs-string">data.edges</span>,<br><br>        <span class="hljs-attr">edgeSymbol:</span> [<span class="hljs-string">&#x27;circle&#x27;</span>, <span class="hljs-string">&#x27;arrow&#x27;</span>],<br>        <span class="hljs-attr">edgeSymbolSize:</span> [<span class="hljs-number">4</span>, <span class="hljs-number">4</span>],<br>        <span class="hljs-attr">edgeLabel:</span> &#123;<br>            <span class="hljs-attr">normal:</span> &#123;<br>                <span class="hljs-attr">show:</span> <span class="hljs-literal">false</span>,<br>                <span class="hljs-attr">formatter:</span> <span class="hljs-string">function</span> <span class="hljs-string">(x)</span> &#123;<br>                    <span class="hljs-string">return</span> <span class="hljs-string">x.data.relationship;</span><br>                &#125;,<br>                <span class="hljs-attr">textStyle:</span> &#123;<br>                    <span class="hljs-attr">fontSize:</span> <span class="hljs-number">10</span><br>                &#125;<br>            &#125;<br>        &#125;,<br>        <span class="hljs-attr">lineStyle:</span> &#123;<br>            <span class="hljs-attr">normal:</span> &#123;<br>                <span class="hljs-attr">width:</span> <span class="hljs-number">2</span>,<br>                <span class="hljs-attr">color:</span> <span class="hljs-string">&#x27;#4b565b&#x27;</span>,<br>            &#125;<br>        &#125;,<br>    &#125;]<br>&#125;<span class="hljs-string">;</span><br></code></pre></td></tr></table></figure><p>剩下的就是把 data 和 edges 的数据传人即可。<br>同样可以用python进行请求获取</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs kotlin"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> json<br><br>login_url = <span class="hljs-string">&#x27;http://localhost:7474/db/data/transaction/commit&#x27;</span><br>headers = &#123;<span class="hljs-string">&#x27;Content-Type&#x27;</span>:<span class="hljs-string">&#x27;application/json&#x27;</span>&#125;<br>name_pwd = &#123;<span class="hljs-string">&#x27;username&#x27;</span>:<span class="hljs-string">&#x27;neo4j&#x27;</span>,<br>      <span class="hljs-string">&#x27;password&#x27;</span>:<span class="hljs-string">&#x27;neo4j&#x27;</span>,<br>      &#125;<br><br><br>def request_result(login_url, query_cql, headers):<br><br>    nodes, edges = [], []<br><br>    query_json = &#123;<br>        <span class="hljs-string">&quot;statements&quot;</span> : [ &#123;<br>        <span class="hljs-string">&quot;statement&quot;</span> : query_cql,<br>        <span class="hljs-string">&quot;resultDataContents&quot;</span> : [ <span class="hljs-string">&quot;row&quot;</span>, <span class="hljs-string">&quot;graph&quot;</span> ]<br>        &#125;]<br>    &#125;<br><br>    response = requests.post(url=login_url, json= query_json, headers=headers)<br>    print(response.status_code) <br><br>    result = json.loads(response.text)[<span class="hljs-string">&#x27;results&#x27;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;data&#x27;</span>]<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(result)):<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> result[i][<span class="hljs-string">&#x27;graph&#x27;</span>][<span class="hljs-string">&#x27;nodes&#x27;</span>]:<br>            <span class="hljs-keyword">if</span> n not <span class="hljs-keyword">in</span> nodes:<br>                nodes.append(n)<br><br>        <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> result[i][<span class="hljs-string">&#x27;graph&#x27;</span>][<span class="hljs-string">&#x27;relationships&#x27;</span>]:<br>            <span class="hljs-keyword">if</span> r not <span class="hljs-keyword">in</span> edges:<br>                edges.append(r)<br><br>    <span class="hljs-keyword">return</span> nodes, edges<br><br><br>def query_all():<br>    query_cql = <span class="hljs-string">&#x27;match p=()--&gt;() return p&#x27;</span><br><br>    nodes, edges = request_result(login_url, query_cql, headers)<br>    <br>    <span class="hljs-keyword">return</span> nodes, edges<br><br><br>## 修改成 Echarts 需要的格式<br>def buildNodes(nodeRecord):<br>    <span class="hljs-keyword">data</span> = &#123;<span class="hljs-string">&quot;id&quot;</span>: str(nodeRecord[<span class="hljs-string">&#x27;id&#x27;</span>]), <span class="hljs-string">&quot;label&quot;</span>: next(iter(nodeRecord[<span class="hljs-string">&#x27;labels&#x27;</span>]))&#125;<br>    <span class="hljs-keyword">data</span>.update(nodeRecord[<span class="hljs-string">&#x27;properties&#x27;</span>])<br><br>    tradeNames = <span class="hljs-keyword">data</span>.<span class="hljs-keyword">get</span>(<span class="hljs-string">&#x27;TradeNames&#x27;</span>)<br>    <span class="hljs-keyword">if</span> tradeNames and len(tradeNames) &gt; <span class="hljs-number">20</span>:<br>        tradeNames = tradeNames[:<span class="hljs-number">20</span>] + <span class="hljs-string">&#x27;...&#x27;</span><br>        <span class="hljs-keyword">data</span>[<span class="hljs-string">&#x27;TradeNames&#x27;</span>] = tradeNames<br><br>    <span class="hljs-keyword">data</span>[<span class="hljs-string">&#x27;category&#x27;</span>] = category_dict.<span class="hljs-keyword">get</span>(<span class="hljs-keyword">data</span>[<span class="hljs-string">&#x27;label&#x27;</span>])<br>    # newdata = &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-keyword">data</span>[<span class="hljs-string">&#x27;name&#x27;</span>], <span class="hljs-string">&#x27;category&#x27;</span>: category_dict.<span class="hljs-keyword">get</span>(<span class="hljs-keyword">data</span>[<span class="hljs-string">&#x27;label&#x27;</span>])&#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">data</span><br><br>def buildEdges(relationRecord):<br>    <span class="hljs-keyword">data</span> = &#123;<span class="hljs-string">&quot;source&quot;</span>: str(relationRecord[<span class="hljs-string">&#x27;startNode&#x27;</span>]), <br>            <span class="hljs-string">&quot;target&quot;</span>: str(relationRecord[<span class="hljs-string">&#x27;endNode&#x27;</span>]), <br>            <span class="hljs-string">&quot;relationship&quot;</span>: relationRecord[<span class="hljs-string">&#x27;type&#x27;</span>]&#125;<br>    # <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&quot;links&quot;</span>: <span class="hljs-keyword">data</span>&#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">data</span><br><br><br>nodes, edges = query_all()<br>nodes = [buildNodes(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> nodes]<br>edges = [buildEdges(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> edges]<br><br></code></pre></td></tr></table></figure><p>整体前端框架参考</p><ul><li><a href="https://github.com/chizhu/KGQA_HLM">https://github.com/chizhu/KGQA_HLM</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Neo4j</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记17-时间序列数据建模流程范例</title>
    <link href="/2022/02/09/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B/"/>
    <url>/2022/02/09/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="时间序列数据"><a href="#时间序列数据" class="headerlink" title="时间序列数据"></a>时间序列数据</h2><span id="more"></span><p>本文基于中国2020年3月之前的疫情数据，建立时间序列RNN模型，对中国的新冠肺炎疫情结束时间进行预测。</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> datetime<br><span class="hljs-keyword">import</span> importlib <br><span class="hljs-keyword">import</span> torchkeras<br><br><span class="hljs-meta">#打印时间</span><br><span class="hljs-title">def</span> printbar():<br>    nowtime = datetime.datetime.now().strftime(&#x27;%<span class="hljs-type">Y</span>-%m-%d %<span class="hljs-type">H</span>:%<span class="hljs-type">M</span>:%<span class="hljs-type">S&#x27;</span>)<br>    print(<span class="hljs-string">&quot;\n&quot;</span>+<span class="hljs-string">&quot;==========&quot;</span>*<span class="hljs-number">8</span> + <span class="hljs-string">&quot;%s&quot;</span>%nowtime)<br><br><span class="hljs-meta">#mac系统上pytorch和matplotlib在jupyter中同时跑需要更改环境变量</span><br><span class="hljs-title">os</span>.environ[<span class="hljs-string">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class="hljs-string">&quot;TRUE&quot;</span> <br></code></pre></td></tr></table></figure><h4 id="一，准备数据"><a href="#一，准备数据" class="headerlink" title="一，准备数据"></a>一，准备数据</h4><p>本文的数据集取自tushare，获取该数据集的方法参考了以下文章。</p><ul><li>《<a href="https://zhuanlan.zhihu.com/p/109556102%E3%80%8B">https://zhuanlan.zhihu.com/p/109556102》</a></li></ul><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-built_in">import</span> numpy as np<br><span class="hljs-built_in">import</span> pandas as pd <br><span class="hljs-built_in">import</span> matplotlib.pyplot as plt<br><br><br>%matplotlib inline<br>%config InlineBackend.<span class="hljs-attr">figure_format</span> = &#x27;svg&#x27;<br><br><span class="hljs-attr">df</span> = pd.read_csv(<span class="hljs-string">&quot;/home/kesci/input/data6936/data/covid-19.csv&quot;</span>,<span class="hljs-attr">sep</span> = <span class="hljs-string">&quot;\t&quot;</span>)<br>df.plot(<span class="hljs-attr">x</span> = <span class="hljs-string">&quot;date&quot;</span>,<span class="hljs-attr">y</span> = [<span class="hljs-string">&quot;confirmed_num&quot;</span>,<span class="hljs-string">&quot;cured_num&quot;</span>,<span class="hljs-string">&quot;dead_num&quot;</span>],<span class="hljs-attr">figsize=(10,6))</span><br>plt.xticks(<span class="hljs-attr">rotation=60)</span><br><br><span class="hljs-attr">dfdata</span> = df.set_index(<span class="hljs-string">&quot;date&quot;</span>)<br><span class="hljs-attr">dfdiff</span> = dfdata.diff(<span class="hljs-attr">periods=1).dropna()</span><br><span class="hljs-attr">dfdiff</span> = dfdiff.reset_index(<span class="hljs-string">&quot;date&quot;</span>)<br><br>dfdiff.plot(<span class="hljs-attr">x</span> = <span class="hljs-string">&quot;date&quot;</span>,<span class="hljs-attr">y</span> = [<span class="hljs-string">&quot;confirmed_num&quot;</span>,<span class="hljs-string">&quot;cured_num&quot;</span>,<span class="hljs-string">&quot;dead_num&quot;</span>],<span class="hljs-attr">figsize=(10,6))</span><br>plt.xticks(<span class="hljs-attr">rotation=60)</span><br><span class="hljs-attr">dfdiff</span> = dfdiff.drop(<span class="hljs-string">&quot;date&quot;</span>,<span class="hljs-attr">axis</span> = <span class="hljs-number">1</span>).astype(<span class="hljs-string">&quot;float32&quot;</span>)<br></code></pre></td></tr></table></figure><p>下面我们通过继承torch.utils.data.Dataset实现自定义时间序列数据集。</p><p>torch.utils.data.Dataset是一个抽象类，用户想要加载自定义的数据只需要继承这个类，并且覆写其中的两个方法即可：</p><ul><li>__len__:实现len(dataset)返回整个数据集的大小。</li><li>__getitem__:用来获取一些索引的数据，使dataset[i]返回数据集中第i个样本。<br>不覆写这两个方法会直接返回错误。</li></ul><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> torch <br><span class="hljs-title">from</span> torch <span class="hljs-keyword">import</span> nn <br><span class="hljs-title">from</span> torch.utils.<span class="hljs-class"><span class="hljs-keyword">data</span> import <span class="hljs-type">Dataset</span>,<span class="hljs-type">DataLoader</span>,<span class="hljs-type">TensorDataset</span></span><br><br><br><span class="hljs-meta">#用某日前8天窗口数据作为输入预测该日数据</span><br><span class="hljs-type">WINDOW_SIZE</span> = <span class="hljs-number">8</span><br><span class="hljs-class"></span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">Covid19Dataset</span>(<span class="hljs-type">Dataset</span>):</span><br><span class="hljs-class">        </span><br><span class="hljs-class">    def __len__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        return len(<span class="hljs-title">dfdiff</span>) - <span class="hljs-type">WINDOW_SIZE</span></span><br><span class="hljs-class">    </span><br><span class="hljs-class">    def __getitem__(<span class="hljs-title">self</span>,<span class="hljs-title">i</span>):</span><br><span class="hljs-class">        x = dfdiff.loc[i:i+<span class="hljs-type">WINDOW_SIZE</span>-1,:]</span><br><span class="hljs-class">        feature = torch.tensor(<span class="hljs-title">x</span>.<span class="hljs-title">values</span>)</span><br><span class="hljs-class">        y = dfdiff.loc[i+<span class="hljs-type">WINDOW_SIZE</span>,:]</span><br><span class="hljs-class">        label = torch.tensor(<span class="hljs-title">y</span>.<span class="hljs-title">values</span>)</span><br><span class="hljs-class">        return (<span class="hljs-title">feature</span>,<span class="hljs-title">label</span>)</span><br><span class="hljs-class">    </span><br><span class="hljs-class">ds_train = <span class="hljs-type">Covid19Dataset</span>()</span><br><span class="hljs-class"></span><br><span class="hljs-class">#数据较小，可以将全部训练数据放入到一个batch中，提升性能</span><br><span class="hljs-class">dl_train = <span class="hljs-type">DataLoader</span>(<span class="hljs-title">ds_train</span>,<span class="hljs-title">batch_size</span> = 38)</span><br></code></pre></td></tr></table></figure><h4 id="二，定义模型"><a href="#二，定义模型" class="headerlink" title="二，定义模型"></a>二，定义模型</h4><p>使用Pytorch通常有三种方式构建模型：使用nn.Sequential按层顺序构建模型，继承nn.Module基类构建自定义模型，继承nn.Module基类构建模型并辅助应用模型容器进行封装。</p><p>此处选择第二种方式构建模型。</p><p>由于接下来使用类形式的训练循环，我们进一步将模型封装成torchkeras中的Model类来获得类似Keras中高阶模型接口的功能。</p><p>Model类实际上继承自nn.Module类。</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> torch<br><span class="hljs-title">from</span> torch <span class="hljs-keyword">import</span> nn <br><span class="hljs-keyword">import</span> importlib <br><span class="hljs-keyword">import</span> torchkeras <br><br><span class="hljs-title">torch</span>.random.seed()<br><span class="hljs-class"></span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">Block</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">Block</span>,<span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">    </span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>,<span class="hljs-title">x</span>,<span class="hljs-title">x_input</span>):</span><br><span class="hljs-class">        x_out = torch.max((1+<span class="hljs-title">x</span>)*x_input[:,-1,:],torch.tensor(0.0))</span><br><span class="hljs-class">        return x_out</span><br><span class="hljs-class">    </span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">Net</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">Net</span>, <span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">        # 3层lstm</span><br><span class="hljs-class">        self.lstm = nn.<span class="hljs-type">LSTM</span>(<span class="hljs-title">input_size</span> = 3,<span class="hljs-title">hidden_size</span> = 3,<span class="hljs-title">num_layers</span> = 5,<span class="hljs-title">batch_first</span> = <span class="hljs-type">True</span>)</span><br><span class="hljs-class">        self.linear = nn.<span class="hljs-type">Linear</span>(3,3)</span><br><span class="hljs-class">        self.block = <span class="hljs-type">Block</span>()</span><br><span class="hljs-class">        </span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>,<span class="hljs-title">x_input</span>):</span><br><span class="hljs-class">        x = self.lstm(<span class="hljs-title">x_input</span>)[0][:,-1,:]</span><br><span class="hljs-class">        x = self.linear(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        y = self.block(<span class="hljs-title">x</span>,<span class="hljs-title">x_input</span>)</span><br><span class="hljs-class">        return y</span><br><span class="hljs-class">        </span><br><span class="hljs-class">net = <span class="hljs-type">Net</span>()</span><br><span class="hljs-class">model = torchkeras.<span class="hljs-type">Model</span>(<span class="hljs-title">net</span>)</span><br><span class="hljs-class">print(<span class="hljs-title">model</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class">model.summary(<span class="hljs-title">input_shape</span>=(8,3),input_dtype = torch.<span class="hljs-type">FloatTensor</span>)</span><br></code></pre></td></tr></table></figure><h4 id="三，训练模型"><a href="#三，训练模型" class="headerlink" title="三，训练模型"></a>三，训练模型</h4><p>训练Pytorch通常需要用户编写自定义训练循环，训练循环的代码风格因人而异。</p><p>有3类典型的训练循环代码风格：脚本形式训练循环，函数形式训练循环，类形式训练循环。</p><p>此处介绍一种类形式的训练循环。</p><p>我们仿照Keras定义了一个高阶的模型接口Model,实现 fit, validate，predict, summary 方法，相当于用户自定义高阶API。</p><p>注：循环神经网络调试较为困难，需要设置多个不同的学习率多次尝试，以取得较好的效果。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">def</span> mspe(y_pred,y_true):<br>    <span class="hljs-attribute">err_percent</span> = (y_true - y_pred)**<span class="hljs-number">2</span>/(torch.max(y_true**<span class="hljs-number">2</span>,torch.tensor(<span class="hljs-number">1</span>e-<span class="hljs-number">7</span>)))<br>    <span class="hljs-attribute">return</span> torch.mean(err_percent)<br><br><span class="hljs-attribute">model</span>.compile(loss_func = mspe,optimizer = torch.optim.Adagrad(model.parameters(),lr = <span class="hljs-number">0</span>.<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">dfhistory</span> = model.fit(<span class="hljs-number">100</span>,dl_train,log_step_freq=<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><h4 id="四，评估模型"><a href="#四，评估模型" class="headerlink" title="四，评估模型"></a>四，评估模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib inline<br>%config InlineBackend.figure_format = <span class="hljs-string">&#x27;svg&#x27;</span><br><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_metric</span>(<span class="hljs-params">dfhistory, metric</span>):</span><br>    train_metrics = dfhistory[metric]<br>    epochs = <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(train_metrics) + <span class="hljs-number">1</span>)<br>    plt.plot(epochs, train_metrics, <span class="hljs-string">&#x27;bo--&#x27;</span>)<br>    plt.title(<span class="hljs-string">&#x27;Training &#x27;</span>+ metric)<br>    plt.xlabel(<span class="hljs-string">&quot;Epochs&quot;</span>)<br>    plt.ylabel(metric)<br>    plt.legend([<span class="hljs-string">&quot;train_&quot;</span>+metric])<br>    plt.show()<br><br>plot_metric(dfhistory,<span class="hljs-string">&quot;loss&quot;</span>)<br><br></code></pre></td></tr></table></figure><h4 id="五，使用模型"><a href="#五，使用模型" class="headerlink" title="五，使用模型"></a>五，使用模型</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">#使用dfresult记录现有数据以及此后预测的疫情数据<br>dfresult = dfdiff<span class="hljs-selector-attr">[[<span class="hljs-string">&quot;confirmed_num&quot;</span>,<span class="hljs-string">&quot;cured_num&quot;</span>,<span class="hljs-string">&quot;dead_num&quot;</span>]</span>]<span class="hljs-selector-class">.copy</span>()<br>dfresult<span class="hljs-selector-class">.tail</span>()<br></code></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs css">#预测此后<span class="hljs-number">500</span>天的新增走势,将其结果添加到dfresult中<br>for <span class="hljs-selector-tag">i</span> in range(<span class="hljs-number">500</span>):<br>    arr_input = torch.<span class="hljs-built_in">unsqueeze</span>(torch.<span class="hljs-built_in">from_numpy</span>(dfresult.values[-<span class="hljs-number">38</span>:,:]),axis=<span class="hljs-number">0</span>)<br>    arr_predict = model.<span class="hljs-built_in">forward</span>(arr_input)<br><br>    dfpredict = pd.<span class="hljs-built_in">DataFrame</span>(torch.<span class="hljs-built_in">floor</span>(arr_predict).data.<span class="hljs-built_in">numpy</span>(),<br>                columns = dfresult.columns)<br>    dfresult = dfresult.<span class="hljs-built_in">append</span>(dfpredict,ignore_index=True)<br></code></pre></td></tr></table></figure><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs vala">dfresult.query(<span class="hljs-string">&quot;confirmed_num==0&quot;</span>).head()<br><br><span class="hljs-meta"># 第50天开始新增确诊降为0，第45天对应3月10日，也就是5天后，即预计3月15日新增确诊降为0</span><br><span class="hljs-meta"># 注：该预测偏乐观</span><br></code></pre></td></tr></table></figure><h4 id="六，保存模型"><a href="#六，保存模型" class="headerlink" title="六，保存模型"></a>六，保存模型</h4><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># 保存模型参数<br><br>torch.save(model.net.state<span class="hljs-constructor">_dict()</span>, <span class="hljs-string">&quot;./data/model_parameter.pkl&quot;</span>)<br><br>net_clone = <span class="hljs-constructor">Net()</span><br>net_clone.load<span class="hljs-constructor">_state_dict(<span class="hljs-params">torch</span>.<span class="hljs-params">load</span>(<span class="hljs-string">&quot;./data/model_parameter.pkl&quot;</span>)</span>)<br>model_clone = torchkeras.<span class="hljs-constructor">Model(<span class="hljs-params">net_clone</span>)</span><br>model_clone.compile(loss_func = mspe)<br><br># 评估模型<br>model_clone.evaluate(dl_train)<br></code></pre></td></tr></table></figure><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记16-文本数据建模流程范例</title>
    <link href="/2022/02/09/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B/"/>
    <url>/2022/02/09/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="文本数据"><a href="#文本数据" class="headerlink" title="文本数据"></a>文本数据</h2><span id="more"></span><h4 id="一，准备数据"><a href="#一，准备数据" class="headerlink" title="一，准备数据"></a>一，准备数据</h4><p>imdb数据集的目标是根据电影评论的文本内容预测评论的情感标签。</p><p>训练集有20000条电影评论文本，测试集有5000条电影评论文本，其中正面评论和负面评论都各占一半。</p><p>文本数据预处理较为繁琐，包括中文切词（本示例不涉及），构建词典，编码转换，序列填充，构建数据管道等等。</p><p>在torch中预处理文本数据一般使用torchtext或者自定义Dataset，torchtext功能非常强大，可以构建文本分类，序列标注，问答模型，机器翻译等NLP任务的数据集。</p><p>下面仅演示使用它来构建文本分类数据集的方法。</p><p>较完整的教程可以参考以下知乎文章：《pytorch学习笔记—Torchtext》</p><ul><li><a href="https://zhuanlan.zhihu.com/p/65833208">https://zhuanlan.zhihu.com/p/65833208</a></li></ul><p>torchtext常见API一览</p><ul><li>torchtext.data.Example : 用来表示一个样本，数据和标签</li><li>torchtext.vocab.Vocab: 词汇表，可以导入一些预训练词向量</li><li>torchtext.data.Datasets: 数据集类，__getitem__返回 Example实例, torchtext.data.TabularDataset是其子类。</li><li>torchtext.data.Field : 用来定义字段的处理方法（文本字段，标签字段）创建 Example时的 预处理，batch 时的一些处理操作。</li><li>torchtext.data.Iterator: 迭代器，用来生成 batch</li><li>torchtext.datasets: 包含了常见的数据集.</li></ul><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br><span class="hljs-title">from</span> collections <span class="hljs-keyword">import</span> OrderedDict<br><span class="hljs-keyword">import</span> re,string<br><span class="hljs-type">MAX_WORDS</span> = <span class="hljs-number">10000</span>  # 仅考虑最高频的<span class="hljs-number">10000</span>个词<br><span class="hljs-type">MAX_LEN</span> = <span class="hljs-number">200</span>  # 每个样本保留<span class="hljs-number">200</span>个词的长度<br><span class="hljs-type">BATCH_SIZE</span> = <span class="hljs-number">20</span> <br><span class="hljs-title">train_data_path</span> = &#x27;/home/kesci/input/data6936/<span class="hljs-class"><span class="hljs-keyword">data</span>/imdb/train.tsv&#x27;</span><br><span class="hljs-title">test_data_path</span> = &#x27;/home/kesci/input/data6936/<span class="hljs-class"><span class="hljs-keyword">data</span>/imdb/test.tsv&#x27;</span><br><span class="hljs-title">train_token_path</span> = &#x27;/home/kesci/input/data6936/<span class="hljs-class"><span class="hljs-keyword">data</span>/imdb/train_token.tsv&#x27;</span><br><span class="hljs-title">test_token_path</span> =  &#x27;/home/kesci/input/data6936/<span class="hljs-class"><span class="hljs-keyword">data</span>/imdb/test_token.tsv&#x27;</span><br><span class="hljs-title">train_samples_path</span> = &#x27;/home/kesci/input/data6936/<span class="hljs-class"><span class="hljs-keyword">data</span>/imdb/train_samples/&#x27;</span><br><span class="hljs-title">test_samples_path</span> =  &#x27;/home/kesci/input/data6936/<span class="hljs-class"><span class="hljs-keyword">data</span>/imdb/test_samples/&#x27;</span><br></code></pre></td></tr></table></figure><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><span class="hljs-comment">##构建词典</span><br><br>word_count_dict = &#123;&#125;<br><br><span class="hljs-comment">#清洗文本</span><br>def clean_text(<span class="hljs-built_in">text</span>):<br>    lowercase = <span class="hljs-built_in">text</span>.lower().replace(<span class="hljs-string">&quot;\n&quot;</span>,<span class="hljs-string">&quot; &quot;</span>)<br>    stripped_html = re.sub(&#x27;&lt;br /&gt;&#x27;, &#x27; &#x27;,lowercase)<br>    cleaned_punctuation = re.sub(&#x27;[%s]&#x27;%re.escape(<span class="hljs-built_in">string</span>.punctuation),&#x27;&#x27;,stripped_html)<br><span class="hljs-built_in">    return</span> cleaned_punctuation<br><br><span class="hljs-keyword">with</span> open(train_data_path,<span class="hljs-string">&quot;r&quot;</span>,encoding = &#x27;utf<span class="hljs-number">-8</span>&#x27;) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f:<br>        label,<span class="hljs-built_in">text</span> = line.split(<span class="hljs-string">&quot;\t&quot;</span>)<br>        cleaned_text = clean_text(<span class="hljs-built_in">text</span>)<br>        <span class="hljs-keyword">for</span> <span class="hljs-built_in">word</span> <span class="hljs-keyword">in</span> cleaned_text.split(<span class="hljs-string">&quot; &quot;</span>):<br>            word_count_dict[<span class="hljs-built_in">word</span>] = word_count_dict.<span class="hljs-keyword">get</span>(<span class="hljs-built_in">word</span>,<span class="hljs-number">0</span>)+<span class="hljs-number">1</span> <br><br>df_word_dict = pd.DataFrame(pd.Series(word_count_dict,<span class="hljs-built_in">name</span> = <span class="hljs-string">&quot;count&quot;</span>))<br>df_word_dict = df_word_dict.sort_values(<span class="hljs-keyword">by</span> = <span class="hljs-string">&quot;count&quot;</span>,ascending =False)<br><br>df_word_dict = df_word_dict[<span class="hljs-number">0</span>:MAX_WORDS<span class="hljs-number">-2</span>] <span class="hljs-comment">#  </span><br>df_word_dict[<span class="hljs-string">&quot;word_id&quot;</span>] = range(<span class="hljs-number">2</span>,MAX_WORDS) <span class="hljs-comment">#编号0和1分别留给未知词&lt;unkown&gt;和填充&lt;padding&gt;</span><br><br>word_id_dict = df_word_dict[<span class="hljs-string">&quot;word_id&quot;</span>].to_dict()<br><br>df_word_dict.head(<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><p>利用构建好的词典，将文本转换成token序号。</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-comment">#转换token</span><br><br><span class="hljs-comment"># 填充文本</span><br>def pad(data_list,pad_length):<br>    padded_list = data_list.copy()<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(data_list)&gt; pad_length:<br>         padded_list = data_list[-pad_length:]<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(data_list)&lt; pad_length:<br>         padded_list = [<span class="hljs-number">1</span>]*(pad_length-<span class="hljs-built_in">len</span>(data_list))+data_list<br>    <span class="hljs-literal">return</span> padded_list<br><br>def text_to_token(text_file,token_file):<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(text_file,<span class="hljs-string">&quot;r&quot;</span>,encoding = <span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fin,\<br>      <span class="hljs-built_in">open</span>(token_file,<span class="hljs-string">&quot;w&quot;</span>,encoding = <span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fout:<br>        <span class="hljs-keyword">for</span> <span class="hljs-built_in">line</span> <span class="hljs-keyword">in</span> fin:<br>            label,<span class="hljs-keyword">text</span> = <span class="hljs-built_in">line</span>.<span class="hljs-built_in">split</span>(<span class="hljs-string">&quot;\t&quot;</span>)<br>            cleaned_text = clean_text(<span class="hljs-keyword">text</span>)<br>            word_token_list = [word_id_dict.<span class="hljs-built_in">get</span>(<span class="hljs-built_in">word</span>, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> <span class="hljs-built_in">word</span> <span class="hljs-keyword">in</span> cleaned_text.<span class="hljs-built_in">split</span>(<span class="hljs-string">&quot; &quot;</span>)]<br>            pad_list = pad(word_token_list,MAX_LEN)<br>            out_line = label+<span class="hljs-string">&quot;\t&quot;</span>+<span class="hljs-string">&quot; &quot;</span>.join([str(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> pad_list])<br>            fout.<span class="hljs-built_in">write</span>(out_line+<span class="hljs-string">&quot;\n&quot;</span>)<br>        <br>text_to_token(train_data_path,train_token_path)<br>text_to_token(test_data_path,test_token_path)<br></code></pre></td></tr></table></figure><p>接着将token文本按照样本分割，每个文件存放一个样本的数据。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># 分割样本<br>import os<br><br><span class="hljs-keyword">if</span> not os.path.exists(train_samples_path):<br>    os.mkdir(train_samples_path)<br>    <br><span class="hljs-keyword">if</span> not os.path.exists(test_samples_path):<br>    os.mkdir(test_samples_path)<br>    <br>    <br>def split<span class="hljs-constructor">_samples(<span class="hljs-params">token_path</span>,<span class="hljs-params">samples_dir</span>)</span>:<br>    <span class="hljs-keyword">with</span> <span class="hljs-keyword">open</span>(token_path,<span class="hljs-string">&quot;r&quot;</span>,encoding = &#x27;utf-<span class="hljs-number">8</span>&#x27;) <span class="hljs-keyword">as</span> fin:<br>        i = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> fin:<br>            <span class="hljs-keyword">with</span> <span class="hljs-keyword">open</span>(samples_dir+<span class="hljs-string">&quot;%d.txt&quot;</span>%i,<span class="hljs-string">&quot;w&quot;</span>,encoding = <span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> fout:<br>                fout.write(line)<br>            i = i+<span class="hljs-number">1</span><br><br>split<span class="hljs-constructor">_samples(<span class="hljs-params">train_token_path</span>,<span class="hljs-params">train_samples_path</span>)</span><br>split<span class="hljs-constructor">_samples(<span class="hljs-params">test_token_path</span>,<span class="hljs-params">test_samples_path</span>)</span><br></code></pre></td></tr></table></figure><p>创建数据集Dataset, 从文件名称列表中读取文件内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset,DataLoader <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">imdbDataset</span>(<span class="hljs-params">Dataset</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,samples_dir</span>):</span><br>        self.samples_dir = samples_dir<br>        self.samples_paths = os.listdir(samples_dir)<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.samples_paths)<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span>(<span class="hljs-params">self,index</span>):</span><br>        path = self.samples_dir + self.samples_paths[index]<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(path,<span class="hljs-string">&quot;r&quot;</span>,encoding = <span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>            line = f.readline()<br>            label,tokens = line.split(<span class="hljs-string">&quot;\t&quot;</span>)<br>            label = torch.tensor([<span class="hljs-built_in">float</span>(label)],dtype = torch.<span class="hljs-built_in">float</span>)<br>            feature = torch.tensor([<span class="hljs-built_in">int</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> tokens.split(<span class="hljs-string">&quot; &quot;</span>)],dtype = torch.long)<br>            <span class="hljs-keyword">return</span>  (feature,label)<br>ds_train = imdbDataset(train_samples_path)<br>ds_test = imdbDataset(test_samples_path)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(ds_train))<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(ds_test))<br><br>dl_train = DataLoader(ds_train,batch_size = BATCH_SIZE,shuffle = <span class="hljs-literal">True</span>,num_workers=<span class="hljs-number">4</span>)<br>dl_test = DataLoader(ds_test,batch_size = BATCH_SIZE,num_workers=<span class="hljs-number">4</span>)<br><br><span class="hljs-keyword">for</span> features,labels <span class="hljs-keyword">in</span> dl_train:<br>    <span class="hljs-built_in">print</span>(features)<br>    <span class="hljs-built_in">print</span>(labels)<br>    <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure><h4 id="二，定义模型"><a href="#二，定义模型" class="headerlink" title="二，定义模型"></a>二，定义模型</h4><p>使用Pytorch通常有三种方式构建模型：使用nn.Sequential按层顺序构建模型，继承nn.Module基类构建自定义模型，继承nn.Module基类构建模型并辅助应用模型容器(nn.Sequential,nn.ModuleList,nn.ModuleDict)进行封装。</p><p>此处选择使用第三种方式进行构建。</p><p>由于接下来使用类形式的训练循环，我们将模型封装成torchkeras.Model类来获得类似Keras中高阶模型接口的功能。</p><p>Model类实际上继承自nn.Module类。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">import torch<br>from torch import nn <br>import torchkeras<br><br>torch.random.seed<span class="hljs-literal">()</span><br>import torch<br>from torch import nn <br><br><span class="hljs-keyword">class</span> <span class="hljs-constructor">Net(<span class="hljs-params">torchkeras</span>.Model)</span>:<br>    <br>    def <span class="hljs-constructor">__init__(<span class="hljs-params">self</span>)</span>:<br>        super(Net, self).<span class="hljs-constructor">__init__()</span><br>        <br>        #设置padding_idx参数后将在训练过程中将填充的token始终赋值为<span class="hljs-number">0</span>向量<br>        self.embedding = nn.<span class="hljs-constructor">Embedding(<span class="hljs-params">num_embeddings</span> = MAX_WORDS,<span class="hljs-params">embedding_dim</span> = 3,<span class="hljs-params">padding_idx</span> = 1)</span><br>        self.conv = nn.<span class="hljs-constructor">Sequential()</span><br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;conv_1&quot;</span>,<span class="hljs-params">nn</span>.Conv1d(<span class="hljs-params">in_channels</span> = 3,<span class="hljs-params">out_channels</span> = 16,<span class="hljs-params">kernel_size</span> = 5)</span>)<br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;pool_1&quot;</span>,<span class="hljs-params">nn</span>.MaxPool1d(<span class="hljs-params">kernel_size</span> = 2)</span>)<br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;relu_1&quot;</span>,<span class="hljs-params">nn</span>.ReLU()</span>)<br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;conv_2&quot;</span>,<span class="hljs-params">nn</span>.Conv1d(<span class="hljs-params">in_channels</span> = 16,<span class="hljs-params">out_channels</span> = 128,<span class="hljs-params">kernel_size</span> = 2)</span>)<br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;pool_2&quot;</span>,<span class="hljs-params">nn</span>.MaxPool1d(<span class="hljs-params">kernel_size</span> = 2)</span>)<br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;relu_2&quot;</span>,<span class="hljs-params">nn</span>.ReLU()</span>)<br>        <br>        self.dense = nn.<span class="hljs-constructor">Sequential()</span><br>        self.dense.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;flatten&quot;</span>,<span class="hljs-params">nn</span>.Flatten()</span>)<br>        self.dense.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;linear&quot;</span>,<span class="hljs-params">nn</span>.Linear(6144,1)</span>)<br>        self.dense.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;sigmoid&quot;</span>,<span class="hljs-params">nn</span>.Sigmoid()</span>)<br>        <br>    def forward(self,x):<br>        x = self.embedding(x).transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)<br>        x = self.conv(x)<br>        y = self.dense(x)<br>        return y<br>        <br><br>model = <span class="hljs-constructor">Net()</span><br>print(model)<br><br>model.summary(input_shape = (<span class="hljs-number">200</span>,),input_dtype = torch.LongTensor)<br></code></pre></td></tr></table></figure><h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><p>训练Pytorch通常需要用户编写自定义训练循环，训练循环的代码风格因人而异。</p><p>有3类典型的训练循环代码风格：脚本形式训练循环，函数形式训练循环，类形式训练循环。</p><p>此处介绍一种类形式的训练循环。</p><p>我们仿照Keras定义了一个高阶的模型接口Model,实现 fit, validate，predict, summary 方法，相当于用户自定义高阶API。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># 准确率<br>def accuracy(y_pred,y_true):<br>    y_pred = torch.where(y_pred&gt;<span class="hljs-number">0.5</span>,torch.ones<span class="hljs-constructor">_like(<span class="hljs-params">y_pred</span>,<span class="hljs-params">dtype</span> = <span class="hljs-params">torch</span>.<span class="hljs-params">float32</span>)</span>,<br>                      torch.zeros<span class="hljs-constructor">_like(<span class="hljs-params">y_pred</span>,<span class="hljs-params">dtype</span> = <span class="hljs-params">torch</span>.<span class="hljs-params">float32</span>)</span>)<br>    acc = torch.mean(<span class="hljs-number">1</span>-torch.abs(y_true-y_pred))<br>    return acc<br><br>model.compile(loss_func = nn.<span class="hljs-constructor">BCELoss()</span>,optimizer= torch.optim.<span class="hljs-constructor">Adagrad(<span class="hljs-params">model</span>.<span class="hljs-params">parameters</span>()</span>,lr = <span class="hljs-number">0.02</span>),<br>             metrics_dict=&#123;<span class="hljs-string">&quot;accuracy&quot;</span>:accuracy&#125;)<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># 有时候模型训练过程中不收敛，需要多试几次</span><br><span class="hljs-attribute">dfhistory</span> = model.fit(<span class="hljs-number">20</span>,dl_train,dl_val=dl_test,log_step_freq= <span class="hljs-number">200</span>)<br></code></pre></td></tr></table></figure><h4 id="四，评估模型"><a href="#四，评估模型" class="headerlink" title="四，评估模型"></a>四，评估模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib inline<br>%config InlineBackend.figure_format = <span class="hljs-string">&#x27;svg&#x27;</span><br><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_metric</span>(<span class="hljs-params">dfhistory, metric</span>):</span><br>    train_metrics = dfhistory[metric]<br>    val_metrics = dfhistory[<span class="hljs-string">&#x27;val_&#x27;</span>+metric]<br>    epochs = <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(train_metrics) + <span class="hljs-number">1</span>)<br>    plt.plot(epochs, train_metrics, <span class="hljs-string">&#x27;bo--&#x27;</span>)<br>    plt.plot(epochs, val_metrics, <span class="hljs-string">&#x27;ro-&#x27;</span>)<br>    plt.title(<span class="hljs-string">&#x27;Training and validation &#x27;</span>+ metric)<br>    plt.xlabel(<span class="hljs-string">&quot;Epochs&quot;</span>)<br>    plt.ylabel(metric)<br>    plt.legend([<span class="hljs-string">&quot;train_&quot;</span>+metric, <span class="hljs-string">&#x27;val_&#x27;</span>+metric])<br>    plt.show()<br><br>plot_metric(dfhistory,<span class="hljs-string">&quot;loss&quot;</span>)<br>plot_metric(dfhistory,<span class="hljs-string">&quot;accuracy&quot;</span>)<br><br><span class="hljs-comment"># 评估</span><br>model.evaluate(dl_test)<br></code></pre></td></tr></table></figure><h4 id="五，使用模型"><a href="#五，使用模型" class="headerlink" title="五，使用模型"></a>五，使用模型</h4><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-keyword">model</span>.predict(dl_test)<br></code></pre></td></tr></table></figure><h4 id="六，保存模型"><a href="#六，保存模型" class="headerlink" title="六，保存模型"></a>六，保存模型</h4><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># 保存模型参数<br><br>torch.save(model.state<span class="hljs-constructor">_dict()</span>, <span class="hljs-string">&quot;./data/model_parameter.pkl&quot;</span>)<br><br>model_clone = <span class="hljs-constructor">Net()</span><br>model_clone.load<span class="hljs-constructor">_state_dict(<span class="hljs-params">torch</span>.<span class="hljs-params">load</span>(<span class="hljs-string">&quot;./data/model_parameter.pkl&quot;</span>)</span>)<br><br>model_clone.compile(loss_func = nn.<span class="hljs-constructor">BCELoss()</span>,optimizer= torch.optim.<span class="hljs-constructor">Adagrad(<span class="hljs-params">model</span>.<span class="hljs-params">parameters</span>()</span>,lr = <span class="hljs-number">0.02</span>),<br>             metrics_dict=&#123;<span class="hljs-string">&quot;accuracy&quot;</span>:accuracy&#125;)<br><br># 评估模型<br>model_clone.evaluate(dl_test)<br></code></pre></td></tr></table></figure><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记15-图片数据建模流程范例</title>
    <link href="/2022/02/08/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B/"/>
    <url>/2022/02/08/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="图片数据"><a href="#图片数据" class="headerlink" title="图片数据"></a>图片数据</h2><span id="more"></span><figure class="highlight cos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cos">import os<br>import datetime<br><br>#打印时间<br>def printbar():<br>    nowtime = datetime.datetime.now().strftime(&#x27;<span class="hljs-built_in">%Y</span>-<span class="hljs-built_in">%m</span>-<span class="hljs-built_in">%d</span> <span class="hljs-built_in">%H</span>:<span class="hljs-built_in">%M</span>:<span class="hljs-built_in">%S</span>&#x27;)<br>    <span class="hljs-keyword">print</span>(<span class="hljs-string">&quot;\n&quot;</span>+<span class="hljs-string">&quot;==========&quot;</span>*<span class="hljs-number">8</span> + <span class="hljs-string">&quot;%s&quot;</span><span class="hljs-built_in">%nowtime</span>)<br><br>#mac系统上pytorch和matplotlib在jupyter中同时跑需要更改环境变量<br>os.environ[<span class="hljs-string">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class="hljs-string">&quot;TRUE&quot;</span> <br></code></pre></td></tr></table></figure><h4 id="一，准备数据"><a href="#一，准备数据" class="headerlink" title="一，准备数据"></a>一，准备数据</h4><p>cifar2数据集为cifar10数据集的子集，只包括前两种类别airplane和automobile。</p><p>训练集有airplane和automobile图片各5000张，测试集有airplane和automobile图片各1000张。</p><p>cifar2任务的目标是训练一个模型来对飞机airplane和机动车automobile两种图片进行分类。</p><p>在Pytorch中构建图片数据管道通常有两种方法。</p><p>第一种是使用 torchvision中的datasets.ImageFolder来读取图片然后用 DataLoader来并行加载。</p><p>第二种是通过继承 torch.utils.data.Dataset 实现用户自定义读取逻辑然后用 DataLoader来并行加载。</p><p>第二种方法是读取用户自定义数据集的通用方法，既可以读取图片数据集，也可以读取文本数据集。</p><p>本篇我们介绍第一种方法。</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-built_in">import</span> torch <br>from torch <span class="hljs-built_in">import</span> nn<br>from torch.utils.data <span class="hljs-built_in">import</span> Dataset,DataLoader<br>from torchvision <span class="hljs-built_in">import</span> transforms,datasets <br><br><br><span class="hljs-attr">transform_train</span> = transforms.Compose(<br>    [transforms.ToTensor()])<br><span class="hljs-attr">transform_valid</span> = transforms.Compose(<br>    [transforms.ToTensor()])<br><br><br><span class="hljs-attr">ds_train</span> = datasets.ImageFolder(<span class="hljs-string">&quot;/home/kesci/input/data6936/data/cifar2/train/&quot;</span>,<br>            <span class="hljs-attr">transform</span> = transform_train,<span class="hljs-attr">target_transform=</span> lambda t:torch.tensor([t]).float())<br><span class="hljs-attr">ds_valid</span> = datasets.ImageFolder(<span class="hljs-string">&quot;/home/kesci/input/data6936/data/cifar2/test/&quot;</span>,<br>            <span class="hljs-attr">transform</span> = transform_train,<span class="hljs-attr">target_transform=</span> lambda t:torch.tensor([t]).float())<br><br>print(ds_train.class_to_idx)<br></code></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">dl_train = <span class="hljs-constructor">DataLoader(<span class="hljs-params">ds_train</span>,<span class="hljs-params">batch_size</span> = 50,<span class="hljs-params">shuffle</span> = True,<span class="hljs-params">num_workers</span>=3)</span><br>dl_valid = <span class="hljs-constructor">DataLoader(<span class="hljs-params">ds_valid</span>,<span class="hljs-params">batch_size</span> = 50,<span class="hljs-params">shuffle</span> = True,<span class="hljs-params">num_workers</span>=3)</span><br></code></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">%matplotlib inline<br>%config <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">InlineBackend</span>.</span></span>figure_format = &#x27;svg&#x27;<br><br>#查看部分样本<br>from matplotlib import pyplot <span class="hljs-keyword">as</span> plt <br><br>plt.figure(figsize=(<span class="hljs-number">8</span>,<span class="hljs-number">8</span>)) <br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):<br>    img,label = ds_train<span class="hljs-literal">[<span class="hljs-identifier">i</span>]</span><br>    img = img.permute(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>)<br>    ax=plt.subplot(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,i+<span class="hljs-number">1</span>)<br>    ax.imshow(img.numpy<span class="hljs-literal">()</span>)<br>    ax.set<span class="hljs-constructor">_title(<span class="hljs-string">&quot;label = %d&quot;</span>%<span class="hljs-params">label</span>.<span class="hljs-params">item</span>()</span>)<br>    ax.set<span class="hljs-constructor">_xticks([])</span><br>    ax.set<span class="hljs-constructor">_yticks([])</span> <br>plt.show<span class="hljs-literal">()</span><br></code></pre></td></tr></table></figure><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs processing"># Pytorch的图片默认顺序是 Batch,Channel,Width,Height<br><span class="hljs-keyword">for</span> x,y in dl_train:<br>    <span class="hljs-built_in">print</span>(x.<span class="hljs-built_in">shape</span>,y.<span class="hljs-built_in">shape</span>) <br>    <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure><h4 id="二，定义模型"><a href="#二，定义模型" class="headerlink" title="二，定义模型"></a>二，定义模型</h4><p>使用Pytorch通常有三种方式构建模型：使用nn.Sequential按层顺序构建模型，继承nn.Module基类构建自定义模型，继承nn.Module基类构建模型并辅助应用模型容器(nn.Sequential,nn.ModuleList,nn.ModuleDict)进行封装。</p><p>此处选择通过继承nn.Module基类构建自定义模型。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment">#测试AdaptiveMaxPool2d的效果</span><br><span class="hljs-attribute">pool</span> = nn.AdaptiveMaxPool<span class="hljs-number">2</span>d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br><span class="hljs-attribute">t</span> = torch.randn(<span class="hljs-number">10</span>,<span class="hljs-number">8</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>)<br><span class="hljs-attribute">pool</span>(t).shape <br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">class</span> Net(nn.Module):<br>    <br>    <span class="hljs-attribute">def</span> __init__(self):<br>        <span class="hljs-attribute">super</span>(Net, self).__init__()<br>        <span class="hljs-attribute">self</span>.conv<span class="hljs-number">1</span> = nn.Conv<span class="hljs-number">2</span>d(in_channels=<span class="hljs-number">3</span>,out_channels=<span class="hljs-number">32</span>,kernel_size = <span class="hljs-number">3</span>)<br>        <span class="hljs-attribute">self</span>.pool = nn.MaxPool<span class="hljs-number">2</span>d(kernel_size = <span class="hljs-number">2</span>,stride = <span class="hljs-number">2</span>)<br>        <span class="hljs-attribute">self</span>.conv<span class="hljs-number">2</span> = nn.Conv<span class="hljs-number">2</span>d(in_channels=<span class="hljs-number">32</span>,out_channels=<span class="hljs-number">64</span>,kernel_size = <span class="hljs-number">5</span>)<br>        <span class="hljs-attribute">self</span>.dropout = nn.Dropout<span class="hljs-number">2</span>d(p = <span class="hljs-number">0</span>.<span class="hljs-number">1</span>)<br>        <span class="hljs-attribute">self</span>.adaptive_pool = nn.AdaptiveMaxPool<span class="hljs-number">2</span>d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br>        <span class="hljs-attribute">self</span>.flatten = nn.Flatten()<br>        <span class="hljs-attribute">self</span>.linear<span class="hljs-number">1</span> = nn.Linear(<span class="hljs-number">64</span>,<span class="hljs-number">32</span>)<br>        <span class="hljs-attribute">self</span>.relu = nn.ReLU()<br>        <span class="hljs-attribute">self</span>.linear<span class="hljs-number">2</span> = nn.Linear(<span class="hljs-number">32</span>,<span class="hljs-number">1</span>)<br>        <span class="hljs-attribute">self</span>.sigmoid = nn.Sigmoid()<br>        <br>    <span class="hljs-attribute">def</span> forward(self,x):<br>        <span class="hljs-attribute">x</span> = self.conv<span class="hljs-number">1</span>(x)<br>        <span class="hljs-attribute">x</span> = self.pool(x)<br>        <span class="hljs-attribute">x</span> = self.conv<span class="hljs-number">2</span>(x)<br>        <span class="hljs-attribute">x</span> = self.pool(x)<br>        <span class="hljs-attribute">x</span> = self.dropout(x)<br>        <span class="hljs-attribute">x</span> = self.adaptive_pool(x)<br>        <span class="hljs-attribute">x</span> = self.flatten(x)<br>        <span class="hljs-attribute">x</span> = self.linear<span class="hljs-number">1</span>(x)<br>        <span class="hljs-attribute">x</span> = self.relu(x)<br>        <span class="hljs-attribute">x</span> = self.linear<span class="hljs-number">2</span>(x)<br>        <span class="hljs-attribute">y</span> = self.sigmoid(x)<br>        <span class="hljs-attribute">return</span> y<br>        <br><span class="hljs-attribute">net</span> = Net()<br><span class="hljs-attribute">print</span>(net)<br><br><span class="hljs-attribute">import</span> torchkeras<br><span class="hljs-attribute">torchkeras</span>.summary(net,input_shape= (<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>))<br></code></pre></td></tr></table></figure><h4 id="三，训练模型"><a href="#三，训练模型" class="headerlink" title="三，训练模型"></a>三，训练模型</h4><p>Pytorch通常需要用户编写自定义训练循环，训练循环的代码风格因人而异。</p><p>有3类典型的训练循环代码风格：脚本形式训练循环，函数形式训练循环，类形式训练循环。</p><p>此处介绍一种较通用的函数形式训练循环。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import pandas as pd <br>from sklearn<span class="hljs-selector-class">.metrics</span> import roc_auc_score<br><br>model = net<br>model<span class="hljs-selector-class">.optimizer</span> = torch<span class="hljs-selector-class">.optim</span><span class="hljs-selector-class">.SGD</span>(model<span class="hljs-selector-class">.parameters</span>(),lr = <span class="hljs-number">0.01</span>)<br>model<span class="hljs-selector-class">.loss_func</span> = torch<span class="hljs-selector-class">.nn</span><span class="hljs-selector-class">.BCELoss</span>()<br>model<span class="hljs-selector-class">.metric_func</span> = lambda y_pred,y_true: roc_auc_score(y_true<span class="hljs-selector-class">.data</span><span class="hljs-selector-class">.numpy</span>(),y_pred<span class="hljs-selector-class">.data</span><span class="hljs-selector-class">.numpy</span>())<br>model<span class="hljs-selector-class">.metric_name</span> = <span class="hljs-string">&quot;auc&quot;</span><br></code></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">def train<span class="hljs-constructor">_step(<span class="hljs-params">model</span>,<span class="hljs-params">features</span>,<span class="hljs-params">labels</span>)</span>:<br>    <br>    # 训练模式，dropout层发生作用<br>    model.train<span class="hljs-literal">()</span><br>    <br>    # 梯度清零<br>    model.optimizer.zero<span class="hljs-constructor">_grad()</span><br>    <br>    # 正向传播求损失<br>    predictions = model(features)<br>    loss = model.loss<span class="hljs-constructor">_func(<span class="hljs-params">predictions</span>,<span class="hljs-params">labels</span>)</span><br>    metric = model.metric<span class="hljs-constructor">_func(<span class="hljs-params">predictions</span>,<span class="hljs-params">labels</span>)</span><br><br>    # 反向传播求梯度<br>    loss.backward<span class="hljs-literal">()</span><br>    model.optimizer.step<span class="hljs-literal">()</span><br><br>    return loss.item<span class="hljs-literal">()</span>,metric.item<span class="hljs-literal">()</span><br><br>def valid<span class="hljs-constructor">_step(<span class="hljs-params">model</span>,<span class="hljs-params">features</span>,<span class="hljs-params">labels</span>)</span>:<br>    <br>    # 预测模式，dropout层不发生作用<br>    model.eval<span class="hljs-literal">()</span><br>    # 关闭梯度计算<br>    <span class="hljs-keyword">with</span> torch.no<span class="hljs-constructor">_grad()</span>:<br>        predictions = model(features)<br>        loss = model.loss<span class="hljs-constructor">_func(<span class="hljs-params">predictions</span>,<span class="hljs-params">labels</span>)</span><br>        metric = model.metric<span class="hljs-constructor">_func(<span class="hljs-params">predictions</span>,<span class="hljs-params">labels</span>)</span><br>    <br>    return loss.item<span class="hljs-literal">()</span>, metric.item<span class="hljs-literal">()</span><br><br><br># 测试train_step效果<br>features,labels = next(iter(dl_train))<br>train<span class="hljs-constructor">_step(<span class="hljs-params">model</span>,<span class="hljs-params">features</span>,<span class="hljs-params">labels</span>)</span><br></code></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def train_model(model,epochs,dl_train,dl_valid,log_step_freq):<br><br>    metric_name = model.metric_name<br>    dfhistory = pd.DataFrame(columns = [<span class="hljs-string">&quot;epoch&quot;</span>,<span class="hljs-string">&quot;loss&quot;</span>,metric_name,<span class="hljs-string">&quot;val_loss&quot;</span>,<span class="hljs-string">&quot;val_&quot;</span>+metric_name]) <br>    <span class="hljs-builtin-name">print</span>(<span class="hljs-string">&quot;Start Training...&quot;</span>)<br>    nowtime = datetime.datetime.now().strftime(<span class="hljs-string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)<br>    <span class="hljs-builtin-name">print</span>(<span class="hljs-string">&quot;==========&quot;</span><span class="hljs-number">*8</span> + <span class="hljs-string">&quot;%s&quot;</span>%nowtime)<br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(1,epochs+1):  <br><br>        # 1，训练循环-------------------------------------------------<br>        loss_sum = 0.0<br>        metric_sum = 0.0<br>        <span class="hljs-keyword">step</span> = 1<br><br>        <span class="hljs-keyword">for</span> <span class="hljs-keyword">step</span>, (features,labels) <span class="hljs-keyword">in</span> enumerate(dl_train, 1):<br><br>            loss,metric = train_step(model,features,labels)<br><br>            # 打印batch级别日志<br>            loss_sum += loss<br>            metric_sum += metric<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">step</span>%log_step_freq == 0:   <br>                <span class="hljs-builtin-name">print</span>((<span class="hljs-string">&quot;[step = %d] loss: %.3f, &quot;</span>+metric_name+<span class="hljs-string">&quot;: %.3f&quot;</span>) %<br>                      (<span class="hljs-keyword">step</span>, loss_sum/<span class="hljs-keyword">step</span>, metric_sum/<span class="hljs-keyword">step</span>))<br><br>        # 2，验证循环-------------------------------------------------<br>        val_loss_sum = 0.0<br>        val_metric_sum = 0.0<br>        val_step = 1<br><br>        <span class="hljs-keyword">for</span> val_step, (features,labels) <span class="hljs-keyword">in</span> enumerate(dl_valid, 1):<br><br>            val_loss,val_metric = valid_step(model,features,labels)<br><br>            val_loss_sum += val_loss<br>            val_metric_sum += val_metric<br><br>        # 3，记录日志-------------------------------------------------<br>        <span class="hljs-builtin-name">info</span> = (epoch, loss_sum/<span class="hljs-keyword">step</span>, metric_sum/<span class="hljs-keyword">step</span>, <br>                val_loss_sum/val_step, val_metric_sum/val_step)<br>        dfhistory.loc[epoch-1] = <span class="hljs-builtin-name">info</span><br><br>        # 打印epoch级别日志<br>        <span class="hljs-builtin-name">print</span>((<span class="hljs-string">&quot;\nEPOCH = %d, loss = %.3f,&quot;</span>+ metric_name + \<br>              <span class="hljs-string">&quot;  = %.3f, val_loss = %.3f, &quot;</span>+<span class="hljs-string">&quot;val_&quot;</span>+ metric_name+<span class="hljs-string">&quot; = %.3f&quot;</span>) <br>              %info)<br>        nowtime = datetime.datetime.now().strftime(<span class="hljs-string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)<br>        <span class="hljs-builtin-name">print</span>(<span class="hljs-string">&quot;\n&quot;</span>+<span class="hljs-string">&quot;==========&quot;</span><span class="hljs-number">*8</span> + <span class="hljs-string">&quot;%s&quot;</span>%nowtime)<br><br>    <span class="hljs-builtin-name">print</span>(<span class="hljs-string">&#x27;Finished Training...&#x27;</span>)<br>    <br>    return dfhistory<br></code></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">epochs = <span class="hljs-number">20</span><br><br>dfhistory = train<span class="hljs-constructor">_model(<span class="hljs-params">model</span>,<span class="hljs-params">epochs</span>,<span class="hljs-params">dl_train</span>,<span class="hljs-params">dl_valid</span>,<span class="hljs-params">log_step_freq</span> = 50)</span><br><br></code></pre></td></tr></table></figure><h4 id="四，评估模型"><a href="#四，评估模型" class="headerlink" title="四，评估模型"></a>四，评估模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib inline<br>%config InlineBackend.figure_format = <span class="hljs-string">&#x27;svg&#x27;</span><br><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_metric</span>(<span class="hljs-params">dfhistory, metric</span>):</span><br>    train_metrics = dfhistory[metric]<br>    val_metrics = dfhistory[<span class="hljs-string">&#x27;val_&#x27;</span>+metric]<br>    epochs = <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(train_metrics) + <span class="hljs-number">1</span>)<br>    plt.plot(epochs, train_metrics, <span class="hljs-string">&#x27;bo--&#x27;</span>)<br>    plt.plot(epochs, val_metrics, <span class="hljs-string">&#x27;ro-&#x27;</span>)<br>    plt.title(<span class="hljs-string">&#x27;Training and validation &#x27;</span>+ metric)<br>    plt.xlabel(<span class="hljs-string">&quot;Epochs&quot;</span>)<br>    plt.ylabel(metric)<br>    plt.legend([<span class="hljs-string">&quot;train_&quot;</span>+metric, <span class="hljs-string">&#x27;val_&#x27;</span>+metric])<br>    plt.show()<br><br><br>plot_metric(dfhistory,<span class="hljs-string">&quot;loss&quot;</span>)<br>plot_metric(dfhistory,<span class="hljs-string">&quot;auc&quot;</span>)<br></code></pre></td></tr></table></figure><h4 id="五，使用模型"><a href="#五，使用模型" class="headerlink" title="五，使用模型"></a>五，使用模型</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs css">def predict(model,<span class="hljs-selector-tag">dl</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    with torch.<span class="hljs-built_in">no_grad</span>():<br>        result = torch.<span class="hljs-built_in">cat</span>([model.<span class="hljs-built_in">forward</span>(t[<span class="hljs-number">0</span>]) for t in dl])<br>    <span class="hljs-built_in">return</span>(result.data)<br><br><br>#预测概率<br>y_pred_probs = <span class="hljs-built_in">predict</span>(model,dl_valid)<br>y_pred_probs<br><br>#预测类别<br>y_pred = torch.<span class="hljs-built_in">where</span>(y_pred_probs&gt;<span class="hljs-number">0.5</span>,<br>        torch.<span class="hljs-built_in">ones_like</span>(y_pred_probs),torch.<span class="hljs-built_in">zeros_like</span>(y_pred_probs))<br>y_pred<br></code></pre></td></tr></table></figure><h4 id="六，保存模型"><a href="#六，保存模型" class="headerlink" title="六，保存模型"></a>六，保存模型</h4><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">print(model.state<span class="hljs-constructor">_dict()</span>.keys<span class="hljs-literal">()</span>)<br><br># 保存模型参数<br><br>torch.save(model.state<span class="hljs-constructor">_dict()</span>, <span class="hljs-string">&quot;./data/model_parameter.pkl&quot;</span>)<br><br>net_clone = <span class="hljs-constructor">Net()</span><br>net_clone.load<span class="hljs-constructor">_state_dict(<span class="hljs-params">torch</span>.<span class="hljs-params">load</span>(<span class="hljs-string">&quot;./data/model_parameter.pkl&quot;</span>)</span>)<br><br>predict(net_clone,dl_valid)<br></code></pre></td></tr></table></figure><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记14-结构化数据建模流程范例</title>
    <link href="/2022/02/08/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B/"/>
    <url>/2022/02/08/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B%E8%8C%83%E4%BE%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="结构化数据"><a href="#结构化数据" class="headerlink" title="结构化数据"></a>结构化数据</h2><span id="more"></span><figure class="highlight cos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cos">import os<br>import datetime<br><br>#打印时间<br>def printbar():<br>    nowtime = datetime.datetime.now().strftime(&#x27;<span class="hljs-built_in">%Y</span>-<span class="hljs-built_in">%m</span>-<span class="hljs-built_in">%d</span> <span class="hljs-built_in">%H</span>:<span class="hljs-built_in">%M</span>:<span class="hljs-built_in">%S</span>&#x27;)<br>    <span class="hljs-keyword">print</span>(<span class="hljs-string">&quot;\n&quot;</span>+<span class="hljs-string">&quot;==========&quot;</span>*<span class="hljs-number">8</span> + <span class="hljs-string">&quot;%s&quot;</span><span class="hljs-built_in">%nowtime</span>)<br><br>#mac系统上pytorch和matplotlib在jupyter中同时跑需要更改环境变量<br>os.environ[<span class="hljs-string">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class="hljs-string">&quot;TRUE&quot;</span> <br></code></pre></td></tr></table></figure><h4 id="一，准备数据"><a href="#一，准备数据" class="headerlink" title="一，准备数据"></a>一，准备数据</h4><p>titanic数据集的目标是根据乘客信息预测他们在Titanic号撞击冰山沉没后能否生存。</p><p>结构化数据一般会使用Pandas中的DataFrame进行预处理。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">import numpy <span class="hljs-keyword">as</span> np <br>import pandas <span class="hljs-keyword">as</span> pd <br>import matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>import torch <br>from torch import nn <br>from torch.utils.data import Dataset,DataLoader,TensorDataset<br><br>dftrain_raw = pd.read<span class="hljs-constructor">_csv(&#x27;<span class="hljs-operator">/</span><span class="hljs-params">home</span><span class="hljs-operator">/</span><span class="hljs-params">kesci</span><span class="hljs-operator">/</span><span class="hljs-params">input</span><span class="hljs-operator">/</span><span class="hljs-params">data6936</span><span class="hljs-operator">/</span><span class="hljs-params">data</span><span class="hljs-operator">/</span><span class="hljs-params">titanic</span><span class="hljs-operator">/</span><span class="hljs-params">train</span>.<span class="hljs-params">csv</span>&#x27;)</span><br>dftest_raw = pd.read<span class="hljs-constructor">_csv(&#x27;<span class="hljs-operator">/</span><span class="hljs-params">home</span><span class="hljs-operator">/</span><span class="hljs-params">kesci</span><span class="hljs-operator">/</span><span class="hljs-params">input</span><span class="hljs-operator">/</span><span class="hljs-params">data6936</span><span class="hljs-operator">/</span><span class="hljs-params">data</span><span class="hljs-operator">/</span><span class="hljs-params">titanic</span><span class="hljs-operator">/</span><span class="hljs-params">test</span>.<span class="hljs-params">csv</span>&#x27;)</span><br>dftrain_raw.head(<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><p>利用Pandas的数据可视化功能我们可以简单地进行探索性数据分析EDA（Exploratory Data Analysis）。</p><p>label分布情况</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">%matplotlib inline<br>%config InlineBackend.figure_format = &#x27;png&#x27;<br>ax = dftrain_raw[&#x27;Survived&#x27;]<span class="hljs-string">.value_counts</span><span class="hljs-params">()</span><span class="hljs-string">.plot</span><span class="hljs-params">(<span class="hljs-attr">kind</span> = &#x27;bar&#x27;,</span><br><span class="hljs-params">     <span class="hljs-attr">figsize</span> = (12,8)</span>,fontsize=15,rot = 0)<br>ax.<span class="hljs-keyword">set</span>_ylabel<span class="hljs-params">(&#x27;Counts&#x27;,<span class="hljs-attr">fontsize</span> = 15)</span><br>ax.<span class="hljs-keyword">set</span>_xlabel<span class="hljs-params">(&#x27;Survived&#x27;,<span class="hljs-attr">fontsize</span> = 15)</span><br>plt.show<span class="hljs-params">()</span><br></code></pre></td></tr></table></figure><p>数据预处理</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs prolog">def preprocessing(dfdata):<br><br>    dfresult= pd.<span class="hljs-symbol">DataFrame</span>()<br><br>    #<span class="hljs-symbol">Pclass</span><br>    dfPclass = pd.get_dummies(dfdata[<span class="hljs-string">&#x27;Pclass&#x27;</span>])<br>    dfPclass.columns = [<span class="hljs-string">&#x27;Pclass_&#x27;</span> +str(x) for x in dfPclass.columns ]<br>    dfresult = pd.concat([dfresult,dfPclass],axis = <span class="hljs-number">1</span>)<br><br>    #<span class="hljs-symbol">Sex</span><br>    dfSex = pd.get_dummies(dfdata[<span class="hljs-string">&#x27;Sex&#x27;</span>])<br>    dfresult = pd.concat([dfresult,dfSex],axis = <span class="hljs-number">1</span>)<br><br>    #<span class="hljs-symbol">Age</span><br>    dfresult[<span class="hljs-string">&#x27;Age&#x27;</span>] = dfdata[<span class="hljs-string">&#x27;Age&#x27;</span>].fillna(<span class="hljs-number">0</span>)<br>    dfresult[<span class="hljs-string">&#x27;Age_null&#x27;</span>] = pd.isna(dfdata[<span class="hljs-string">&#x27;Age&#x27;</span>]).astype(<span class="hljs-string">&#x27;int32&#x27;</span>)<br><br>    #<span class="hljs-symbol">SibSp</span>,<span class="hljs-symbol">Parch</span>,<span class="hljs-symbol">Fare</span><br>    dfresult[<span class="hljs-string">&#x27;SibSp&#x27;</span>] = dfdata[<span class="hljs-string">&#x27;SibSp&#x27;</span>]<br>    dfresult[<span class="hljs-string">&#x27;Parch&#x27;</span>] = dfdata[<span class="hljs-string">&#x27;Parch&#x27;</span>]<br>    dfresult[<span class="hljs-string">&#x27;Fare&#x27;</span>] = dfdata[<span class="hljs-string">&#x27;Fare&#x27;</span>]<br><br>    #<span class="hljs-symbol">Carbin</span><br>    dfresult[<span class="hljs-string">&#x27;Cabin_null&#x27;</span>] =  pd.isna(dfdata[<span class="hljs-string">&#x27;Cabin&#x27;</span>]).astype(<span class="hljs-string">&#x27;int32&#x27;</span>)<br><br>    #<span class="hljs-symbol">Embarked</span><br>    dfEmbarked = pd.get_dummies(dfdata[<span class="hljs-string">&#x27;Embarked&#x27;</span>],dummy_na=<span class="hljs-symbol">True</span>)<br>    dfEmbarked.columns = [<span class="hljs-string">&#x27;Embarked_&#x27;</span> + str(x) for x in dfEmbarked.columns]<br>    dfresult = pd.concat([dfresult,dfEmbarked],axis = <span class="hljs-number">1</span>)<br><br>    return(dfresult)<br><br>x_train = preprocessing(dftrain_raw).values<br>y_train = dftrain_raw[[<span class="hljs-string">&#x27;Survived&#x27;</span>]].values<br><br>x_test = preprocessing(dftest_raw).values<br>y_test = dftest_raw[[<span class="hljs-string">&#x27;Survived&#x27;</span>]].values<br><br>print(<span class="hljs-string">&quot;x_train.shape =&quot;</span>, x_train.shape )<br>print(<span class="hljs-string">&quot;x_test.shape =&quot;</span>, x_test.shape )<br><br>print(<span class="hljs-string">&quot;y_train.shape =&quot;</span>, y_train.shape )<br>print(<span class="hljs-string">&quot;y_test.shape =&quot;</span>, y_test.shape )<br></code></pre></td></tr></table></figure><p>进一步使用DataLoader和TensorDataset封装成可以迭代的数据管道。</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs lisp">dl_train = DataLoader(<span class="hljs-name">TensorDataset</span>(<span class="hljs-name">torch</span>.tensor(<span class="hljs-name">x_train</span>).float(),torch.tensor(<span class="hljs-name">y_train</span>).float()),<br>                     shuffle = True, batch_size = <span class="hljs-number">8</span>)<br>dl_valid = DataLoader(<span class="hljs-name">TensorDataset</span>(<span class="hljs-name">torch</span>.tensor(<span class="hljs-name">x_test</span>).float(),torch.tensor(<span class="hljs-name">y_test</span>).float()),<br>                     shuffle = False, batch_size = <span class="hljs-number">8</span>)<br></code></pre></td></tr></table></figure><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs maxima"># 测试数据管道<br><span class="hljs-keyword">for</span> <span class="hljs-built_in">features</span>,<span class="hljs-built_in">labels</span> <span class="hljs-keyword">in</span> dl_train:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">features</span>,<span class="hljs-built_in">labels</span>)<br>    <span class="hljs-built_in">break</span><br></code></pre></td></tr></table></figure><h4 id="二，定义模型"><a href="#二，定义模型" class="headerlink" title="二，定义模型"></a>二，定义模型</h4><p>使用Pytorch通常有三种方式构建模型：使用nn.Sequential按层顺序构建模型，继承nn.Module基类构建自定义模型，继承nn.Module基类构建模型并辅助应用模型容器进行封装。</p><p>此处选择使用最简单的nn.Sequential，按层顺序模型。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs css">def create_net():<br>    net = nn.<span class="hljs-built_in">Sequential</span>()<br>    net.<span class="hljs-built_in">add_module</span>(<span class="hljs-string">&quot;linear1&quot;</span>,nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">15</span>,<span class="hljs-number">20</span>))<br>    net.<span class="hljs-built_in">add_module</span>(<span class="hljs-string">&quot;relu1&quot;</span>,nn.<span class="hljs-built_in">ReLU</span>())<br>    net.<span class="hljs-built_in">add_module</span>(<span class="hljs-string">&quot;linear2&quot;</span>,nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">20</span>,<span class="hljs-number">15</span>))<br>    net.<span class="hljs-built_in">add_module</span>(<span class="hljs-string">&quot;relu2&quot;</span>,nn.<span class="hljs-built_in">ReLU</span>())<br>    net.<span class="hljs-built_in">add_module</span>(<span class="hljs-string">&quot;linear3&quot;</span>,nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">15</span>,<span class="hljs-number">1</span>))<br>    net.<span class="hljs-built_in">add_module</span>(<span class="hljs-string">&quot;sigmoid&quot;</span>,nn.<span class="hljs-built_in">Sigmoid</span>())<br>    return net<br>    <br>net = <span class="hljs-built_in">create_net</span>()<br><span class="hljs-built_in">print</span>(net)<br></code></pre></td></tr></table></figure><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">from</span> torchkeras <span class="hljs-keyword">import</span> <span class="hljs-keyword">summary</span><br><span class="hljs-keyword">summary</span>(net,input_shape=(<span class="hljs-number">15</span>,))<br></code></pre></td></tr></table></figure><h4 id="三，训练模型"><a href="#三，训练模型" class="headerlink" title="三，训练模型"></a>三，训练模型</h4><p>Pytorch通常需要用户编写自定义训练循环，训练循环的代码风格因人而异。</p><p>有3类典型的训练循环代码风格：脚本形式训练循环，函数形式训练循环，类形式训练循环。</p><p>此处介绍一种较通用的脚本形式。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">from sklearn.metrics import accuracy_score<br><br>loss_func = nn.<span class="hljs-constructor">BCELoss()</span><br>optimizer = torch.optim.<span class="hljs-constructor">Adam(<span class="hljs-params">params</span>=<span class="hljs-params">net</span>.<span class="hljs-params">parameters</span>()</span>,lr = <span class="hljs-number">0.01</span>)<br>metric_func = lambda y_pred,y_true: accuracy<span class="hljs-constructor">_score(<span class="hljs-params">y_true</span>.<span class="hljs-params">data</span>.<span class="hljs-params">numpy</span>()</span>,y_pred.data.numpy<span class="hljs-literal">()</span>&gt;<span class="hljs-number">0.5</span>)<br>metric_name = <span class="hljs-string">&quot;accuracy&quot;</span><br></code></pre></td></tr></table></figure><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs maxima">epochs = <span class="hljs-number">10</span><br>log_step_freq = <span class="hljs-number">30</span><br><br>dfhistory = pd.DataFrame(<span class="hljs-built_in">columns</span> = [<span class="hljs-string">&quot;epoch&quot;</span>,<span class="hljs-string">&quot;loss&quot;</span>,metric_name,<span class="hljs-string">&quot;val_loss&quot;</span>,<span class="hljs-string">&quot;val_&quot;</span>+metric_name]) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Start Training...&quot;</span>)<br>nowtime = datetime.datetime.now().strftime(&#x27;%Y-<span class="hljs-built_in">%m</span>-%d %H:%M:%S&#x27;)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;==========&quot;</span>*<span class="hljs-number">8</span> + <span class="hljs-string">&quot;%s&quot;</span>%nowtime)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,epochs+<span class="hljs-number">1</span>):  <br><br>    # <span class="hljs-number">1</span>，训练循环-------------------------------------------------<br>    net.train()<br>    loss_sum = <span class="hljs-number">0.0</span><br>    metric_sum = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">step</span> = <span class="hljs-number">1</span><br>    <br>    <span class="hljs-keyword">for</span> <span class="hljs-keyword">step</span>, (<span class="hljs-built_in">features</span>,<span class="hljs-built_in">labels</span>) <span class="hljs-keyword">in</span> enumerate(dl_train, <span class="hljs-number">1</span>):<br>    <br>        # 梯度清零<br>        optimizer.zero_grad()<br><br>        # 正向传播求损失<br>        predictions = net(<span class="hljs-built_in">features</span>)<br>        loss = loss_func(predictions,<span class="hljs-built_in">labels</span>)<br>        metric = metric_func(predictions,<span class="hljs-built_in">labels</span>)<br>        <br>        # 反向传播求梯度<br>        loss.backward()<br>        optimizer.<span class="hljs-keyword">step</span>()<br><br>        # 打印<span class="hljs-built_in">batch</span>级别日志<br>        loss_sum += loss.item()<br>        metric_sum += metric.item()<br>        <span class="hljs-keyword">if</span> step%log_step_freq == <span class="hljs-number">0</span>:   <br>            <span class="hljs-built_in">print</span>((<span class="hljs-string">&quot;[step = %d] loss: %.3f, &quot;</span>+metric_name+<span class="hljs-string">&quot;: %.3f&quot;</span>) <span class="hljs-symbol">%</span><br>                  (<span class="hljs-keyword">step</span>, loss_sum/<span class="hljs-keyword">step</span>, metric_sum/<span class="hljs-keyword">step</span>))<br>            <br>    # <span class="hljs-number">2</span>，验证循环-------------------------------------------------<br>    net.<span class="hljs-built_in">eval</span>()<br>    val_loss_sum = <span class="hljs-number">0.0</span><br>    val_metric_sum = <span class="hljs-number">0.0</span><br>    val_step = <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">for</span> val_step, (<span class="hljs-built_in">features</span>,<span class="hljs-built_in">labels</span>) <span class="hljs-keyword">in</span> enumerate(dl_valid, <span class="hljs-number">1</span>):<br>        # 关闭梯度计算<br>        with torch.no_grad():<br>            predictions = net(<span class="hljs-built_in">features</span>)<br>            val_loss = loss_func(predictions,<span class="hljs-built_in">labels</span>)<br>            val_metric = metric_func(predictions,<span class="hljs-built_in">labels</span>)<br>        val_loss_sum += val_loss.item()<br>        val_metric_sum += val_metric.item()<br><br>    # <span class="hljs-number">3</span>，记录日志-------------------------------------------------<br>    info = (epoch, loss_sum/<span class="hljs-keyword">step</span>, metric_sum/<span class="hljs-keyword">step</span>, <br>            val_loss_sum/val_step, val_metric_sum/val_step)<br>    dfhistory.loc[epoch-<span class="hljs-number">1</span>] = info<br>    <br>    # 打印epoch级别日志<br>    <span class="hljs-built_in">print</span>((<span class="hljs-string">&quot;\nEPOCH = %d, loss = %.3f,&quot;</span>+ metric_name + \<br>          <span class="hljs-string">&quot;  = %.3f, val_loss = %.3f, &quot;</span>+<span class="hljs-string">&quot;val_&quot;</span>+ metric_name+<span class="hljs-string">&quot; = %.3f&quot;</span>) <br>          %info)<br>    nowtime = datetime.datetime.now().strftime(&#x27;%Y-<span class="hljs-built_in">%m</span>-%d %H:%M:%S&#x27;)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n&quot;</span>+<span class="hljs-string">&quot;==========&quot;</span>*<span class="hljs-number">8</span> + <span class="hljs-string">&quot;%s&quot;</span>%nowtime)<br>        <br><span class="hljs-built_in">print</span>(&#x27;Finished Training...&#x27;)<br></code></pre></td></tr></table></figure><h4 id="四，评估模型"><a href="#四，评估模型" class="headerlink" title="四，评估模型"></a>四，评估模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib inline<br>%config InlineBackend.figure_format = <span class="hljs-string">&#x27;svg&#x27;</span><br><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_metric</span>(<span class="hljs-params">dfhistory, metric</span>):</span><br>    train_metrics = dfhistory[metric]<br>    val_metrics = dfhistory[<span class="hljs-string">&#x27;val_&#x27;</span>+metric]<br>    epochs = <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(train_metrics) + <span class="hljs-number">1</span>)<br>    plt.plot(epochs, train_metrics, <span class="hljs-string">&#x27;bo--&#x27;</span>)<br>    plt.plot(epochs, val_metrics, <span class="hljs-string">&#x27;ro-&#x27;</span>)<br>    plt.title(<span class="hljs-string">&#x27;Training and validation &#x27;</span>+ metric)<br>    plt.xlabel(<span class="hljs-string">&quot;Epochs&quot;</span>)<br>    plt.ylabel(metric)<br>    plt.legend([<span class="hljs-string">&quot;train_&quot;</span>+metric, <span class="hljs-string">&#x27;val_&#x27;</span>+metric])<br>    plt.show()<br><br>plot_metric(dfhistory,<span class="hljs-string">&quot;loss&quot;</span>)<br>plot_metric(dfhistory,<span class="hljs-string">&quot;accuracy&quot;</span>)<br></code></pre></td></tr></table></figure><h4 id="五，使用模型"><a href="#五，使用模型" class="headerlink" title="五，使用模型"></a>五，使用模型</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">#预测概率<br>y_pred_probs = net(torch<span class="hljs-selector-class">.tensor</span>(x_test<span class="hljs-selector-attr">[0:10]</span>)<span class="hljs-selector-class">.float</span>())<span class="hljs-selector-class">.data</span><br>y_pred_probs<br><br>#预测类别<br>y_pred = torch<span class="hljs-selector-class">.where</span>(y_pred_probs&gt;<span class="hljs-number">0.5</span>,<br>        torch<span class="hljs-selector-class">.ones_like</span>(y_pred_probs),torch<span class="hljs-selector-class">.zeros_like</span>(y_pred_probs))<br>y_pred<br></code></pre></td></tr></table></figure><h4 id="六，保存模型"><a href="#六，保存模型" class="headerlink" title="六，保存模型"></a>六，保存模型</h4><p>Pytorch 有两种保存模型的方式，都是通过调用pickle序列化方法实现的。</p><p>第一种方法只保存模型参数。</p><p>第二种方法保存完整模型。</p><p>推荐使用第一种，第二种方法可能在切换设备和目录的时候出现各种问题。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(net.state_dict()</span></span><span class="hljs-selector-class">.keys</span>())<br></code></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># 保存模型参数<br><br>torch.save(net.state<span class="hljs-constructor">_dict()</span>, <span class="hljs-string">&quot;./data/net_parameter.pkl&quot;</span>)<br><br>net_clone = create<span class="hljs-constructor">_net()</span><br>net_clone.load<span class="hljs-constructor">_state_dict(<span class="hljs-params">torch</span>.<span class="hljs-params">load</span>(<span class="hljs-string">&quot;./data/net_parameter.pkl&quot;</span>)</span>)<br><br>net_clone.forward(torch.tensor(x_test<span class="hljs-literal">[<span class="hljs-number">0</span>:<span class="hljs-number">10</span>]</span>).<span class="hljs-built_in">float</span><span class="hljs-literal">()</span>).data<br></code></pre></td></tr></table></figure><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记13-使用GPU训练模型</title>
    <link href="/2022/02/07/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-%E4%BD%BF%E7%94%A8GPU%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <url>/2022/02/07/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-%E4%BD%BF%E7%94%A8GPU%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="如题"><a href="#如题" class="headerlink" title="如题"></a>如题</h2><span id="more"></span><p>训练过程的耗时主要来自于两个部分，一部分来自数据准备，另一部分来自参数迭代。</p><p>当数据准备过程还是模型训练时间的主要瓶颈时，我们可以使用更多进程来准备数据。</p><p>当参数迭代过程成为训练时间的主要瓶颈时，我们通常的方法是应用GPU来进行加速。</p><p>Pytorch中使用GPU加速模型非常简单，只要将模型和数据移动到GPU上。核心代码只有以下几行。</p><figure class="highlight monkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs monkey"><span class="hljs-meta"># 定义模型  </span><br>...   <br><br>device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)  <br>model.<span class="hljs-keyword">to</span>(device)<span class="hljs-meta"> # 移动模型到cuda  </span><span class="hljs-meta"></span><br><span class="hljs-meta"></span><br><span class="hljs-meta"># 训练模型  </span><br>...  <br><br>features = features.<span class="hljs-keyword">to</span>(device)<span class="hljs-meta"> # 移动数据到cuda  </span><br>labels = labels.<span class="hljs-keyword">to</span>(device)<span class="hljs-meta"> # 或者  labels = labels.cuda() <span class="hljs-meta-keyword">if</span> torch.cuda.is_available() <span class="hljs-meta-keyword">else</span> labels  </span><br>...<br></code></pre></td></tr></table></figure><p>如果要使用多个GPU训练模型，也非常简单。只需要在将模型设置为数据并行风格模型。<br>则模型移动到GPU上之后，会在每一个GPU上拷贝一个副本，并把数据平分到各个GPU上进行训练。核心代码如下。</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs maxima"># 定义模型  <br>...   <br><br><span class="hljs-keyword">if</span> torch.cuda.device_count() &gt; <span class="hljs-number">1</span>:  <br>    model = nn.DataParallel(model) # 包装为并行风格模型  <br><br># 训练模型  <br>...  <br><span class="hljs-built_in">features</span> = <span class="hljs-built_in">features</span>.to(device) # 移动数据到cuda  <br><span class="hljs-built_in">labels</span> = <span class="hljs-built_in">labels</span>.to(device) # 或者 <span class="hljs-built_in">labels</span> = <span class="hljs-built_in">labels</span>.cuda() <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-built_in">labels</span>  <br></code></pre></td></tr></table></figure><h4 id="一些和GPU有关的基本操作汇总"><a href="#一些和GPU有关的基本操作汇总" class="headerlink" title="一些和GPU有关的基本操作汇总"></a>一些和GPU有关的基本操作汇总</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch <br><span class="hljs-keyword">from</span> torch import nn <br><br><span class="hljs-comment"># 1，查看gpu信息</span><br>if_cuda = torch.cuda.is_available()<br><span class="hljs-builtin-name">print</span>(<span class="hljs-string">&quot;if_cuda=&quot;</span>,if_cuda)<br><br>gpu_count = torch.cuda.device_count()<br><span class="hljs-builtin-name">print</span>(<span class="hljs-string">&quot;gpu_count=&quot;</span>,gpu_count)<br><br><br><span class="hljs-comment"># 2，将张量在gpu和cpu间移动</span><br>tensor = torch.rand((100,100))<br>tensor_gpu = tensor.<span class="hljs-keyword">to</span>(<span class="hljs-string">&quot;cuda:0&quot;</span>) # 或者 tensor_gpu = tensor.cuda()<br><span class="hljs-builtin-name">print</span>(tensor_gpu.device)<br><span class="hljs-builtin-name">print</span>(tensor_gpu.is_cuda)<br><br>tensor_cpu = tensor_gpu.<span class="hljs-keyword">to</span>(<span class="hljs-string">&quot;cpu&quot;</span>) # 或者 tensor_cpu = tensor_gpu.cpu() <br><span class="hljs-builtin-name">print</span>(tensor_cpu.device)<br><br><span class="hljs-comment"># 3，将模型中的全部张量移动到gpu上</span><br>net = nn.Linear(2,1)<br><span class="hljs-builtin-name">print</span>(next(net.parameters()).is_cuda)<br>net.<span class="hljs-keyword">to</span>(<span class="hljs-string">&quot;cuda:0&quot;</span>) # 将模型中的全部参数张量依次到GPU上，注意，无需重新赋值为 net = net.<span class="hljs-keyword">to</span>(<span class="hljs-string">&quot;cuda:0&quot;</span>)<br><span class="hljs-comment">#查看模型是否已经移动到GPU上</span><br><span class="hljs-builtin-name">print</span>(<span class="hljs-string">&quot;if on cuda:&quot;</span>,next(net.parameters()).is_cuda)<br><span class="hljs-comment">#print(next(net.parameters()).is_cuda)</span><br><span class="hljs-builtin-name">print</span>(next(net.parameters()).device)<br><br><span class="hljs-comment"># 4，创建支持多个gpu数据并行的模型</span><br>linear = nn.Linear(2,1)<br><span class="hljs-builtin-name">print</span>(next(linear.parameters()).device)<br><br>model = nn.DataParallel(linear)<br><span class="hljs-builtin-name">print</span>(model.device_ids)<br><span class="hljs-builtin-name">print</span>(next(model.module.parameters()).device) <br><br><span class="hljs-comment">#注意保存参数时要指定保存model.module的参数</span><br>torch.save(model.module.state_dict(), <span class="hljs-string">&quot;model_parameter.pkl&quot;</span>) <br><br>linear = nn.Linear(2,1)<br>linear.load_state_dict(torch.load(<span class="hljs-string">&quot;model_parameter.pkl&quot;</span>)) <br><br><br><span class="hljs-comment"># 5，清空cuda缓存</span><br><br><span class="hljs-comment"># 该方法在cuda超内存时十分有用</span><br>torch.cuda.empty_cache()<br></code></pre></td></tr></table></figure><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记12-训练模型的3种方法</title>
    <link href="/2022/02/07/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%843%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
    <url>/2022/02/07/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%843%E7%A7%8D%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="如题"><a href="#如题" class="headerlink" title="如题"></a>如题</h2><span id="more"></span><h4 id="〇，准备数据"><a href="#〇，准备数据" class="headerlink" title="〇，准备数据"></a>〇，准备数据</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch <br><span class="hljs-keyword">from</span> torch import nn <br><span class="hljs-keyword">from</span> torchkeras import summary,Model <br><br>import torchvision <br><span class="hljs-keyword">from</span> torchvision import transforms<br><br>transform = transforms.Compose([transforms.ToTensor()])<br><br>ds_train = torchvision.datasets.MNIST(<span class="hljs-attribute">root</span>=<span class="hljs-string">&quot;/home/kesci/input/data6936/data/minist/&quot;</span>,train=True,download=True,transform=transform)<br>ds_valid = torchvision.datasets.MNIST(<span class="hljs-attribute">root</span>=<span class="hljs-string">&quot;/home/kesci/input/data6936/data/minist/&quot;</span>,train=False,download=True,transform=transform)<br><br>dl_train =  torch.utils.data.DataLoader(ds_train, <span class="hljs-attribute">batch_size</span>=128, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">num_workers</span>=4)<br>dl_valid =  torch.utils.data.DataLoader(ds_valid, <span class="hljs-attribute">batch_size</span>=128, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">num_workers</span>=4)<br><br><span class="hljs-builtin-name">print</span>(len(ds_train))<br><span class="hljs-builtin-name">print</span>(len(ds_valid))<br></code></pre></td></tr></table></figure><figure class="highlight sas"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs sas"><span class="hljs-name">%matplotlib</span> inline<br><span class="hljs-name">%config</span> InlineBackend.figure_format = <span class="hljs-string">&#x27;svg&#x27;</span><br><br>#查看部分样本<br><span class="hljs-meta">from</span> matplotlib import pyplot <span class="hljs-meta">as</span> plt <br><br>plt.figure(figsize=(8,8)) <br>for i <span class="hljs-meta">in</span><span class="hljs-meta"> range(</span>9):<br>    img,<span class="hljs-meta">label</span> = ds_train[i]<br>    img = torch.squeeze(img)<br>    ax=plt.subplot(3,3,i+1)<br>    ax.imshow(img.numpy())<br>    ax.set_title(<span class="hljs-string">&quot;label = %d&quot;</span><span class="hljs-built_in">%label</span>)<br>    ax.set_xticks([])<br>    ax.set_yticks([]) <br>plt.show()<br></code></pre></td></tr></table></figure><h4 id="一，脚本风格"><a href="#一，脚本风格" class="headerlink" title="一，脚本风格"></a>一，脚本风格</h4><p>脚本风格的训练循环最为常见。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">net = nn.<span class="hljs-constructor">Sequential()</span><br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;conv1&quot;</span>,<span class="hljs-params">nn</span>.Conv2d(<span class="hljs-params">in_channels</span>=1,<span class="hljs-params">out_channels</span>=32,<span class="hljs-params">kernel_size</span> = 3)</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;pool1&quot;</span>,<span class="hljs-params">nn</span>.MaxPool2d(<span class="hljs-params">kernel_size</span> = 2,<span class="hljs-params">stride</span> = 2)</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;conv2&quot;</span>,<span class="hljs-params">nn</span>.Conv2d(<span class="hljs-params">in_channels</span>=32,<span class="hljs-params">out_channels</span>=64,<span class="hljs-params">kernel_size</span> = 5)</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;pool2&quot;</span>,<span class="hljs-params">nn</span>.MaxPool2d(<span class="hljs-params">kernel_size</span> = 2,<span class="hljs-params">stride</span> = 2)</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;dropout&quot;</span>,<span class="hljs-params">nn</span>.Dropout2d(<span class="hljs-params">p</span> = 0.1)</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;adaptive_pool&quot;</span>,<span class="hljs-params">nn</span>.AdaptiveMaxPool2d((1,1)</span>))<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;flatten&quot;</span>,<span class="hljs-params">nn</span>.Flatten()</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;linear1&quot;</span>,<span class="hljs-params">nn</span>.Linear(64,32)</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;relu&quot;</span>,<span class="hljs-params">nn</span>.ReLU()</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;linear2&quot;</span>,<span class="hljs-params">nn</span>.Linear(32,10)</span>)<br><br>print(net)<br></code></pre></td></tr></table></figure><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> datetime<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br><span class="hljs-title">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><br><span class="hljs-title">def</span> accuracy(y_pred,y_true):<br>    y_pred_cls = torch.argmax(nn.<span class="hljs-type">Softmax</span>(dim=<span class="hljs-number">1</span>)(y_pred),dim=<span class="hljs-number">1</span>).<span class="hljs-class"><span class="hljs-keyword">data</span></span><br>    return accuracy_score(y_true,y_pred_cls)<br><br><span class="hljs-title">loss_func</span> = nn.<span class="hljs-type">CrossEntropyLoss</span>()<br><span class="hljs-title">optimizer</span> = torch.optim.<span class="hljs-type">Adam</span>(params=net.parameters(),lr = <span class="hljs-number">0.01</span>)<br><span class="hljs-title">metric_func</span> = accuracy<br><span class="hljs-title">metric_name</span> = <span class="hljs-string">&quot;accuracy&quot;</span><br></code></pre></td></tr></table></figure><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs maxima">epochs = <span class="hljs-number">3</span><br>log_step_freq = <span class="hljs-number">100</span><br><br>dfhistory = pd.DataFrame(<span class="hljs-built_in">columns</span> = [<span class="hljs-string">&quot;epoch&quot;</span>,<span class="hljs-string">&quot;loss&quot;</span>,metric_name,<span class="hljs-string">&quot;val_loss&quot;</span>,<span class="hljs-string">&quot;val_&quot;</span>+metric_name]) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Start Training...&quot;</span>)<br>nowtime = datetime.datetime.now().strftime(&#x27;%Y-<span class="hljs-built_in">%m</span>-%d %H:%M:%S&#x27;)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;==========&quot;</span>*<span class="hljs-number">8</span> + <span class="hljs-string">&quot;%s&quot;</span>%nowtime)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,epochs+<span class="hljs-number">1</span>):  <br><br>    # <span class="hljs-number">1</span>，训练循环-------------------------------------------------<br>    net.train()<br>    loss_sum = <span class="hljs-number">0.0</span><br>    metric_sum = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">step</span> = <span class="hljs-number">1</span><br>    <br>    <span class="hljs-keyword">for</span> <span class="hljs-keyword">step</span>, (<span class="hljs-built_in">features</span>,<span class="hljs-built_in">labels</span>) <span class="hljs-keyword">in</span> enumerate(dl_train, <span class="hljs-number">1</span>):<br>    <br>        # 梯度清零<br>        optimizer.zero_grad()<br><br>        # 正向传播求损失<br>        predictions = net(<span class="hljs-built_in">features</span>)<br>        loss = loss_func(predictions,<span class="hljs-built_in">labels</span>)<br>        metric = metric_func(predictions,<span class="hljs-built_in">labels</span>)<br>        <br>        # 反向传播求梯度<br>        loss.backward()<br>        optimizer.<span class="hljs-keyword">step</span>()<br><br>        # 打印<span class="hljs-built_in">batch</span>级别日志<br>        loss_sum += loss.item()<br>        metric_sum += metric.item()<br>        <span class="hljs-keyword">if</span> step%log_step_freq == <span class="hljs-number">0</span>:   <br>            <span class="hljs-built_in">print</span>((<span class="hljs-string">&quot;[step = %d] loss: %.3f, &quot;</span>+metric_name+<span class="hljs-string">&quot;: %.3f&quot;</span>) <span class="hljs-symbol">%</span><br>                  (<span class="hljs-keyword">step</span>, loss_sum/<span class="hljs-keyword">step</span>, metric_sum/<span class="hljs-keyword">step</span>))<br>            <br>    # <span class="hljs-number">2</span>，验证循环-------------------------------------------------<br>    net.<span class="hljs-built_in">eval</span>()<br>    val_loss_sum = <span class="hljs-number">0.0</span><br>    val_metric_sum = <span class="hljs-number">0.0</span><br>    val_step = <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">for</span> val_step, (<span class="hljs-built_in">features</span>,<span class="hljs-built_in">labels</span>) <span class="hljs-keyword">in</span> enumerate(dl_valid, <span class="hljs-number">1</span>):<br>        with torch.no_grad():<br>            predictions = net(<span class="hljs-built_in">features</span>)<br>            val_loss = loss_func(predictions,<span class="hljs-built_in">labels</span>)<br>            val_metric = metric_func(predictions,<span class="hljs-built_in">labels</span>)<br><br>        val_loss_sum += val_loss.item()<br>        val_metric_sum += val_metric.item()<br><br>    # <span class="hljs-number">3</span>，记录日志-------------------------------------------------<br>    info = (epoch, loss_sum/<span class="hljs-keyword">step</span>, metric_sum/<span class="hljs-keyword">step</span>, <br>            val_loss_sum/val_step, val_metric_sum/val_step)<br>    dfhistory.loc[epoch-<span class="hljs-number">1</span>] = info<br>    <br>    # 打印epoch级别日志<br>    <span class="hljs-built_in">print</span>((<span class="hljs-string">&quot;\nEPOCH = %d, loss = %.3f,&quot;</span>+ metric_name + \<br>          <span class="hljs-string">&quot;  = %.3f, val_loss = %.3f, &quot;</span>+<span class="hljs-string">&quot;val_&quot;</span>+ metric_name+<span class="hljs-string">&quot; = %.3f&quot;</span>) <br>          %info)<br>    nowtime = datetime.datetime.now().strftime(&#x27;%Y-<span class="hljs-built_in">%m</span>-%d %H:%M:%S&#x27;)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n&quot;</span>+<span class="hljs-string">&quot;==========&quot;</span>*<span class="hljs-number">8</span> + <span class="hljs-string">&quot;%s&quot;</span>%nowtime)<br>        <br><span class="hljs-built_in">print</span>(&#x27;Finished Training...&#x27;)<br></code></pre></td></tr></table></figure><h4 id="二，函数风格"><a href="#二，函数风格" class="headerlink" title="二，函数风格"></a>二，函数风格</h4><p>该风格在脚本形式上作了简单的函数封装。</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">Net</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">Net</span>, <span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">        self.layers = nn.<span class="hljs-type">ModuleList</span>([</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">in_channels</span>=1,<span class="hljs-title">out_channels</span>=32,<span class="hljs-title">kernel_size</span> = 3),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(<span class="hljs-title">kernel_size</span> = 2,<span class="hljs-title">stride</span> = 2),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">in_channels</span>=32,<span class="hljs-title">out_channels</span>=64,<span class="hljs-title">kernel_size</span> = 5),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(<span class="hljs-title">kernel_size</span> = 2,<span class="hljs-title">stride</span> = 2),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Dropout2d</span>(<span class="hljs-title">p</span> = 0.1),</span><br><span class="hljs-class">            nn.<span class="hljs-type">AdaptiveMaxPool2d</span>((1,1)),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Flatten</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(64,32),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(32,10)]</span><br><span class="hljs-class">        )</span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>,<span class="hljs-title">x</span>):</span><br><span class="hljs-class">        for layer in self.layers:</span><br><span class="hljs-class">            x = layer(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        return x</span><br><span class="hljs-class">net = <span class="hljs-type">Net</span>()</span><br><span class="hljs-class">print(<span class="hljs-title">net</span>)</span><br></code></pre></td></tr></table></figure><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> datetime<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br><span class="hljs-title">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><br><span class="hljs-title">def</span> accuracy(y_pred,y_true):<br>    y_pred_cls = torch.argmax(nn.<span class="hljs-type">Softmax</span>(dim=<span class="hljs-number">1</span>)(y_pred),dim=<span class="hljs-number">1</span>).<span class="hljs-class"><span class="hljs-keyword">data</span></span><br>    return accuracy_score(y_true,y_pred_cls)<br><br><span class="hljs-title">model</span> = net<br><span class="hljs-title">model</span>.optimizer = torch.optim.<span class="hljs-type">SGD</span>(model.parameters(),lr = <span class="hljs-number">0.01</span>)<br><span class="hljs-title">model</span>.loss_func = nn.<span class="hljs-type">CrossEntropyLoss</span>()<br><span class="hljs-title">model</span>.metric_func = accuracy<br><span class="hljs-title">model</span>.metric_name = <span class="hljs-string">&quot;accuracy&quot;</span><br></code></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">def train<span class="hljs-constructor">_step(<span class="hljs-params">model</span>,<span class="hljs-params">features</span>,<span class="hljs-params">labels</span>)</span>:<br>    <br>    # 训练模式，dropout层发生作用<br>    model.train<span class="hljs-literal">()</span><br>    <br>    # 梯度清零<br>    model.optimizer.zero<span class="hljs-constructor">_grad()</span><br>    <br>    # 正向传播求损失<br>    predictions = model(features)<br>    loss = model.loss<span class="hljs-constructor">_func(<span class="hljs-params">predictions</span>,<span class="hljs-params">labels</span>)</span><br>    metric = model.metric<span class="hljs-constructor">_func(<span class="hljs-params">predictions</span>,<span class="hljs-params">labels</span>)</span><br><br>    # 反向传播求梯度<br>    loss.backward<span class="hljs-literal">()</span><br>    model.optimizer.step<span class="hljs-literal">()</span><br><br>    return loss.item<span class="hljs-literal">()</span>,metric.item<span class="hljs-literal">()</span><br><br>@torch.no<span class="hljs-constructor">_grad()</span><br>def valid<span class="hljs-constructor">_step(<span class="hljs-params">model</span>,<span class="hljs-params">features</span>,<span class="hljs-params">labels</span>)</span>:<br>    <br>    # 预测模式，dropout层不发生作用<br>    model.eval<span class="hljs-literal">()</span><br>    <br>    predictions = model(features)<br>    loss = model.loss<span class="hljs-constructor">_func(<span class="hljs-params">predictions</span>,<span class="hljs-params">labels</span>)</span><br>    metric = model.metric<span class="hljs-constructor">_func(<span class="hljs-params">predictions</span>,<span class="hljs-params">labels</span>)</span><br>    <br>    return loss.item<span class="hljs-literal">()</span>, metric.item<span class="hljs-literal">()</span><br><br><br># 测试train_step效果<br>features,labels = next(iter(dl_train))<br>train<span class="hljs-constructor">_step(<span class="hljs-params">model</span>,<span class="hljs-params">features</span>,<span class="hljs-params">labels</span>)</span><br></code></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def train_model(model,epochs,dl_train,dl_valid,log_step_freq):<br><br>    metric_name = model.metric_name<br>    dfhistory = pd.DataFrame(columns = [<span class="hljs-string">&quot;epoch&quot;</span>,<span class="hljs-string">&quot;loss&quot;</span>,metric_name,<span class="hljs-string">&quot;val_loss&quot;</span>,<span class="hljs-string">&quot;val_&quot;</span>+metric_name]) <br>    <span class="hljs-builtin-name">print</span>(<span class="hljs-string">&quot;Start Training...&quot;</span>)<br>    nowtime = datetime.datetime.now().strftime(<span class="hljs-string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)<br>    <span class="hljs-builtin-name">print</span>(<span class="hljs-string">&quot;==========&quot;</span><span class="hljs-number">*8</span> + <span class="hljs-string">&quot;%s&quot;</span>%nowtime)<br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(1,epochs+1):  <br><br>        # 1，训练循环-------------------------------------------------<br>        loss_sum = 0.0<br>        metric_sum = 0.0<br>        <span class="hljs-keyword">step</span> = 1<br><br>        <span class="hljs-keyword">for</span> <span class="hljs-keyword">step</span>, (features,labels) <span class="hljs-keyword">in</span> enumerate(dl_train, 1):<br><br>            loss,metric = train_step(model,features,labels)<br><br>            # 打印batch级别日志<br>            loss_sum += loss<br>            metric_sum += metric<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">step</span>%log_step_freq == 0:   <br>                <span class="hljs-builtin-name">print</span>((<span class="hljs-string">&quot;[step = %d] loss: %.3f, &quot;</span>+metric_name+<span class="hljs-string">&quot;: %.3f&quot;</span>) %<br>                      (<span class="hljs-keyword">step</span>, loss_sum/<span class="hljs-keyword">step</span>, metric_sum/<span class="hljs-keyword">step</span>))<br><br>        # 2，验证循环-------------------------------------------------<br>        val_loss_sum = 0.0<br>        val_metric_sum = 0.0<br>        val_step = 1<br><br>        <span class="hljs-keyword">for</span> val_step, (features,labels) <span class="hljs-keyword">in</span> enumerate(dl_valid, 1):<br><br>            val_loss,val_metric = valid_step(model,features,labels)<br><br>            val_loss_sum += val_loss<br>            val_metric_sum += val_metric<br><br>        # 3，记录日志-------------------------------------------------<br>        <span class="hljs-builtin-name">info</span> = (epoch, loss_sum/<span class="hljs-keyword">step</span>, metric_sum/<span class="hljs-keyword">step</span>, <br>                val_loss_sum/val_step, val_metric_sum/val_step)<br>        dfhistory.loc[epoch-1] = <span class="hljs-builtin-name">info</span><br><br>        # 打印epoch级别日志<br>        <span class="hljs-builtin-name">print</span>((<span class="hljs-string">&quot;\nEPOCH = %d, loss = %.3f,&quot;</span>+ metric_name + \<br>              <span class="hljs-string">&quot;  = %.3f, val_loss = %.3f, &quot;</span>+<span class="hljs-string">&quot;val_&quot;</span>+ metric_name+<span class="hljs-string">&quot; = %.3f&quot;</span>) <br>              %info)<br>        nowtime = datetime.datetime.now().strftime(<span class="hljs-string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)<br>        <span class="hljs-builtin-name">print</span>(<span class="hljs-string">&quot;\n&quot;</span>+<span class="hljs-string">&quot;==========&quot;</span><span class="hljs-number">*8</span> + <span class="hljs-string">&quot;%s&quot;</span>%nowtime)<br><br>    <span class="hljs-builtin-name">print</span>(<span class="hljs-string">&#x27;Finished Training...&#x27;</span>)<br>    return dfhistory<br></code></pre></td></tr></table></figure><h4 id="三，类风格"><a href="#三，类风格" class="headerlink" title="三，类风格"></a>三，类风格</h4><p>此处使用torchkeras中定义的模型接口构建模型，并调用compile方法和fit方法训练模型。</p><p>使用该形式训练模型非常简洁明了。推荐使用该形式。</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> torchkeras <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">CnnModel</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super().__init__()</span><br><span class="hljs-class">        self.layers = nn.<span class="hljs-type">ModuleList</span>([</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">in_channels</span>=1,<span class="hljs-title">out_channels</span>=32,<span class="hljs-title">kernel_size</span> = 3),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(<span class="hljs-title">kernel_size</span> = 2,<span class="hljs-title">stride</span> = 2),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">in_channels</span>=32,<span class="hljs-title">out_channels</span>=64,<span class="hljs-title">kernel_size</span> = 5),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(<span class="hljs-title">kernel_size</span> = 2,<span class="hljs-title">stride</span> = 2),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Dropout2d</span>(<span class="hljs-title">p</span> = 0.1),</span><br><span class="hljs-class">            nn.<span class="hljs-type">AdaptiveMaxPool2d</span>((1,1)),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Flatten</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(64,32),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(32,10)]</span><br><span class="hljs-class">        )</span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>,<span class="hljs-title">x</span>):</span><br><span class="hljs-class">        for layer in self.layers:</span><br><span class="hljs-class">            x = layer(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        return x</span><br><span class="hljs-class">model = torchkeras.<span class="hljs-type">Model</span>(<span class="hljs-type">CnnModel</span>())</span><br><span class="hljs-class">print(<span class="hljs-title">model</span>)</span><br></code></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">from</span> sklearn<span class="hljs-selector-class">.metrics</span> import accuracy_score<br><br>def accuracy(y_pred,y_true):<br>    y_pred_cls = torch.<span class="hljs-built_in">argmax</span>(nn.<span class="hljs-built_in">Softmax</span>(dim=<span class="hljs-number">1</span>)(y_pred),dim=<span class="hljs-number">1</span>).data<br>    return <span class="hljs-built_in">accuracy_score</span>(y_true.<span class="hljs-built_in">numpy</span>(),y_pred_cls.<span class="hljs-built_in">numpy</span>())<br><br>model.<span class="hljs-built_in">compile</span>(loss_func = nn.<span class="hljs-built_in">CrossEntropyLoss</span>(),<br>             optimizer= torch.optim.<span class="hljs-built_in">Adam</span>(model.<span class="hljs-built_in">parameters</span>(),lr = <span class="hljs-number">0.02</span>),<br>             metrics_dict=&#123;<span class="hljs-string">&quot;accuracy&quot;</span>:accuracy&#125;)<br></code></pre></td></tr></table></figure><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-attr">dfhistory</span> = model.fit(<span class="hljs-number">3</span>,<span class="hljs-attr">dl_train</span> = dl_train, <span class="hljs-attr">dl_val=dl_valid,</span> <span class="hljs-attr">log_step_freq=100)</span> <br></code></pre></td></tr></table></figure><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记11-构建模型的3种方法</title>
    <link href="/2022/02/07/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B%E7%9A%843%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
    <url>/2022/02/07/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B%E7%9A%843%E7%A7%8D%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="如题"><a href="#如题" class="headerlink" title="如题"></a>如题</h2><span id="more"></span><p>可以使用以下3种方式构建模型：</p><ul><li><p>1，继承nn.Module基类构建自定义模型。</p></li><li><p>2，使用nn.Sequential按层顺序构建模型。</p></li><li><p>3，继承nn.Module基类构建模型并辅助应用模型容器进行封装(nn.Sequential,nn.ModuleList,nn.ModuleDict)。</p></li></ul><p>其中 第1种方式最为常见，第2种方式最简单，第3种方式最为灵活也较为复杂。</p><p>推荐使用第1种方式构建模型。</p><h4 id="一，继承nn-Module基类构建自定义模型"><a href="#一，继承nn-Module基类构建自定义模型" class="headerlink" title="一，继承nn.Module基类构建自定义模型"></a>一，继承nn.Module基类构建自定义模型</h4><p>以下是继承nn.Module基类构建自定义模型的一个范例。模型中的用到的层一般在__init__函数中定义，然后在forward方法中定义模型的正向传播逻辑。</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> torch <br><span class="hljs-title">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-title">from</span> torchkeras <span class="hljs-keyword">import</span> summary<br><span class="hljs-class"></span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">Net</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    </span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">Net</span>, <span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">        self.conv1 = nn.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">in_channels</span>=3,<span class="hljs-title">out_channels</span>=32,<span class="hljs-title">kernel_size</span> = 3)</span><br><span class="hljs-class">        self.pool1 = nn.<span class="hljs-type">MaxPool2d</span>(<span class="hljs-title">kernel_size</span> = 2,<span class="hljs-title">stride</span> = 2)</span><br><span class="hljs-class">        self.conv2 = nn.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">in_channels</span>=32,<span class="hljs-title">out_channels</span>=64,<span class="hljs-title">kernel_size</span> = 5)</span><br><span class="hljs-class">        self.pool2 = nn.<span class="hljs-type">MaxPool2d</span>(<span class="hljs-title">kernel_size</span> = 2,<span class="hljs-title">stride</span> = 2)</span><br><span class="hljs-class">        self.dropout = nn.<span class="hljs-type">Dropout2d</span>(<span class="hljs-title">p</span> = 0.1)</span><br><span class="hljs-class">        self.adaptive_pool = nn.<span class="hljs-type">AdaptiveMaxPool2d</span>((1,1))</span><br><span class="hljs-class">        self.flatten = nn.<span class="hljs-type">Flatten</span>()</span><br><span class="hljs-class">        self.linear1 = nn.<span class="hljs-type">Linear</span>(64,32)</span><br><span class="hljs-class">        self.relu = nn.<span class="hljs-type">ReLU</span>()</span><br><span class="hljs-class">        self.linear2 = nn.<span class="hljs-type">Linear</span>(32,1)</span><br><span class="hljs-class">        self.sigmoid = nn.<span class="hljs-type">Sigmoid</span>()</span><br><span class="hljs-class">        </span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>,<span class="hljs-title">x</span>):</span><br><span class="hljs-class">        x = self.conv1(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.pool1(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.conv2(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.pool2(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.dropout(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.adaptive_pool(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.flatten(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.linear1(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.relu(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.linear2(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        y = self.sigmoid(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        return y</span><br><span class="hljs-class">        </span><br><span class="hljs-class">net = <span class="hljs-type">Net</span>()</span><br><span class="hljs-class">print(<span class="hljs-title">net</span>)</span><br></code></pre></td></tr></table></figure><h4 id="二，使用nn-Sequential按层顺序构建模型"><a href="#二，使用nn-Sequential按层顺序构建模型" class="headerlink" title="二，使用nn.Sequential按层顺序构建模型"></a>二，使用nn.Sequential按层顺序构建模型</h4><p>使用nn.Sequential按层顺序构建模型无需定义forward方法。仅仅适合于简单的模型。</p><p>以下是使用nn.Sequential搭建模型的一些等价方法。</p><h5 id="1，利用add-module方法"><a href="#1，利用add-module方法" class="headerlink" title="1，利用add_module方法"></a>1，利用add_module方法</h5><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">net = nn.<span class="hljs-constructor">Sequential()</span><br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;conv1&quot;</span>,<span class="hljs-params">nn</span>.Conv2d(<span class="hljs-params">in_channels</span>=3,<span class="hljs-params">out_channels</span>=32,<span class="hljs-params">kernel_size</span> = 3)</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;pool1&quot;</span>,<span class="hljs-params">nn</span>.MaxPool2d(<span class="hljs-params">kernel_size</span> = 2,<span class="hljs-params">stride</span> = 2)</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;conv2&quot;</span>,<span class="hljs-params">nn</span>.Conv2d(<span class="hljs-params">in_channels</span>=32,<span class="hljs-params">out_channels</span>=64,<span class="hljs-params">kernel_size</span> = 5)</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;pool2&quot;</span>,<span class="hljs-params">nn</span>.MaxPool2d(<span class="hljs-params">kernel_size</span> = 2,<span class="hljs-params">stride</span> = 2)</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;dropout&quot;</span>,<span class="hljs-params">nn</span>.Dropout2d(<span class="hljs-params">p</span> = 0.1)</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;adaptive_pool&quot;</span>,<span class="hljs-params">nn</span>.AdaptiveMaxPool2d((1,1)</span>))<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;flatten&quot;</span>,<span class="hljs-params">nn</span>.Flatten()</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;linear1&quot;</span>,<span class="hljs-params">nn</span>.Linear(64,32)</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;relu&quot;</span>,<span class="hljs-params">nn</span>.ReLU()</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;linear2&quot;</span>,<span class="hljs-params">nn</span>.Linear(32,1)</span>)<br>net.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;sigmoid&quot;</span>,<span class="hljs-params">nn</span>.Sigmoid()</span>)<br><br>print(net)<br></code></pre></td></tr></table></figure><h5 id="2，利用变长参数"><a href="#2，利用变长参数" class="headerlink" title="2，利用变长参数"></a>2，利用变长参数</h5><p>这种方式构建时不能给每个层指定名称。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs stylus">net = nn<span class="hljs-selector-class">.Sequential</span>(<br>    nn<span class="hljs-selector-class">.Conv2d</span>(in_channels=<span class="hljs-number">3</span>,out_channels=<span class="hljs-number">32</span>,kernel_size = <span class="hljs-number">3</span>),<br>    nn<span class="hljs-selector-class">.MaxPool2d</span>(kernel_size = <span class="hljs-number">2</span>,stride = <span class="hljs-number">2</span>),<br>    nn<span class="hljs-selector-class">.Conv2d</span>(in_channels=<span class="hljs-number">32</span>,out_channels=<span class="hljs-number">64</span>,kernel_size = <span class="hljs-number">5</span>),<br>    nn<span class="hljs-selector-class">.MaxPool2d</span>(kernel_size = <span class="hljs-number">2</span>,stride = <span class="hljs-number">2</span>),<br>    nn<span class="hljs-selector-class">.Dropout2d</span>(<span class="hljs-selector-tag">p</span> = <span class="hljs-number">0.1</span>),<br>    nn<span class="hljs-selector-class">.AdaptiveMaxPool2d</span>((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>    nn<span class="hljs-selector-class">.Flatten</span>(),<br>    nn<span class="hljs-selector-class">.Linear</span>(<span class="hljs-number">64</span>,<span class="hljs-number">32</span>),<br>    nn<span class="hljs-selector-class">.ReLU</span>(),<br>    nn<span class="hljs-selector-class">.Linear</span>(<span class="hljs-number">32</span>,<span class="hljs-number">1</span>),<br>    nn<span class="hljs-selector-class">.Sigmoid</span>()<br>)<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(net)</span></span><br></code></pre></td></tr></table></figure><h5 id="3，利用OrderedDict"><a href="#3，利用OrderedDict" class="headerlink" title="3，利用OrderedDict"></a>3，利用OrderedDict</h5><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs vim">from collections import OrderedDict<br><br>net = <span class="hljs-keyword">nn</span>.Sequential(OrderedDict(<br>          [(<span class="hljs-string">&quot;conv1&quot;</span>,<span class="hljs-keyword">nn</span>.Conv2d(in_channels=<span class="hljs-number">3</span>,out_channels=<span class="hljs-number">32</span>,kernel_size = <span class="hljs-number">3</span>)),<br>            (<span class="hljs-string">&quot;pool1&quot;</span>,<span class="hljs-keyword">nn</span>.MaxPool2d(kernel_size = <span class="hljs-number">2</span>,stride = <span class="hljs-number">2</span>)),<br>            (<span class="hljs-string">&quot;conv2&quot;</span>,<span class="hljs-keyword">nn</span>.Conv2d(in_channels=<span class="hljs-number">32</span>,out_channels=<span class="hljs-number">64</span>,kernel_size = <span class="hljs-number">5</span>)),<br>            (<span class="hljs-string">&quot;pool2&quot;</span>,<span class="hljs-keyword">nn</span>.MaxPool2d(kernel_size = <span class="hljs-number">2</span>,stride = <span class="hljs-number">2</span>)),<br>            (<span class="hljs-string">&quot;dropout&quot;</span>,<span class="hljs-keyword">nn</span>.Dropout2d(<span class="hljs-keyword">p</span> = <span class="hljs-number">0.1</span>)),<br>            (<span class="hljs-string">&quot;adaptive_pool&quot;</span>,<span class="hljs-keyword">nn</span>.AdaptiveMaxPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))),<br>            (<span class="hljs-string">&quot;flatten&quot;</span>,<span class="hljs-keyword">nn</span>.Flatten()),<br>            (<span class="hljs-string">&quot;linear1&quot;</span>,<span class="hljs-keyword">nn</span>.Linear(<span class="hljs-number">64</span>,<span class="hljs-number">32</span>)),<br>            (<span class="hljs-string">&quot;relu&quot;</span>,<span class="hljs-keyword">nn</span>.ReLU()),<br>            (<span class="hljs-string">&quot;linear2&quot;</span>,<span class="hljs-keyword">nn</span>.Linear(<span class="hljs-number">32</span>,<span class="hljs-number">1</span>)),<br>            (<span class="hljs-string">&quot;sigmoid&quot;</span>,<span class="hljs-keyword">nn</span>.Sigmoid())<br>          ])<br>        )<br><span class="hljs-keyword">print</span>(net)<br></code></pre></td></tr></table></figure><h4 id="三，继承nn-Module基类构建模型并辅助应用模型容器进行封装"><a href="#三，继承nn-Module基类构建模型并辅助应用模型容器进行封装" class="headerlink" title="三，继承nn.Module基类构建模型并辅助应用模型容器进行封装"></a>三，继承nn.Module基类构建模型并辅助应用模型容器进行封装</h4><p>当模型的结构比较复杂时，我们可以应用模型容器(nn.Sequential,nn.ModuleList,nn.ModuleDict)对模型的部分结构进行封装。</p><p>这样做会让模型整体更加有层次感，有时候也能减少代码量。</p><p>注意，在下面的范例中我们每次仅仅使用一种模型容器，但实际上这些模型容器的使用是非常灵活的，可以在一个模型中任意组合任意嵌套使用。</p><h5 id="1，nn-Sequential作为模型容器"><a href="#1，nn-Sequential作为模型容器" class="headerlink" title="1，nn.Sequential作为模型容器"></a>1，nn.Sequential作为模型容器</h5><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">Net</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    </span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">Net</span>, <span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">        self.conv = nn.<span class="hljs-type">Sequential</span>(</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">in_channels</span>=3,<span class="hljs-title">out_channels</span>=32,<span class="hljs-title">kernel_size</span> = 3),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(<span class="hljs-title">kernel_size</span> = 2,<span class="hljs-title">stride</span> = 2),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">in_channels</span>=32,<span class="hljs-title">out_channels</span>=64,<span class="hljs-title">kernel_size</span> = 5),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(<span class="hljs-title">kernel_size</span> = 2,<span class="hljs-title">stride</span> = 2),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Dropout2d</span>(<span class="hljs-title">p</span> = 0.1),</span><br><span class="hljs-class">            nn.<span class="hljs-type">AdaptiveMaxPool2d</span>((1,1))</span><br><span class="hljs-class">        )</span><br><span class="hljs-class">        self.dense = nn.<span class="hljs-type">Sequential</span>(</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Flatten</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(64,32),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(32,1),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Sigmoid</span>()</span><br><span class="hljs-class">        )</span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>,<span class="hljs-title">x</span>):</span><br><span class="hljs-class">        x = self.conv(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        y = self.dense(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        return y </span><br><span class="hljs-class">    </span><br><span class="hljs-class">net = <span class="hljs-type">Net</span>()</span><br><span class="hljs-class">print(<span class="hljs-title">net</span>)</span><br></code></pre></td></tr></table></figure><h5 id="2，nn-ModuleList作为模型容器"><a href="#2，nn-ModuleList作为模型容器" class="headerlink" title="2，nn.ModuleList作为模型容器"></a>2，nn.ModuleList作为模型容器</h5><p>注意下面中的ModuleList不能用Python中的列表代替。</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">Net</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    </span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">Net</span>, <span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">        self.layers = nn.<span class="hljs-type">ModuleList</span>([</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">in_channels</span>=3,<span class="hljs-title">out_channels</span>=32,<span class="hljs-title">kernel_size</span> = 3),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(<span class="hljs-title">kernel_size</span> = 2,<span class="hljs-title">stride</span> = 2),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">in_channels</span>=32,<span class="hljs-title">out_channels</span>=64,<span class="hljs-title">kernel_size</span> = 5),</span><br><span class="hljs-class">            nn.<span class="hljs-type">MaxPool2d</span>(<span class="hljs-title">kernel_size</span> = 2,<span class="hljs-title">stride</span> = 2),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Dropout2d</span>(<span class="hljs-title">p</span> = 0.1),</span><br><span class="hljs-class">            nn.<span class="hljs-type">AdaptiveMaxPool2d</span>((1,1)),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Flatten</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(64,32),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(32,1),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Sigmoid</span>()]</span><br><span class="hljs-class">        )</span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>,<span class="hljs-title">x</span>):</span><br><span class="hljs-class">        for layer in self.layers:</span><br><span class="hljs-class">            x = layer(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        return x</span><br><span class="hljs-class">net = <span class="hljs-type">Net</span>()</span><br><span class="hljs-class">print(<span class="hljs-title">net</span>)</span><br></code></pre></td></tr></table></figure><h5 id="3，nn-ModuleDict作为模型容器"><a href="#3，nn-ModuleDict作为模型容器" class="headerlink" title="3，nn.ModuleDict作为模型容器"></a>3，nn.ModuleDict作为模型容器</h5><p>注意下面中的ModuleDict不能用Python中的字典代替。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs css">class Net(nn<span class="hljs-selector-class">.Module</span>):<br>    <br>    def <span class="hljs-built_in">__init__</span>(self):<br>        <span class="hljs-built_in">super</span>(Net, self).<span class="hljs-built_in">__init__</span>()<br>        self.layers_dict = nn.<span class="hljs-built_in">ModuleDict</span>(&#123;<span class="hljs-string">&quot;conv1&quot;</span>:nn.<span class="hljs-built_in">Conv2d</span>(in_channels=<span class="hljs-number">3</span>,out_channels=<span class="hljs-number">32</span>,kernel_size = <span class="hljs-number">3</span>),<br>               <span class="hljs-string">&quot;pool&quot;</span>: nn.<span class="hljs-built_in">MaxPool2d</span>(kernel_size = <span class="hljs-number">2</span>,stride = <span class="hljs-number">2</span>),<br>               <span class="hljs-string">&quot;conv2&quot;</span>:nn.<span class="hljs-built_in">Conv2d</span>(in_channels=<span class="hljs-number">32</span>,out_channels=<span class="hljs-number">64</span>,kernel_size = <span class="hljs-number">5</span>),<br>               <span class="hljs-string">&quot;dropout&quot;</span>: nn.<span class="hljs-built_in">Dropout2d</span>(p = <span class="hljs-number">0.1</span>),<br>               <span class="hljs-string">&quot;adaptive&quot;</span>:nn.<span class="hljs-built_in">AdaptiveMaxPool2d</span>((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>               <span class="hljs-string">&quot;flatten&quot;</span>: nn.<span class="hljs-built_in">Flatten</span>(),<br>               <span class="hljs-string">&quot;linear1&quot;</span>: nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">64</span>,<span class="hljs-number">32</span>),<br>               <span class="hljs-string">&quot;relu&quot;</span>:nn.<span class="hljs-built_in">ReLU</span>(),<br>               <span class="hljs-string">&quot;linear2&quot;</span>: nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">32</span>,<span class="hljs-number">1</span>),<br>               <span class="hljs-string">&quot;sigmoid&quot;</span>: nn.<span class="hljs-built_in">Sigmoid</span>()<br>              &#125;)<br>    def forward(self,x):<br>        layers = [<span class="hljs-string">&quot;conv1&quot;</span>,<span class="hljs-string">&quot;pool&quot;</span>,<span class="hljs-string">&quot;conv2&quot;</span>,<span class="hljs-string">&quot;pool&quot;</span>,<span class="hljs-string">&quot;dropout&quot;</span>,<span class="hljs-string">&quot;adaptive&quot;</span>,<br>                  <span class="hljs-string">&quot;flatten&quot;</span>,<span class="hljs-string">&quot;linear1&quot;</span>,<span class="hljs-string">&quot;relu&quot;</span>,<span class="hljs-string">&quot;linear2&quot;</span>,<span class="hljs-string">&quot;sigmoid&quot;</span>]<br>        for layer in layers:<br>            x = self.layers_dict[layer](x)<br>        return x<br>net = <span class="hljs-built_in">Net</span>()<br><span class="hljs-built_in">print</span>(net)<br></code></pre></td></tr></table></figure><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记10-TensorBoard可视化</title>
    <link href="/2022/01/29/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <url>/2022/01/29/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="如题"><a href="#如题" class="headerlink" title="如题"></a>如题</h2><span id="more"></span><p>Pytorch中利用TensorBoard可视化的大概过程如下：</p><p>首先在Pytorch中指定一个目录创建一个torch.utils.tensorboard.SummaryWriter日志写入器。</p><p>然后根据需要可视化的信息，利用日志写入器将相应信息日志写入我们指定的目录。</p><p>最后就可以传入日志目录作为参数启动TensorBoard，然后就可以在TensorBoard中愉快地看片了。</p><p>我们主要介绍Pytorch中利用TensorBoard进行如下方面信息的可视化的方法。</p><ul><li><p>可视化模型结构： writer.add_graph</p></li><li><p>可视化指标变化： writer.add_scalar</p></li><li><p>可视化参数分布： writer.add_histogram</p></li><li><p>可视化原始图像： writer.add_image 或 writer.add_images</p></li><li><p>可视化人工绘图： writer.add_figure</p></li></ul><h4 id="可视化模型结构"><a href="#可视化模型结构" class="headerlink" title="可视化模型结构"></a>可视化模型结构</h4><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> torch <br><span class="hljs-title">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-title">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><span class="hljs-title">from</span> torchkeras <span class="hljs-keyword">import</span> Model,summary<br><span class="hljs-class"></span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">Net</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    </span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">Net</span>, <span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">        self.conv1 = nn.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">in_channels</span>=3,<span class="hljs-title">out_channels</span>=32,<span class="hljs-title">kernel_size</span> = 3)</span><br><span class="hljs-class">        self.pool = nn.<span class="hljs-type">MaxPool2d</span>(<span class="hljs-title">kernel_size</span> = 2,<span class="hljs-title">stride</span> = 2)</span><br><span class="hljs-class">        self.conv2 = nn.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">in_channels</span>=32,<span class="hljs-title">out_channels</span>=64,<span class="hljs-title">kernel_size</span> = 5)</span><br><span class="hljs-class">        self.dropout = nn.<span class="hljs-type">Dropout2d</span>(<span class="hljs-title">p</span> = 0.1)</span><br><span class="hljs-class">        self.adaptive_pool = nn.<span class="hljs-type">AdaptiveMaxPool2d</span>((1,1))</span><br><span class="hljs-class">        self.flatten = nn.<span class="hljs-type">Flatten</span>()</span><br><span class="hljs-class">        self.linear1 = nn.<span class="hljs-type">Linear</span>(64,32)</span><br><span class="hljs-class">        self.relu = nn.<span class="hljs-type">ReLU</span>()</span><br><span class="hljs-class">        self.linear2 = nn.<span class="hljs-type">Linear</span>(32,1)</span><br><span class="hljs-class">        self.sigmoid = nn.<span class="hljs-type">Sigmoid</span>()</span><br><span class="hljs-class">        </span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>,<span class="hljs-title">x</span>):</span><br><span class="hljs-class">        x = self.conv1(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.pool(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.conv2(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.pool(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.dropout(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.adaptive_pool(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.flatten(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.linear1(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.relu(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = self.linear2(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        y = self.sigmoid(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        return y</span><br><span class="hljs-class">        </span><br><span class="hljs-class">net = <span class="hljs-type">Net</span>()</span><br><span class="hljs-class">print(<span class="hljs-title">net</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class">writer = <span class="hljs-type">SummaryWriter</span>(&#x27;./<span class="hljs-title">data</span>/<span class="hljs-title">tensorboard&#x27;</span>)</span><br><span class="hljs-class">writer.add_graph(<span class="hljs-title">net</span>,<span class="hljs-title">input_to_model</span> = <span class="hljs-title">torch</span>.<span class="hljs-title">rand</span>(1,3,32,32))</span><br><span class="hljs-class">writer.close()</span><br></code></pre></td></tr></table></figure><figure class="highlight cos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cos"><span class="hljs-built_in">%load</span>_ext tensorboard<br>#<span class="hljs-built_in">%tensorboard</span> --logdir ./data/tensorboard<br></code></pre></td></tr></table></figure><figure class="highlight capnproto"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs capnproto"><span class="hljs-keyword">from</span> tensorboard <span class="hljs-keyword">import</span> notebook<br><span class="hljs-comment">#查看启动的tensorboard程序</span><br>notebook.list() <br></code></pre></td></tr></table></figure><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs vala"><span class="hljs-meta">#启动tensorboard程序</span><br>notebook.start(<span class="hljs-string">&quot;--logdir ./data/tensorboard&quot;</span>)<br><span class="hljs-meta">#等价于在命令行中执行 tensorboard --logdir ./data/tensorboard</span><br><span class="hljs-meta">#可以在浏览器中打开 http://localhost:6006/ 查看</span><br></code></pre></td></tr></table></figure><h4 id="可视化指标变化"><a href="#可视化指标变化" class="headerlink" title="可视化指标变化"></a>可视化指标变化</h4><p>有时候在训练过程中，如果能够实时动态地查看loss和各种metric的变化曲线，那么无疑可以帮助我们更加直观地了解模型的训练情况。</p><p>注意，writer.add_scalar仅能对标量的值的变化进行可视化。因此它一般用于对loss和metric的变化进行可视化分析。</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-built_in">import</span> numpy as np <br><span class="hljs-built_in">import</span> torch <br>from torch.utils.tensorboard <span class="hljs-built_in">import</span> SummaryWriter<br><br><span class="hljs-comment"># f(x) = a*x**2 + b*x + c的最小值</span><br><span class="hljs-attr">x</span> = torch.tensor(<span class="hljs-number">0.0</span>,<span class="hljs-attr">requires_grad</span> = True) <span class="hljs-comment"># x需要被求导</span><br><span class="hljs-attr">a</span> = torch.tensor(<span class="hljs-number">1.0</span>)<br><span class="hljs-attr">b</span> = torch.tensor(-<span class="hljs-number">2.0</span>)<br><span class="hljs-attr">c</span> = torch.tensor(<span class="hljs-number">1.0</span>)<br><br><span class="hljs-attr">optimizer</span> = torch.optim.SGD(<span class="hljs-attr">params=[x],lr</span> = <span class="hljs-number">0.01</span>)<br><br><br>def f(x):<br>    <span class="hljs-attr">result</span> = a*torch.pow(x,<span class="hljs-number">2</span>) + b*x + c <br>    return(result)<br><br><span class="hljs-attr">writer</span> = SummaryWriter(&#x27;./data/tensorboard&#x27;)<br>for i <span class="hljs-keyword">in</span> range(<span class="hljs-number">500</span>):<br>    optimizer.zero_grad()<br>    <span class="hljs-attr">y</span> = f(x)<br>    y.backward()<br>    optimizer.step()<br>    writer.add_scalar(<span class="hljs-string">&quot;x&quot;</span>,x.item(),i) <span class="hljs-comment">#日志中记录x在第step i 的值</span><br>    writer.add_scalar(<span class="hljs-string">&quot;y&quot;</span>,y.item(),i) <span class="hljs-comment">#日志中记录y在第step i 的值</span><br><br>writer.close()<br>    <br>print(<span class="hljs-string">&quot;y=&quot;</span>,f(x).data,<span class="hljs-string">&quot;;&quot;</span>,<span class="hljs-string">&quot;x=&quot;</span>,x.data)<br></code></pre></td></tr></table></figure><h4 id="可视化参数分布"><a href="#可视化参数分布" class="headerlink" title="可视化参数分布"></a>可视化参数分布</h4><p>如果需要对模型的参数(一般非标量)在训练过程中的变化进行可视化，可以使用 writer.add_histogram。</p><p>它能够观测张量值分布的直方图随训练步骤的变化趋势。</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs maxima">import numpy as <span class="hljs-built_in">np</span> <br>import torch <br>from torch.utils.tensorboard import SummaryWriter<br><br># 创建正态分布的张量模拟参数矩阵<br>def norm(<span class="hljs-built_in">mean</span>,<span class="hljs-built_in">std</span>):<br>    t = <span class="hljs-built_in">std</span>*torch.randn((<span class="hljs-number">100</span>,<span class="hljs-number">20</span>))+<span class="hljs-built_in">mean</span><br>    <span class="hljs-built_in">return</span> t<br><br>writer = SummaryWriter(&#x27;./data/tensorboard&#x27;)<br><span class="hljs-keyword">for</span> <span class="hljs-keyword">step</span>,<span class="hljs-built_in">mean</span> <span class="hljs-keyword">in</span> enumerate(<span class="hljs-built_in">range</span>(-<span class="hljs-number">10</span>,<span class="hljs-number">10</span>,<span class="hljs-number">1</span>)):<br>    w = norm(<span class="hljs-built_in">mean</span>,<span class="hljs-number">1</span>)<br>    writer.add_histogram(<span class="hljs-string">&quot;w&quot;</span>,w, <span class="hljs-keyword">step</span>)<br>    writer.<span class="hljs-built_in">flush</span>()<br>writer.<span class="hljs-built_in">close</span>()<br></code></pre></td></tr></table></figure><h4 id="可视化原始图像"><a href="#可视化原始图像" class="headerlink" title="可视化原始图像"></a>可视化原始图像</h4><p>如果我们做图像相关的任务，也可以将原始的图片在tensorboard中进行可视化展示。</p><p>如果只写入一张图片信息，可以使用writer.add_image。</p><p>如果要写入多张图片信息，可以使用writer.add_images。</p><p>也可以用 torchvision.utils.make_grid将多张图片拼成一张图片，然后用writer.add_image写入。</p><p>注意，传入的是代表图片信息的Pytorch中的张量数据。</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs angelscript"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset,DataLoader<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms,datasets <br><br><br>transform_train = transforms.Compose(<br><span class="hljs-string">    [transforms.ToTensor()]</span>)<br>transform_valid = transforms.Compose(<br><span class="hljs-string">    [transforms.ToTensor()]</span>)<br></code></pre></td></tr></table></figure><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-attr">ds_train</span> = datasets.ImageFolder(<span class="hljs-string">&quot;/home/kesci/input/data6936/data/cifar2/train/&quot;</span>,<br>            <span class="hljs-attr">transform</span> = transform_train,<span class="hljs-attr">target_transform=</span> lambda t:torch.tensor([t]).float())<br><span class="hljs-attr">ds_valid</span> = datasets.ImageFolder(<span class="hljs-string">&quot;/home/kesci/input/data6936/data/cifar2/test/&quot;</span>,<br>            <span class="hljs-attr">transform</span> = transform_train,<span class="hljs-attr">target_transform=</span> lambda t:torch.tensor([t]).float())<br><br>print(ds_train.class_to_idx)<br><br><span class="hljs-attr">dl_train</span> = DataLoader(ds_train,<span class="hljs-attr">batch_size</span> = <span class="hljs-number">50</span>,<span class="hljs-attr">shuffle</span> = True,<span class="hljs-attr">num_workers=3)</span><br><span class="hljs-attr">dl_valid</span> = DataLoader(ds_valid,<span class="hljs-attr">batch_size</span> = <span class="hljs-number">50</span>,<span class="hljs-attr">shuffle</span> = True,<span class="hljs-attr">num_workers=3)</span><br><br><span class="hljs-attr">dl_train_iter</span> = iter(dl_train)<br>images, <span class="hljs-attr">labels</span> = dl_train_iter.next()<br><br><span class="hljs-comment"># 仅查看一张图片</span><br><span class="hljs-attr">writer</span> = SummaryWriter(&#x27;/home/kesci/input/data6936/data/tensorboard&#x27;)<br>writer.add_image(&#x27;images[<span class="hljs-number">0</span>]&#x27;, images[<span class="hljs-number">0</span>])<br>writer.close()<br><br><span class="hljs-comment"># 将多张图片拼接成一张图片，中间用黑色网格分割</span><br><span class="hljs-attr">writer</span> = SummaryWriter(&#x27;/home/kesci/input/data6936/data/tensorboard&#x27;)<br><span class="hljs-comment"># create grid of images</span><br><span class="hljs-attr">img_grid</span> = torchvision.utils.make_grid(images)<br>writer.add_image(&#x27;image_grid&#x27;, img_grid)<br>writer.close()<br><br><span class="hljs-comment"># 将多张图片直接写入</span><br><span class="hljs-attr">writer</span> = SummaryWriter(&#x27;/home/kesci/input/data6936/data/tensorboard&#x27;)<br>writer.add_images(<span class="hljs-string">&quot;images&quot;</span>,images,<span class="hljs-attr">global_step</span> = <span class="hljs-number">0</span>)<br>writer.close()<br></code></pre></td></tr></table></figure><h4 id="可视化人工绘图"><a href="#可视化人工绘图" class="headerlink" title="可视化人工绘图"></a>可视化人工绘图</h4><p>如果我们将matplotlib绘图的结果再 tensorboard中展示，可以使用 add_figure.</p><p>注意，和writer.add_image不同的是，writer.add_figure需要传入matplotlib的figure对象。</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-built_in">import</span> torch<br><span class="hljs-built_in">import</span> torchvision<br>from torch <span class="hljs-built_in">import</span> nn<br>from torch.utils.data <span class="hljs-built_in">import</span> Dataset,DataLoader<br>from torchvision <span class="hljs-built_in">import</span> transforms,datasets <br><br><br><span class="hljs-attr">transform_train</span> = transforms.Compose(<br>    [transforms.ToTensor()])<br><span class="hljs-attr">transform_valid</span> = transforms.Compose(<br>    [transforms.ToTensor()])<br><br><span class="hljs-attr">ds_train</span> = datasets.ImageFolder(<span class="hljs-string">&quot;/home/kesci/input/data6936/data/cifar2/train/&quot;</span>,<br>            <span class="hljs-attr">transform</span> = transform_train,<span class="hljs-attr">target_transform=</span> lambda t:torch.tensor([t]).float())<br><span class="hljs-attr">ds_valid</span> = datasets.ImageFolder(<span class="hljs-string">&quot;/home/kesci/input/data6936/data/cifar2/test/&quot;</span>,<br>            <span class="hljs-attr">transform</span> = transform_train,<span class="hljs-attr">target_transform=</span> lambda t:torch.tensor([t]).float())<br><br>print(ds_train.class_to_idx)<br></code></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">%matplotlib inline<br>%config <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">InlineBackend</span>.</span></span>figure_format = &#x27;svg&#x27;<br>from matplotlib import pyplot <span class="hljs-keyword">as</span> plt <br><br>figure = plt.figure(figsize=(<span class="hljs-number">8</span>,<span class="hljs-number">8</span>)) <br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):<br>    img,label = ds_train<span class="hljs-literal">[<span class="hljs-identifier">i</span>]</span><br>    img = img.permute(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>)<br>    ax=plt.subplot(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,i+<span class="hljs-number">1</span>)<br>    ax.imshow(img.numpy<span class="hljs-literal">()</span>)<br>    ax.set<span class="hljs-constructor">_title(<span class="hljs-string">&quot;label = %d&quot;</span>%<span class="hljs-params">label</span>.<span class="hljs-params">item</span>()</span>)<br>    ax.set<span class="hljs-constructor">_xticks([])</span><br>    ax.set<span class="hljs-constructor">_yticks([])</span> <br>plt.show<span class="hljs-literal">()</span><br><br>writer = <span class="hljs-constructor">SummaryWriter(&#x27;.<span class="hljs-operator">/</span><span class="hljs-params">data</span><span class="hljs-operator">/</span><span class="hljs-params">tensorboard</span>&#x27;)</span><br>writer.add<span class="hljs-constructor">_figure(&#x27;<span class="hljs-params">figure</span>&#x27;,<span class="hljs-params">figure</span>,<span class="hljs-params">global_step</span>=0)</span><br>writer.close<span class="hljs-literal">()</span><br></code></pre></td></tr></table></figure><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记9-损失函数</title>
    <link href="/2022/01/27/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    <url>/2022/01/27/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="各种损失函数"><a href="#各种损失函数" class="headerlink" title="各种损失函数"></a>各种损失函数</h2><span id="more"></span><p>一般来说，监督学习的目标函数由损失函数和正则化项组成。(Objective = Loss + Regularization)</p><p>Pytorch中的损失函数一般在训练模型时候指定。</p><p>注意Pytorch中内置的损失函数的参数和tensorflow不同，是y_pred在前，y_true在后，而Tensorflow是y_true在前，y_pred在后。</p><ul><li><p>对于回归模型，通常使用的内置损失函数是均方损失函数nn.MSELoss 。</p></li><li><p>对于二分类模型，通常使用的是二元交叉熵损失函数nn.BCELoss (输入已经是sigmoid激活函数之后的结果)或者 nn.BCEWithLogitsLoss (输入尚未经过nn.Sigmoid激活函数) 。</p></li><li><p>对于多分类模型，一般推荐使用交叉熵损失函数 nn.CrossEntropyLoss。(y_true需要是一维的，是类别编码。y_pred未经过nn.Softmax激活。)</p></li></ul><p>此外，如果多分类的y_pred经过了nn.LogSoftmax激活，可以使用nn.NLLLoss损失函数(The negative log likelihood loss)。<br>这种方法和直接使用nn.CrossEntropyLoss等价。</p><p>如果有需要，也可以自定义损失函数，自定义损失函数需要接收两个张量y_pred，y_true作为输入参数，并输出一个标量作为损失函数值。</p><p>Pytorch中的正则化项一般通过自定义的方式和损失函数一起添加作为目标函数。</p><p>如果仅仅使用L2正则化，也可以利用优化器的weight_decay参数来实现相同的效果。</p><h4 id="内置损失函数"><a href="#内置损失函数" class="headerlink" title="内置损失函数"></a>内置损失函数</h4><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">import</span> pandas as pd<br><span class="hljs-attribute">import</span> torch <br><span class="hljs-attribute">from</span> torch import nn <br><span class="hljs-attribute">import</span> torch.nn.functional as F <br><br><br><span class="hljs-attribute">y_pred</span> = torch.tensor([[<span class="hljs-number">10</span>.<span class="hljs-number">0</span>,<span class="hljs-number">0</span>.<span class="hljs-number">0</span>,-<span class="hljs-number">10</span>.<span class="hljs-number">0</span>],[<span class="hljs-number">8</span>.<span class="hljs-number">0</span>,<span class="hljs-number">8</span>.<span class="hljs-number">0</span>,<span class="hljs-number">8</span>.<span class="hljs-number">0</span>]])<br><span class="hljs-attribute">y_true</span> = torch.tensor([<span class="hljs-number">0</span>,<span class="hljs-number">2</span>])<br><br><span class="hljs-comment"># 直接调用交叉熵损失</span><br><span class="hljs-attribute">ce</span> = nn.CrossEntropyLoss()(y_pred,y_true)<br><span class="hljs-attribute">print</span>(ce)<br><br><span class="hljs-comment"># 等价于先计算nn.LogSoftmax激活，再调用NLLLoss</span><br><span class="hljs-attribute">y_pred_logsoftmax</span> = nn.LogSoftmax(dim = <span class="hljs-number">1</span>)(y_pred)<br><span class="hljs-attribute">nll</span> = nn.NLLLoss()(y_pred_logsoftmax,y_true)<br><span class="hljs-attribute">print</span>(nll)<br></code></pre></td></tr></table></figure><p>内置的损失函数一般有类的实现和函数的实现两种形式。</p><p>如：nn.BCE 和 F.binary_cross_entropy 都是二元交叉熵损失函数，前者是类的实现形式，后者是函数的实现形式。</p><p>实际上类的实现形式通常是调用函数的实现形式并用nn.Module封装后得到的。</p><p>一般我们常用的是类的实现形式。它们封装在torch.nn模块下，并且类名以Loss结尾。</p><p>常用的一些内置损失函数说明如下。</p><ul><li><p>nn.MSELoss（均方误差损失，也叫做L2损失，用于回归）</p></li><li><p>nn.L1Loss （L1损失，也叫做绝对值误差损失，用于回归）</p></li><li><p>nn.SmoothL1Loss (平滑L1损失，当输入在-1到1之间时，平滑为L2损失，用于回归)</p></li><li><p>nn.BCELoss (二元交叉熵，用于二分类，输入已经过nn.Sigmoid激活，对不平衡数据集可以用weigths参数调整类别权重)</p></li><li><p>nn.BCEWithLogitsLoss (二元交叉熵，用于二分类，输入未经过nn.Sigmoid激活)</p></li><li><p>nn.CrossEntropyLoss (交叉熵，用于多分类，要求label为稀疏编码，输入未经过nn.Softmax激活，对不平衡数据集可以用weigths参数调整类别权重)</p></li><li><p>nn.NLLLoss (负对数似然损失，用于多分类，要求label为稀疏编码，输入经过nn.LogSoftmax激活)</p></li><li><p>nn.CosineSimilarity(余弦相似度，可用于多分类)</p></li><li><p>nn.AdaptiveLogSoftmaxWithLoss (一种适合非常多类别且类别分布很不均衡的损失函数，会自适应地将多个小类别合成一个cluster)</p></li></ul><p>更多损失函数的介绍参考如下知乎文章：</p><p>《PyTorch的十八个损失函数》</p><ul><li><a href="https://zhuanlan.zhihu.com/p/61379965">https://zhuanlan.zhihu.com/p/61379965</a></li></ul><h4 id="自定义损失函数"><a href="#自定义损失函数" class="headerlink" title="自定义损失函数"></a>自定义损失函数</h4><p>自定义损失函数接收两个张量y_pred,y_true作为输入参数，并输出一个标量作为损失函数值。</p><p>也可以对nn.Module进行子类化，重写forward方法实现损失的计算逻辑，从而得到损失函数的类的实现。</p><p>下面是一个Focal Loss的自定义实现示范。Focal Loss是一种对binary_crossentropy的改进损失函数形式。</p><p>它在样本不均衡和存在较多易分类的样本时相比binary_crossentropy具有明显的优势。</p><p>它有两个可调参数，alpha参数和gamma参数。其中alpha参数主要用于衰减负样本的权重，gamma参数主要用于衰减容易训练样本的权重。</p><p>从而让模型更加聚焦在正样本和困难样本上。这就是为什么这个损失函数叫做Focal Loss。</p><p>详见《5分钟理解Focal Loss与GHM——解决样本不平衡利器》</p><ul><li><a href="https://zhuanlan.zhihu.com/p/80594704">https://zhuanlan.zhihu.com/p/80594704</a></li></ul><h4 id="自定义L1和L2正则化项"><a href="#自定义L1和L2正则化项" class="headerlink" title="自定义L1和L2正则化项"></a>自定义L1和L2正则化项</h4><p>通常认为L1 正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择。</p><p>而L2 正则化可以防止模型过拟合（overfitting）。一定程度上，L1也可以防止过拟合。</p><p>下面以一个二分类问题为例，演示给模型的目标函数添加自定义L1和L2正则化项的方法。</p><p>这个范例同时演示了上一个部分的FocalLoss的使用</p><p>示例参考 <a href="https://www.heywhale.com/mw/project/5f33d61caf3980002cb83d18">https://www.heywhale.com/mw/project/5f33d61caf3980002cb83d18</a></p><h4 id="通过优化器实现L2正则化"><a href="#通过优化器实现L2正则化" class="headerlink" title="通过优化器实现L2正则化"></a>通过优化器实现L2正则化</h4><p>如果仅仅需要使用L2正则化，那么也可以利用优化器的weight_decay参数来实现。</p><p>weight_decay参数可以设置参数在训练过程中的衰减，这和L2正则化的作用效果等价。</p><p>Pytorch的优化器支持一种称之为Per-parameter options的操作，就是对每一个参数进行特定的学习率，权重衰减率指定，以满足更为细致的要求。</p><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记8-模型层layers</title>
    <link href="/2022/01/24/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E6%A8%A1%E5%9E%8B%E5%B1%82layers/"/>
    <url>/2022/01/24/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E6%A8%A1%E5%9E%8B%E5%B1%82layers/</url>
    
    <content type="html"><![CDATA[<h2 id="深度学习模型一般由各种模型层组合而成"><a href="#深度学习模型一般由各种模型层组合而成" class="headerlink" title="深度学习模型一般由各种模型层组合而成"></a>深度学习模型一般由各种模型层组合而成</h2><span id="more"></span><p>torch.nn中内置了非常丰富的各种模型层。它们都属于nn.Module的子类，具备参数管理功能。<br>例如：</p><ul><li>nn.Linear, nn.Flatten, nn.Dropout, nn.BatchNorm2d</li><li>nn.Conv2d,nn.AvgPool2d,nn.Conv1d,nn.ConvTranspose2d</li><li>nn.Embedding,nn.GRU,nn.LSTM</li><li>nn.Transformer</li></ul><p>如果这些内置模型层不能够满足需求，我们也可以通过继承nn.Module基类构建自定义的模型层。<br>实际上，pytorch不区分模型和模型层，都是通过继承nn.Module进行构建。<br>因此，我们只要继承nn.Module基类并实现forward方法即可自定义模型层。</p><h4 id="内置模型层"><a href="#内置模型层" class="headerlink" title="内置模型层"></a>内置模型层</h4><p>一些常用的内置模型层简单介绍如下。</p><p>基础层</p><ul><li>nn.Linear：全连接层。参数个数 = 输入层特征数× 输出层特征数(weight)＋ 输出层特征数(bias)</li><li>nn.Flatten：压平层，用于将多维张量样本压成一维张量样本。</li><li>nn.BatchNorm1d：一维批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。可以用afine参数设置该层是否含有可以训练的参数。</li><li>nn.BatchNorm2d：二维批标准化层。</li><li>nn.BatchNorm3d：三维批标准化层。</li><li>nn.Dropout：一维随机丢弃层。一种正则化手段。</li><li>nn.Dropout2d：二维随机丢弃层。</li><li>nn.Dropout3d：三维随机丢弃层。</li><li>nn.Threshold：限幅层。当输入大于或小于阈值范围时，截断之。</li><li>nn.ConstantPad2d： 二维常数填充层。对二维张量样本填充常数扩展长度。</li><li>nn.ReplicationPad1d： 一维复制填充层。对一维张量样本通过复制边缘值填充扩展长度。</li><li>nn.ZeroPad2d：二维零值填充层。对二维张量样本在边缘填充0值.</li><li>nn.GroupNorm：组归一化。一种替代批归一化的方法，将通道分成若干组进行归一。不受batch大小限制，据称性能和效果都优于BatchNorm。</li><li>nn.LayerNorm：层归一化。较少使用。</li><li>nn.InstanceNorm2d: 样本归一化。较少使用。</li></ul><p>各种归一化技术参考如下知乎文章《FAIR何恺明等人提出组归一化：替代批归一化，不受批量大小限制》</p><ul><li><a href="https://zhuanlan.zhihu.com/p/34858971">https://zhuanlan.zhihu.com/p/34858971</a></li></ul><p>卷积网络相关层</p><ul><li><p>nn.Conv1d：普通一维卷积，常用于文本。参数个数 = 输入通道数×卷积核尺寸(如3)×卷积核个数 + 卷积核尺寸(如3）</p></li><li><p>nn.Conv2d：普通二维卷积，常用于图像。参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数 + 卷积核尺寸(如3乘3)<br>通过调整dilation参数大于1，可以变成空洞卷积，增大卷积核感受野。<br>通过调整groups参数不为1，可以变成分组卷积。分组卷积中不同分组使用相同的卷积核，显著减少参数数量。<br>当groups参数等于通道数时，相当于tensorflow中的二维深度卷积层tf.keras.layers.DepthwiseConv2D。<br>利用分组卷积和1乘1卷积的组合操作，可以构造相当于Keras中的二维深度可分离卷积层tf.keras.layers.SeparableConv2D。</p></li><li><p>nn.Conv3d：普通三维卷积，常用于视频。参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数 + 卷积核尺寸(如3乘3乘3) 。</p></li><li><p>nn.MaxPool1d: 一维最大池化。</p></li><li><p>nn.MaxPool2d：二维最大池化。一种下采样方式。没有需要训练的参数。</p></li><li><p>nn.MaxPool3d：三维最大池化。</p></li><li><p>nn.AdaptiveMaxPool2d：二维自适应最大池化。无论输入图像的尺寸如何变化，输出的图像尺寸是固定的。</p></li><li><p>该函数的实现原理，大概是通过输入图像的尺寸和要得到的输出图像的尺寸来反向推算池化算子的padding,stride等参数。</p></li><li><p>nn.FractionalMaxPool2d：二维分数最大池化。普通最大池化通常输入尺寸是输出的整数倍。而分数最大池化则可以不必是整数。分数最大池化使用了一些随机采样策略，有一定的正则效果，可以用它来代替普通最大池化和Dropout层。</p></li><li><p>nn.AvgPool2d：二维平均池化。</p></li><li><p>nn.AdaptiveAvgPool2d：二维自适应平均池化。无论输入的维度如何变化，输出的维度是固定的。</p></li><li><p>nn.ConvTranspose2d：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。在语义分割中可用于上采样。</p></li><li><p>nn.Upsample：上采样层，操作效果和池化相反。可以通过mode参数控制上采样策略为”nearest”最邻近策略或”linear”线性插值策略。</p></li><li><p>nn.Unfold：滑动窗口提取层。其参数和卷积操作nn.Conv2d相同。实际上，卷积操作可以等价于nn.Unfold和nn.Linear以及nn.Fold的一个组合。<br>其中nn.Unfold操作可以从输入中提取各个滑动窗口的数值矩阵，并将其压平成一维。利用nn.Linear将nn.Unfold的输出和卷积核做乘法后，再使用</p></li><li><p>nn.Fold操作将结果转换成输出图片形状。</p></li><li><p>nn.Fold：逆滑动窗口提取层。</p></li></ul><p>循环网络相关层</p><ul><li>nn.Embedding：嵌入层。一种比Onehot更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。</li><li>nn.LSTM：长短记忆循环网络层【支持多层】。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置bidirectional = True时可以得到双向LSTM。需要注意的时，默认的输入和输出形状是(seq,batch,feature), 如果需要将batch维度放在第0维，则要设置batch_first参数设置为True。</li><li>nn.GRU：门控循环网络层【支持多层】。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快。</li><li>nn.RNN：简单循环网络层【支持多层】。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用。</li><li>nn.LSTMCell：长短记忆循环网络单元。和nn.LSTM在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。</li><li>nn.GRUCell：门控循环网络单元。和nn.GRU在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。</li><li>nn.RNNCell：简单循环网络单元。和nn.RNN在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。</li></ul><p>Transformer相关层</p><ul><li>nn.Transformer：Transformer网络结构。Transformer网络结构是替代循环网络的一种结构，解决了循环网络难以并行，难以捕捉长期依赖的缺陷。它是目前NLP任务的主流模型的主要构成部分。Transformer网络结构由TransformerEncoder编码器和TransformerDecoder解码器组成。编码器和解码器的核心是MultiheadAttention多头注意力层。</li><li>nn.TransformerEncoder：Transformer编码器结构。由多个 nn.TransformerEncoderLayer编码器层组成。</li><li>nn.TransformerDecoder：Transformer解码器结构。由多个 nn.TransformerDecoderLayer解码器层组成。</li><li>nn.TransformerEncoderLayer：Transformer的编码器层。</li><li>nn.TransformerDecoderLayer：Transformer的解码器层。</li><li>nn.MultiheadAttention：多头注意力层。</li></ul><p>Transformer原理介绍可以参考如下知乎文章《详解Transformer(Attention Is All You Need)》</p><ul><li><a href="https://zhuanlan.zhihu.com/p/48508221">https://zhuanlan.zhihu.com/p/48508221</a></li></ul><h4 id="自定义模型层"><a href="#自定义模型层" class="headerlink" title="自定义模型层"></a>自定义模型层</h4><p>下面是Pytorch的nn.Linear层的源码，可以仿照它来自定义模型层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Linear</span>(<span class="hljs-params">nn.Module</span>):</span><br>    __constants__ = [<span class="hljs-string">&#x27;in_features&#x27;</span>, <span class="hljs-string">&#x27;out_features&#x27;</span>]<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, in_features, out_features, bias=<span class="hljs-literal">True</span></span>):</span><br>        <span class="hljs-built_in">super</span>(Linear, self).__init__()<br>        self.in_features = in_features<br>        self.out_features = out_features<br>        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))<br>        <span class="hljs-keyword">if</span> bias:<br>            self.bias = nn.Parameter(torch.Tensor(out_features))<br>        <span class="hljs-keyword">else</span>:<br>            self.register_parameter(<span class="hljs-string">&#x27;bias&#x27;</span>, <span class="hljs-literal">None</span>)<br>        self.reset_parameters()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reset_parameters</span>(<span class="hljs-params">self</span>):</span><br>        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(<span class="hljs-number">5</span>))<br>        <span class="hljs-keyword">if</span> self.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)<br>            bound = <span class="hljs-number">1</span> / math.sqrt(fan_in)<br>            nn.init.uniform_(self.bias, -bound, bound)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):</span><br>        <span class="hljs-keyword">return</span> F.linear(<span class="hljs-built_in">input</span>, self.weight, self.bias)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">extra_repr</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;in_features=&#123;&#125;, out_features=&#123;&#125;, bias=&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>            self.in_features, self.out_features, self.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        )<br><br><br>linear = nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">30</span>)<br>inputs = torch.randn(<span class="hljs-number">128</span>, <span class="hljs-number">20</span>)<br>output = linear(inputs)<br><span class="hljs-built_in">print</span>(output.size())<br></code></pre></td></tr></table></figure><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记7-Dataset和DataLoader</title>
    <link href="/2022/01/21/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-Dataset%E5%92%8CDataLoader/"/>
    <url>/2022/01/21/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-Dataset%E5%92%8CDataLoader/</url>
    
    <content type="html"><![CDATA[<h2 id="Pytorch通常使用Dataset和DataLoader这两个工具类来构建数据管道"><a href="#Pytorch通常使用Dataset和DataLoader这两个工具类来构建数据管道" class="headerlink" title="Pytorch通常使用Dataset和DataLoader这两个工具类来构建数据管道"></a>Pytorch通常使用Dataset和DataLoader这两个工具类来构建数据管道</h2><span id="more"></span><p>Dataset定义了数据集的内容，它相当于一个类似列表的数据结构，具有确定的长度，能够用索引获取数据集中的元素。<br>而DataLoader定义了按batch加载数据集的方法，它是一个实现了__iter_方法的可迭代对象，每次迭代输出一个batch的数据。<br>DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。<br>在绝大部分情况下，用户只需实现Dataset的__len__方法和__getitem__方法，就可以轻松构建自己的数据集，并用默认数据管道进行加载。</p><h5 id="Dataset和DataLoader概述"><a href="#Dataset和DataLoader概述" class="headerlink" title="Dataset和DataLoader概述"></a>Dataset和DataLoader概述</h5><h6 id="1，获取一个batch数据的步骤"><a href="#1，获取一个batch数据的步骤" class="headerlink" title="1，获取一个batch数据的步骤"></a>1，获取一个batch数据的步骤</h6><p>(假定数据集的特征和标签分别表示为张量X和Y，数据集可以表示为(X,Y), 假定batch大小为m)</p><p>1，首先我们要确定数据集的长度n。</p><p>结果类似：n = 1000。</p><p>2，然后我们从0到n-1的范围中抽样出m个数(batch大小)。</p><p>假定m=4, 拿到的结果是一个列表，类似：indices = [1,4,8,9]</p><p>3，接着我们从数据集中去取这m个数对应下标的元素。</p><p>拿到的结果是一个元组列表，类似：samples = [(X[1],Y[1]),(X[4],Y[4]),(X[8],Y[8]),(X[9],Y[9])]</p><p>4，最后我们将结果整理成两个张量作为输出。</p><p>拿到的结果是两个张量，类似batch = (features,labels)，</p><p>其中 features = torch.stack([X[1],X[4],X[8],X[9]])</p><p>labels = torch.stack([Y[1],Y[4],Y[8],Y[9]])</p><h6 id="2，Dataset和DataLoader的功能分工"><a href="#2，Dataset和DataLoader的功能分工" class="headerlink" title="2，Dataset和DataLoader的功能分工"></a>2，Dataset和DataLoader的功能分工</h6><p>上述第1个步骤确定数据集的长度是由 Dataset的__len__ 方法实现的。</p><p>第2个步骤从0到n-1的范围中抽样出m个数的方法是由 DataLoader的 sampler和 batch_sampler参数指定的。</p><p>sampler参数指定单个元素抽样方法，一般无需用户设置，程序默认在DataLoader的参数shuffle=True时采用随机抽样，shuffle=False时采用顺序抽样。</p><p>batch_sampler参数将多个抽样的元素整理成一个列表，一般无需用户设置，默认方法在DataLoader的参数drop_last=True时会丢弃数据集最后一个长度不能被batch大小整除的批次，在drop_last=False时保留最后一个批次。</p><p>第3个步骤的核心逻辑根据下标取数据集中的元素 是由 Dataset的 __getitem__方法实现的。</p><p>第4个步骤的逻辑由DataLoader的参数collate_fn指定。一般情况下也无需用户设置。</p><h6 id="3，Dataset和DataLoader的主要接口"><a href="#3，Dataset和DataLoader的主要接口" class="headerlink" title="3，Dataset和DataLoader的主要接口"></a>3，Dataset和DataLoader的主要接口</h6><p>以下是 Dataset和 DataLoader的核心接口逻辑伪代码，不完全和源码一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Dataset</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">pass</span><br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">raise</span> NotImplementedError<br>        <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span>(<span class="hljs-params">self,index</span>):</span><br>        <span class="hljs-keyword">raise</span> NotImplementedError<br>        <br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DataLoader</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,dataset,batch_size,collate_fn,shuffle = <span class="hljs-literal">True</span>,drop_last = <span class="hljs-literal">False</span></span>):</span><br>        self.dataset = dataset<br>        self.sampler =torch.utils.data.RandomSampler <span class="hljs-keyword">if</span> shuffle <span class="hljs-keyword">else</span> \<br>           torch.utils.data.SequentialSampler<br>        self.batch_sampler = torch.utils.data.BatchSampler<br>        self.sample_iter = self.batch_sampler(<br>            self.sampler(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(dataset))),<br>            batch_size = batch_size,drop_last = drop_last)<br>        <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__next__</span>(<span class="hljs-params">self</span>):</span><br>        indices = <span class="hljs-built_in">next</span>(self.sample_iter)<br>        batch = self.collate_fn([self.dataset[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> indices])<br>        <span class="hljs-keyword">return</span> batch<br></code></pre></td></tr></table></figure><h5 id="使用Dataset创建数据集"><a href="#使用Dataset创建数据集" class="headerlink" title="使用Dataset创建数据集"></a>使用Dataset创建数据集</h5><p>Dataset创建数据集常用的方法有：</p><ul><li><p>使用 torch.utils.data.TensorDataset 根据Tensor创建数据集(numpy的array，Pandas的DataFrame需要先转换成Tensor)。</p></li><li><p>使用 torchvision.datasets.ImageFolder 根据图片目录创建图片数据集。</p></li><li><p>继承 torch.utils.data.Dataset 创建自定义数据集。</p></li></ul><p>此外，还可以通过</p><ul><li><p>torch.utils.data.random_split 将一个数据集分割成多份，常用于分割训练集，验证集和测试集。</p></li><li><p>调用Dataset的加法运算符(+)将多个数据集合并成一个数据集。</p></li></ul><h6 id="1，根据Tensor创建数据集"><a href="#1，根据Tensor创建数据集" class="headerlink" title="1，根据Tensor创建数据集"></a>1，根据Tensor创建数据集</h6><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br><span class="hljs-keyword">import</span> torch <br><span class="hljs-title">from</span> torch.utils.data <span class="hljs-keyword">import</span> TensorDataset,Dataset,DataLoader,random_split <br></code></pre></td></tr></table></figure><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># 根据Tensor创建数据集</span><br><br><span class="hljs-title">from</span> sklearn <span class="hljs-keyword">import</span> datasets <br><span class="hljs-title">iris</span> = datasets.load_iris()<br><span class="hljs-title">ds_iris</span> = <span class="hljs-type">TensorDataset</span>(torch.tensor(iris.<span class="hljs-class"><span class="hljs-keyword">data</span>),torch.tensor(<span class="hljs-title">iris</span>.<span class="hljs-title">target</span>))</span><br><br><span class="hljs-meta"># 分割成训练集和预测集</span><br><span class="hljs-title">n_train</span> = int(len(ds_iris)*<span class="hljs-number">0.8</span>)<br><span class="hljs-title">n_valid</span> = len(ds_iris) - n_train<br><span class="hljs-title">ds_train</span>,ds_valid = random_split(ds_iris,[n_train,n_valid])<br><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">type</span>(<span class="hljs-title">ds_iris</span>))</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">type</span>(<span class="hljs-title">ds_train</span>))</span><br></code></pre></td></tr></table></figure><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs maxima"># 使用DataLoader加载数据集<br>dl_train,dl_valid = DataLoader(ds_train,batch_size = <span class="hljs-number">8</span>),DataLoader(ds_valid,batch_size = <span class="hljs-number">8</span>)<br><br><span class="hljs-keyword">for</span> <span class="hljs-built_in">features</span>,<span class="hljs-built_in">labels</span> <span class="hljs-keyword">in</span> dl_train:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">features</span>,<span class="hljs-built_in">labels</span>)<br>    <span class="hljs-built_in">break</span><br></code></pre></td></tr></table></figure><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs go"># 演示加法运算符（<span class="hljs-string">`+`</span>）的合并作用<br>ds_data = ds_train + ds_valid<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;len(ds_train) = &#x27;</span>,<span class="hljs-built_in">len</span>(ds_train))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;len(ds_valid) = &#x27;</span>,<span class="hljs-built_in">len</span>(ds_valid))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;len(ds_train+ds_valid) = &#x27;</span>,<span class="hljs-built_in">len</span>(ds_data))<br></code></pre></td></tr></table></figure><h6 id="2，根据图片目录创建图片数据集"><a href="#2，根据图片目录创建图片数据集" class="headerlink" title="2，根据图片目录创建图片数据集"></a>2，根据图片目录创建图片数据集</h6><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br><span class="hljs-keyword">import</span> torch <br><span class="hljs-title">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-title">from</span> torchvision <span class="hljs-keyword">import</span> transforms,datasets<br></code></pre></td></tr></table></figure><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-comment">#一些常用的图片增强操作</span><br>from PIL <span class="hljs-built_in">import</span> Image<br><span class="hljs-attr">img</span> = Image.open(&#x27;/home/kesci/input/data6936/data/cat.jpeg&#x27;)<br><br><span class="hljs-comment"># 随机数值翻转</span><br>transforms.RandomVerticalFlip()(img)<br><br><span class="hljs-comment">#随机旋转</span><br>transforms.RandomRotation(<span class="hljs-number">45</span>)(img)<br><br><span class="hljs-comment"># 定义图片增强操作</span><br><span class="hljs-attr">transform_train</span> = transforms.Compose([<br>   transforms.RandomHorizontalFlip(), <span class="hljs-comment">#随机水平翻转</span><br>   transforms.RandomVerticalFlip(), <span class="hljs-comment">#随机垂直翻转</span><br>   transforms.RandomRotation(<span class="hljs-number">45</span>),  <span class="hljs-comment">#随机在45度角度内旋转</span><br>   transforms.ToTensor() <span class="hljs-comment">#转换成张量</span><br>  ]<br>) <br><br><span class="hljs-attr">transform_valid</span> = transforms.Compose([<br>    transforms.ToTensor()<br>  ]<br>)<br><br></code></pre></td></tr></table></figure><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs maxima"># 根据图片目录创建数据集<br>ds_train = datasets.ImageFolder(<span class="hljs-string">&quot;/home/kesci/input/data6936/data/cifar2/train/&quot;</span>,<br>            <span class="hljs-built_in">transform</span> = transform_train,target_transform= <span class="hljs-built_in">lambda</span> t:torch.tensor([t]).<span class="hljs-built_in">float</span>())<br>ds_valid = datasets.ImageFolder(<span class="hljs-string">&quot;/home/kesci/input/data6936/data/cifar2/test/&quot;</span>,<br>            <span class="hljs-built_in">transform</span> = transform_train,target_transform= <span class="hljs-built_in">lambda</span> t:torch.tensor([t]).<span class="hljs-built_in">float</span>())<br><br><span class="hljs-built_in">print</span>(ds_train.class_to_idx)<br></code></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 使用DataLoader加载数据集</span><br>dl_train = DataLoader(ds_train,batch_size = 50,shuffle = <span class="hljs-literal">True</span>,<span class="hljs-attribute">num_workers</span>=3)<br>dl_valid = DataLoader(ds_valid,batch_size = 50,shuffle = <span class="hljs-literal">True</span>,<span class="hljs-attribute">num_workers</span>=3)<br><br><span class="hljs-keyword">for</span> features,labels <span class="hljs-keyword">in</span> dl_train:<br>    <span class="hljs-builtin-name">print</span>(features.shape)<br>    <span class="hljs-builtin-name">print</span>(labels.shape)<br>    break<br><br></code></pre></td></tr></table></figure><h6 id="创建自定义数据集"><a href="#创建自定义数据集" class="headerlink" title="创建自定义数据集"></a>创建自定义数据集</h6><p>下面通过继承Dataset类创建imdb文本分类任务的自定义数据集。</p><p>大概思路如下：首先，对训练集文本分词构建词典。然后将训练集文本和测试集文本数据转换成token单词编码。</p><p>接着将转换成单词编码的训练集数据和测试集数据按样本分割成多个文件，一个文件代表一个样本。</p><p>最后，我们可以根据文件名列表获取对应序号的样本内容，从而构建Dataset数据集。</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br><span class="hljs-title">from</span> collections <span class="hljs-keyword">import</span> OrderedDict<br><span class="hljs-keyword">import</span> re,string<br><br><span class="hljs-type">MAX_WORDS</span> = <span class="hljs-number">10000</span>  # 仅考虑最高频的<span class="hljs-number">10000</span>个词<br><span class="hljs-type">MAX_LEN</span> = <span class="hljs-number">200</span>  # 每个样本保留<span class="hljs-number">200</span>个词的长度<br><span class="hljs-type">BATCH_SIZE</span> = <span class="hljs-number">20</span> <br><br><span class="hljs-title">train_data_path</span> = &#x27;/home/kesci/input/data6936/<span class="hljs-class"><span class="hljs-keyword">data</span>/imdb/train.tsv&#x27;</span><br><span class="hljs-title">test_data_path</span> = &#x27;/home/kesci/input/data6936/<span class="hljs-class"><span class="hljs-keyword">data</span>/imdb/test.tsv&#x27;</span><br><span class="hljs-title">train_token_path</span> = &#x27;/home/kesci/input/data6936/<span class="hljs-class"><span class="hljs-keyword">data</span>/imdb/train_token.tsv&#x27;</span><br><span class="hljs-title">test_token_path</span> =  &#x27;/home/kesci/input/data6936/<span class="hljs-class"><span class="hljs-keyword">data</span>/imdb/test_token.tsv&#x27;</span><br><span class="hljs-title">train_samples_path</span> = &#x27;/home/kesci/input/data6936/<span class="hljs-class"><span class="hljs-keyword">data</span>/imdb/train_samples/&#x27;</span><br><span class="hljs-title">test_samples_path</span> =  &#x27;/home/kesci/input/data6936/<span class="hljs-class"><span class="hljs-keyword">data</span>/imdb/test_samples/&#x27;</span><br></code></pre></td></tr></table></figure><p>首先我们构建词典，并保留最高频的MAX_WORDS个词。</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><span class="hljs-comment">##构建词典</span><br><br>word_count_dict = &#123;&#125;<br><br><span class="hljs-comment">#清洗文本</span><br>def clean_text(<span class="hljs-built_in">text</span>):<br>    lowercase = <span class="hljs-built_in">text</span>.lower().replace(<span class="hljs-string">&quot;\n&quot;</span>,<span class="hljs-string">&quot; &quot;</span>)<br>    stripped_html = re.sub(&#x27;&lt;br /&gt;&#x27;, &#x27; &#x27;,lowercase)<br>    cleaned_punctuation = re.sub(&#x27;[%s]&#x27;%re.escape(<span class="hljs-built_in">string</span>.punctuation),&#x27;&#x27;,stripped_html)<br><span class="hljs-built_in">    return</span> cleaned_punctuation<br><br><span class="hljs-keyword">with</span> open(train_data_path,<span class="hljs-string">&quot;r&quot;</span>,encoding = &#x27;utf<span class="hljs-number">-8</span>&#x27;) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f:<br>        label,<span class="hljs-built_in">text</span> = line.split(<span class="hljs-string">&quot;\t&quot;</span>)<br>        cleaned_text = clean_text(<span class="hljs-built_in">text</span>)<br>        <span class="hljs-keyword">for</span> <span class="hljs-built_in">word</span> <span class="hljs-keyword">in</span> cleaned_text.split(<span class="hljs-string">&quot; &quot;</span>):<br>            word_count_dict[<span class="hljs-built_in">word</span>] = word_count_dict.<span class="hljs-keyword">get</span>(<span class="hljs-built_in">word</span>,<span class="hljs-number">0</span>)+<span class="hljs-number">1</span> <br><br>df_word_dict = pd.DataFrame(pd.Series(word_count_dict,<span class="hljs-built_in">name</span> = <span class="hljs-string">&quot;count&quot;</span>))<br>df_word_dict = df_word_dict.sort_values(<span class="hljs-keyword">by</span> = <span class="hljs-string">&quot;count&quot;</span>,ascending =False)<br><br>df_word_dict = df_word_dict[<span class="hljs-number">0</span>:MAX_WORDS<span class="hljs-number">-2</span>] <span class="hljs-comment">#  </span><br>df_word_dict[<span class="hljs-string">&quot;word_id&quot;</span>] = range(<span class="hljs-number">2</span>,MAX_WORDS) <span class="hljs-comment">#编号0和1分别留给未知词&lt;unkown&gt;和填充&lt;padding&gt;</span><br><br>word_id_dict = df_word_dict[<span class="hljs-string">&quot;word_id&quot;</span>].to_dict()<br><br>df_word_dict.head(<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><p>然后我们利用构建好的词典，将文本转换成token序号。</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-comment">#转换token</span><br><br><span class="hljs-comment"># 填充文本</span><br>def pad(data_list,pad_length):<br>    padded_list = data_list.copy()<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(data_list)&gt; pad_length:<br>         padded_list = data_list[-pad_length:]<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(data_list)&lt; pad_length:<br>         padded_list = [<span class="hljs-number">1</span>]*(pad_length-<span class="hljs-built_in">len</span>(data_list))+data_list<br>    <span class="hljs-literal">return</span> padded_list<br><br>def text_to_token(text_file,token_file):<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(text_file,<span class="hljs-string">&quot;r&quot;</span>,encoding = <span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fin,\<br>      <span class="hljs-built_in">open</span>(token_file,<span class="hljs-string">&quot;w&quot;</span>,encoding = <span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fout:<br>        <span class="hljs-keyword">for</span> <span class="hljs-built_in">line</span> <span class="hljs-keyword">in</span> fin:<br>            label,<span class="hljs-keyword">text</span> = <span class="hljs-built_in">line</span>.<span class="hljs-built_in">split</span>(<span class="hljs-string">&quot;\t&quot;</span>)<br>            cleaned_text = clean_text(<span class="hljs-keyword">text</span>)<br>            word_token_list = [word_id_dict.<span class="hljs-built_in">get</span>(<span class="hljs-built_in">word</span>, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> <span class="hljs-built_in">word</span> <span class="hljs-keyword">in</span> cleaned_text.<span class="hljs-built_in">split</span>(<span class="hljs-string">&quot; &quot;</span>)]<br>            pad_list = pad(word_token_list,MAX_LEN)<br>            out_line = label+<span class="hljs-string">&quot;\t&quot;</span>+<span class="hljs-string">&quot; &quot;</span>.join([str(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> pad_list])<br>            fout.<span class="hljs-built_in">write</span>(out_line+<span class="hljs-string">&quot;\n&quot;</span>)<br>        <br>text_to_token(train_data_path,train_token_path)<br>text_to_token(test_data_path,test_token_path)<br></code></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># 分割样本<br>import os<br><br><span class="hljs-keyword">if</span> not os.path.exists(train_samples_path):<br>    os.mkdir(train_samples_path)<br>    <br><span class="hljs-keyword">if</span> not os.path.exists(test_samples_path):<br>    os.mkdir(test_samples_path)<br>    <br>    <br>def split<span class="hljs-constructor">_samples(<span class="hljs-params">token_path</span>,<span class="hljs-params">samples_dir</span>)</span>:<br>    <span class="hljs-keyword">with</span> <span class="hljs-keyword">open</span>(token_path,<span class="hljs-string">&quot;r&quot;</span>,encoding = &#x27;utf-<span class="hljs-number">8</span>&#x27;) <span class="hljs-keyword">as</span> fin:<br>        i = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> fin:<br>            <span class="hljs-keyword">with</span> <span class="hljs-keyword">open</span>(samples_dir+<span class="hljs-string">&quot;%d.txt&quot;</span>%i,<span class="hljs-string">&quot;w&quot;</span>,encoding = <span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> fout:<br>                fout.write(line)<br>            i = i+<span class="hljs-number">1</span><br><br>split<span class="hljs-constructor">_samples(<span class="hljs-params">train_token_path</span>,<span class="hljs-params">train_samples_path</span>)</span><br>split<span class="hljs-constructor">_samples(<span class="hljs-params">test_token_path</span>,<span class="hljs-params">test_samples_path</span>)</span><br></code></pre></td></tr></table></figure><p>一切准备就绪，我们可以创建数据集Dataset, 从文件名称列表中读取文件内容了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">imdbDataset</span>(<span class="hljs-params">Dataset</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,samples_dir</span>):</span><br>        self.samples_dir = samples_dir<br>        self.samples_paths = os.listdir(samples_dir)<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.samples_paths)<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span>(<span class="hljs-params">self,index</span>):</span><br>        path = self.samples_dir + self.samples_paths[index]<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(path,<span class="hljs-string">&quot;r&quot;</span>,encoding = <span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>            line = f.readline()<br>            label,tokens = line.split(<span class="hljs-string">&quot;\t&quot;</span>)<br>            label = torch.tensor([<span class="hljs-built_in">float</span>(label)],dtype = torch.<span class="hljs-built_in">float</span>)<br>            feature = torch.tensor([<span class="hljs-built_in">int</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> tokens.split(<span class="hljs-string">&quot; &quot;</span>)],dtype = torch.long)<br>            <span class="hljs-keyword">return</span>  (feature,label)<br></code></pre></td></tr></table></figure><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">ds_train</span> = imdbDataset(train_samples_path)<br><span class="hljs-attr">ds_test</span> = imdbDataset(test_samples_path)<br></code></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">dl_train = <span class="hljs-constructor">DataLoader(<span class="hljs-params">ds_train</span>,<span class="hljs-params">batch_size</span> = BATCH_SIZE,<span class="hljs-params">shuffle</span> = True,<span class="hljs-params">num_workers</span>=4)</span><br>dl_test = <span class="hljs-constructor">DataLoader(<span class="hljs-params">ds_test</span>,<span class="hljs-params">batch_size</span> = BATCH_SIZE,<span class="hljs-params">num_workers</span>=4)</span><br><br><span class="hljs-keyword">for</span> features,labels <span class="hljs-keyword">in</span> dl_train:<br>    print(features)<br>    print(labels)<br>    break<br></code></pre></td></tr></table></figure><p>最后构建模型测试一下数据集管道是否可用。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">import torch<br>from torch import nn <br>import importlib <br>from torchkeras import Model,summary<br><br><span class="hljs-keyword">class</span> <span class="hljs-constructor">Net(Model)</span>:<br>    <br>    def <span class="hljs-constructor">__init__(<span class="hljs-params">self</span>)</span>:<br>        super(Net, self).<span class="hljs-constructor">__init__()</span><br>        <br>        #设置padding_idx参数后将在训练过程中将填充的token始终赋值为<span class="hljs-number">0</span>向量<br>        self.embedding = nn.<span class="hljs-constructor">Embedding(<span class="hljs-params">num_embeddings</span> = MAX_WORDS,<span class="hljs-params">embedding_dim</span> = 3,<span class="hljs-params">padding_idx</span> = 1)</span><br>        self.conv = nn.<span class="hljs-constructor">Sequential()</span><br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;conv_1&quot;</span>,<span class="hljs-params">nn</span>.Conv1d(<span class="hljs-params">in_channels</span> = 3,<span class="hljs-params">out_channels</span> = 16,<span class="hljs-params">kernel_size</span> = 5)</span>)<br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;pool_1&quot;</span>,<span class="hljs-params">nn</span>.MaxPool1d(<span class="hljs-params">kernel_size</span> = 2)</span>)<br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;relu_1&quot;</span>,<span class="hljs-params">nn</span>.ReLU()</span>)<br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;conv_2&quot;</span>,<span class="hljs-params">nn</span>.Conv1d(<span class="hljs-params">in_channels</span> = 16,<span class="hljs-params">out_channels</span> = 128,<span class="hljs-params">kernel_size</span> = 2)</span>)<br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;pool_2&quot;</span>,<span class="hljs-params">nn</span>.MaxPool1d(<span class="hljs-params">kernel_size</span> = 2)</span>)<br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;relu_2&quot;</span>,<span class="hljs-params">nn</span>.ReLU()</span>)<br>        <br>        self.dense = nn.<span class="hljs-constructor">Sequential()</span><br>        self.dense.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;flatten&quot;</span>,<span class="hljs-params">nn</span>.Flatten()</span>)<br>        self.dense.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;linear&quot;</span>,<span class="hljs-params">nn</span>.Linear(6144,1)</span>)<br>        self.dense.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;sigmoid&quot;</span>,<span class="hljs-params">nn</span>.Sigmoid()</span>)<br>        <br>    def forward(self,x):<br>        x = self.embedding(x).transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)<br>        x = self.conv(x)<br>        y = self.dense(x)<br>        return y<br>        <br>model = <span class="hljs-constructor">Net()</span><br>print(model)<br><br>model.summary(input_shape = (<span class="hljs-number">200</span>,),input_dtype = torch.LongTensor)<br></code></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"># 编译模型<br>def accuracy(y_pred,y_true):<br>    y_pred = torch.where(y_pred&gt;<span class="hljs-number">0.5</span>,torch.ones<span class="hljs-constructor">_like(<span class="hljs-params">y_pred</span>,<span class="hljs-params">dtype</span> = <span class="hljs-params">torch</span>.<span class="hljs-params">float32</span>)</span>,<br>                      torch.zeros<span class="hljs-constructor">_like(<span class="hljs-params">y_pred</span>,<span class="hljs-params">dtype</span> = <span class="hljs-params">torch</span>.<span class="hljs-params">float32</span>)</span>)<br>    acc = torch.mean(<span class="hljs-number">1</span>-torch.abs(y_true-y_pred))<br>    return acc<br><br>model.compile(loss_func = nn.<span class="hljs-constructor">BCELoss()</span>,optimizer= torch.optim.<span class="hljs-constructor">Adagrad(<span class="hljs-params">model</span>.<span class="hljs-params">parameters</span>()</span>,lr = <span class="hljs-number">0.02</span>),<br>             metrics_dict=&#123;<span class="hljs-string">&quot;accuracy&quot;</span>:accuracy&#125;)<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># 训练模型</span><br><span class="hljs-attribute">dfhistory</span> = model.fit(<span class="hljs-number">10</span>,dl_train,dl_val=dl_test,log_step_freq= <span class="hljs-number">200</span>)<br></code></pre></td></tr></table></figure><h5 id="使用DataLoader加载数据集"><a href="#使用DataLoader加载数据集" class="headerlink" title="使用DataLoader加载数据集"></a>使用DataLoader加载数据集</h5><p>DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。</p><p>DataLoader的函数签名如下。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs routeros">DataLoader(  <br>    dataset,  <br>    <span class="hljs-attribute">batch_size</span>=1,  <br>    <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">False</span>,  <br>    <span class="hljs-attribute">sampler</span>=None,  <br>    <span class="hljs-attribute">batch_sampler</span>=None,  <br>    <span class="hljs-attribute">num_workers</span>=0,  <br>    <span class="hljs-attribute">collate_fn</span>=None,  <br>    <span class="hljs-attribute">pin_memory</span>=<span class="hljs-literal">False</span>,  <br>    <span class="hljs-attribute">drop_last</span>=<span class="hljs-literal">False</span>,  <br>    <span class="hljs-attribute">timeout</span>=0,  <br>    <span class="hljs-attribute">worker_init_fn</span>=None,  <br>    <span class="hljs-attribute">multiprocessing_context</span>=None,  <br>)<br></code></pre></td></tr></table></figure><p>一般情况下，我们仅仅会配置 dataset, batch_size, shuffle, num_workers, drop_last这五个参数，其他参数使用默认值即可。</p><p>DataLoader除了可以加载我们前面讲的 torch.utils.data.Dataset 外，还能够加载另外一种数据集 torch.utils.data.IterableDataset。</p><p>和Dataset数据集相当于一种列表结构不同，IterableDataset相当于一种迭代器结构。 它更加复杂，一般较少使用。</p><ul><li>dataset : 数据集</li><li>batch_size: 批次大小</li><li>shuffle: 是否乱序</li><li>sampler: 样本采样函数，一般无需设置。</li><li>batch_sampler: 批次采样函数，一般无需设置。</li><li>num_workers: 使用多进程读取数据，设置的进程数。</li><li>collate_fn: 整理一个批次数据的函数。</li><li>pin_memory: 是否设置为锁业内存。默认为False，锁业内存不会使用虚拟内存(硬盘)，从锁业内存拷贝到GPU上速度会更快。</li><li>drop_last: 是否丢弃最后一个样本数量不足batch_size批次数据。</li><li>timeout: 加载一个数据批次的最长等待时间，一般无需设置。</li><li>worker_init_fn: 每个worker中dataset的初始化函数，常用于 IterableDataset。一般不使用。</li></ul><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-comment">#构建输入数据管道</span><br><span class="hljs-attr">ds</span> = TensorDataset(torch.arange(<span class="hljs-number">1</span>,<span class="hljs-number">50</span>))<br><span class="hljs-attr">dl</span> = DataLoader(ds,<br>                <span class="hljs-attr">batch_size</span> = <span class="hljs-number">10</span>,<br>                <span class="hljs-attr">shuffle=</span> True,<br>                <span class="hljs-attr">num_workers=2,</span><br>                <span class="hljs-attr">drop_last</span> = True)<br><span class="hljs-comment">#迭代数据</span><br>for batch, <span class="hljs-keyword">in</span> dl:<br>    print(batch)<br></code></pre></td></tr></table></figure><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记6-nn.functional和nn.Module</title>
    <link href="/2022/01/20/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-nn-functional%E5%92%8Cnn-Module/"/>
    <url>/2022/01/20/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-nn-functional%E5%92%8Cnn-Module/</url>
    
    <content type="html"><![CDATA[<h2 id="神经网络相关的组件-如激活函数，模型层，损失函数"><a href="#神经网络相关的组件-如激活函数，模型层，损失函数" class="headerlink" title="神经网络相关的组件(如激活函数，模型层，损失函数)"></a>神经网络相关的组件(如激活函数，模型层，损失函数)</h2><span id="more"></span><h5 id="nn-functional-和-nn-Module"><a href="#nn-functional-和-nn-Module" class="headerlink" title="nn.functional 和 nn.Module"></a>nn.functional 和 nn.Module</h5><p>Pytorch和神经网络相关的功能组件大多都封装在 torch.nn模块下。<br>这些功能组件的绝大部分既有函数形式实现，也有类形式实现。<br>其中nn.functional(一般引入后改名为F)有各种功能组件的函数实现。例如：</p><h6 id="激活函数"><a href="#激活函数" class="headerlink" title="(激活函数)"></a>(激活函数)</h6><ul><li>F.relu</li><li>F.sigmoid</li><li>F.tanh</li><li>F.softmax</li></ul><h6 id="模型层"><a href="#模型层" class="headerlink" title="(模型层)"></a>(模型层)</h6><ul><li>F.linear</li><li>F.conv2d</li><li>F.max_pool2d</li><li>F.dropout2d</li><li>F.embedding</li></ul><h6 id="损失函数"><a href="#损失函数" class="headerlink" title="(损失函数)"></a>(损失函数)</h6><ul><li>F.binary_cross_entropy</li><li>F.mse_loss</li><li>F.cross_entropy</li></ul><p>为了便于对参数进行管理，一般通过继承 nn.Module 转换成为类的实现形式，并直接封装在 nn 模块下。例如：</p><h6 id="激活函数-1"><a href="#激活函数-1" class="headerlink" title="(激活函数)"></a>(激活函数)</h6><ul><li>nn.ReLU</li><li>nn.Sigmoid</li><li>nn.Tanh</li><li>nn.Softmax</li></ul><h6 id="模型层-1"><a href="#模型层-1" class="headerlink" title="(模型层)"></a>(模型层)</h6><ul><li>nn.Linear</li><li>nn.Conv2d</li><li>nn.MaxPool2d</li><li>nn.Dropout2d</li><li>nn.Embedding</li></ul><h6 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="(损失函数)"></a>(损失函数)</h6><ul><li>nn.BCELoss</li><li>nn.MSELoss</li><li>nn.CrossEntropyLoss</li></ul><p>实际上nn.Module除了可以管理其引用的各种参数，还可以管理其引用的子模块，功能十分强大。</p><h5 id="使用nn-Module来管理参数"><a href="#使用nn-Module来管理参数" class="headerlink" title="使用nn.Module来管理参数"></a>使用nn.Module来管理参数</h5><p>在Pytorch中，模型的参数是需要被优化器训练的，因此，通常要设置参数为 requires_grad = True 的张量。<br>同时，在一个模型中，往往有许多的参数，要手动管理这些参数并不是一件容易的事情。<br>Pytorch一般将参数用nn.Parameter来表示，并且用nn.Module来管理其结构下的所有参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn <br><span class="hljs-keyword">import</span> torch.nn.functional  <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># nn.Parameter 具有 requires_grad = True 属性</span><br>w = nn.Parameter(torch.randn(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))<br><span class="hljs-built_in">print</span>(w)<br><span class="hljs-built_in">print</span>(w.requires_grad)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">arameter containing:</span><br><span class="hljs-string">tensor([[ 1.2790,  0.6851],</span><br><span class="hljs-string">        [-1.9961,  0.4121]], requires_grad=True)</span><br><span class="hljs-string">True</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># nn.ParameterList 可以将多个nn.Parameter组成一个列表</span><br>params_list = nn.ParameterList([nn.Parameter(torch.rand(<span class="hljs-number">8</span>,i)) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>)])<br><span class="hljs-built_in">print</span>(params_list)<br><span class="hljs-built_in">print</span>(params_list[<span class="hljs-number">0</span>].requires_grad)<br><br><span class="hljs-comment"># nn.ParameterDict 可以将多个nn.Parameter组成一个字典</span><br>params_dict = nn.ParameterDict(&#123;<span class="hljs-string">&quot;a&quot;</span>:nn.Parameter(torch.rand(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)),<br>                               <span class="hljs-string">&quot;b&quot;</span>:nn.Parameter(torch.zeros(<span class="hljs-number">2</span>))&#125;)<br><span class="hljs-built_in">print</span>(params_dict)<br><span class="hljs-built_in">print</span>(params_dict[<span class="hljs-string">&quot;a&quot;</span>].requires_grad)<br></code></pre></td></tr></table></figure><figure class="highlight monkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs monkey"><span class="hljs-meta"># 可以用Module将它们管理起来</span><span class="hljs-meta"></span><br><span class="hljs-meta"># module.parameters()返回一个生成器，包括其结构下的所有parameters</span><br><br><span class="hljs-keyword">module</span> = nn.<span class="hljs-keyword">Module</span>()<br><span class="hljs-keyword">module</span>.w = w<br><span class="hljs-keyword">module</span>.params_list = params_list<br><span class="hljs-keyword">module</span>.params_dict = params_dict<br><br>num_param = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> param in <span class="hljs-keyword">module</span>.parameters():<br>    <span class="hljs-built_in">print</span>(param,<span class="hljs-string">&quot;\n&quot;</span>)<br>    num_param = num_param + <span class="hljs-number">1</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;number of Parameters =&quot;</span>,num_param)<br></code></pre></td></tr></table></figure><p>实践当中，一般通过继承nn.Module来构建模块类，并将所有含有需要学习的参数的部分放在构造函数中。<br>以下范例为Pytorch中nn.Linear的源码的简化版本<br>可以看到它将需要学习的参数放在了__init__构造函数中，并在forward中调用F.linear函数来实现计算逻辑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Linear</span>(<span class="hljs-params">nn.Module</span>):</span><br>    __constants__ = [<span class="hljs-string">&#x27;in_features&#x27;</span>, <span class="hljs-string">&#x27;out_features&#x27;</span>]<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, in_features, out_features, bias=<span class="hljs-literal">True</span></span>):</span><br>        <span class="hljs-built_in">super</span>(Linear, self).__init__()<br>        self.in_features = in_features<br>        self.out_features = out_features<br>        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))<br>        <span class="hljs-keyword">if</span> bias:<br>            self.bias = nn.Parameter(torch.Tensor(out_features))<br>        <span class="hljs-keyword">else</span>:<br>            self.register_parameter(<span class="hljs-string">&#x27;bias&#x27;</span>, <span class="hljs-literal">None</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):</span><br>        <span class="hljs-keyword">return</span> F.linear(<span class="hljs-built_in">input</span>, self.weight, self.bias)<br></code></pre></td></tr></table></figure><h5 id="使用nn-Module来管理子模块"><a href="#使用nn-Module来管理子模块" class="headerlink" title="使用nn.Module来管理子模块"></a>使用nn.Module来管理子模块</h5><p>一般情况下，我们都很少直接使用 nn.Parameter来定义参数构建模型，而是通过一些拼装一些常用的模型层来构造模型。<br>这些模型层也是继承自nn.Module的对象,本身也包括参数，属于我们要定义的模块的子模块。</p><p>nn.Module提供了一些方法可以管理这些子模块。</p><ul><li>children() 方法: 返回生成器，包括模块下的所有子模块。</li><li>named_children()方法：返回一个生成器，包括模块下的所有子模块，以及它们的名字。</li><li>modules()方法：返回一个生成器，包括模块下的所有各个层级的模块，包括模块本身。</li><li>named_modules()方法：返回一个生成器，包括模块下的所有各个层级的模块以及它们的名字，包括模块本身。</li></ul><p>其中chidren()方法和named_children()方法较多使用。<br>modules()方法和named_modules()方法较少使用，其功能可以通过多个named_children()的嵌套使用实现。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">class</span> <span class="hljs-constructor">Net(<span class="hljs-params">nn</span>.Module)</span>:<br>    <br>    def <span class="hljs-constructor">__init__(<span class="hljs-params">self</span>)</span>:<br>        super(Net, self).<span class="hljs-constructor">__init__()</span><br>        <br>        self.embedding = nn.<span class="hljs-constructor">Embedding(<span class="hljs-params">num_embeddings</span> = 10000,<span class="hljs-params">embedding_dim</span> = 3,<span class="hljs-params">padding_idx</span> = 1)</span><br>        self.conv = nn.<span class="hljs-constructor">Sequential()</span><br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;conv_1&quot;</span>,<span class="hljs-params">nn</span>.Conv1d(<span class="hljs-params">in_channels</span> = 3,<span class="hljs-params">out_channels</span> = 16,<span class="hljs-params">kernel_size</span> = 5)</span>)<br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;pool_1&quot;</span>,<span class="hljs-params">nn</span>.MaxPool1d(<span class="hljs-params">kernel_size</span> = 2)</span>)<br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;relu_1&quot;</span>,<span class="hljs-params">nn</span>.ReLU()</span>)<br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;conv_2&quot;</span>,<span class="hljs-params">nn</span>.Conv1d(<span class="hljs-params">in_channels</span> = 16,<span class="hljs-params">out_channels</span> = 128,<span class="hljs-params">kernel_size</span> = 2)</span>)<br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;pool_2&quot;</span>,<span class="hljs-params">nn</span>.MaxPool1d(<span class="hljs-params">kernel_size</span> = 2)</span>)<br>        self.conv.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;relu_2&quot;</span>,<span class="hljs-params">nn</span>.ReLU()</span>)<br>        <br>        self.dense = nn.<span class="hljs-constructor">Sequential()</span><br>        self.dense.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;flatten&quot;</span>,<span class="hljs-params">nn</span>.Flatten()</span>)<br>        self.dense.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;linear&quot;</span>,<span class="hljs-params">nn</span>.Linear(6144,1)</span>)<br>        self.dense.add<span class="hljs-constructor">_module(<span class="hljs-string">&quot;sigmoid&quot;</span>,<span class="hljs-params">nn</span>.Sigmoid()</span>)<br>        <br>    def forward(self,x):<br>        x = self.embedding(x).transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)<br>        x = self.conv(x)<br>        y = self.dense(x)<br>        return y<br>    <br>net = <span class="hljs-constructor">Net()</span><br></code></pre></td></tr></table></figure><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-tag">i</span> = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> child <span class="hljs-keyword">in</span> net<span class="hljs-selector-class">.children</span>():<br>    i+=<span class="hljs-number">1</span><br>    print(child,<span class="hljs-string">&quot;\n&quot;</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;child number&quot;</span>,i)</span></span><br><br><br><span class="hljs-selector-tag">i</span> = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> name,child <span class="hljs-keyword">in</span> net<span class="hljs-selector-class">.named_children</span>():<br>    i+=<span class="hljs-number">1</span><br>    print(name,<span class="hljs-string">&quot;:&quot;</span>,child,<span class="hljs-string">&quot;\n&quot;</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;child number&quot;</span>,i)</span></span><br><br><br><span class="hljs-selector-tag">i</span> = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> net<span class="hljs-selector-class">.modules</span>():<br>    i+=<span class="hljs-number">1</span><br>    print(module)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;module number:&quot;</span>,i)</span></span><br><br></code></pre></td></tr></table></figure><p>下面我们通过named_children方法找到embedding层，并将其参数设置为不可训练(相当于冻结embedding层)。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">children_dict = &#123;name:module <span class="hljs-keyword">for</span> name,module <span class="hljs-keyword">in</span> net<span class="hljs-selector-class">.named_children</span>()&#125;<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(children_dict)</span></span><br>embedding = children_dict<span class="hljs-selector-attr">[<span class="hljs-string">&quot;embedding&quot;</span>]</span><br>embedding<span class="hljs-selector-class">.requires_grad_</span>(False) #冻结其参数<br></code></pre></td></tr></table></figure><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">#可以看到其第一层的参数已经不可以被训练了。<br>for param in embedding.parameters():<br><span class="hljs-code">    print(param.requires_grad)</span><br><span class="hljs-code">    print(param.numel())</span><br><br>&#x27;&#x27;&#x27;<br><span class="hljs-code">False</span><br><span class="hljs-code">30000</span><br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-title">from</span> torchkeras <span class="hljs-keyword">import</span> summary<br><span class="hljs-title">summary</span>(net,input_shape = (<span class="hljs-number">200</span>,),input_d<span class="hljs-keyword">type</span> = torch.<span class="hljs-type">LongTensor</span>)<br># 不可训练参数数量增加<br></code></pre></td></tr></table></figure><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记5-张量的数学运算</title>
    <link href="/2022/01/20/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%BC%A0%E9%87%8F%E7%9A%84%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/"/>
    <url>/2022/01/20/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%BC%A0%E9%87%8F%E7%9A%84%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/</url>
    
    <content type="html"><![CDATA[<h2 id="张量数学运算主要有：标量运算，向量运算，矩阵运算，以及张量运算的广播机制"><a href="#张量数学运算主要有：标量运算，向量运算，矩阵运算，以及张量运算的广播机制" class="headerlink" title="张量数学运算主要有：标量运算，向量运算，矩阵运算，以及张量运算的广播机制"></a>张量数学运算主要有：标量运算，向量运算，矩阵运算，以及张量运算的广播机制</h2><span id="more"></span><h5 id="标量运算"><a href="#标量运算" class="headerlink" title="标量运算"></a>标量运算</h5><p>张量的数学运算符可以分为标量运算符、向量运算符、以及矩阵运算符。<br>加减乘除乘方，以及三角函数，指数，对数等常见函数，逻辑比较运算符等都是标量运算符。<br>标量运算符的特点是对张量实施逐元素运算。<br>有些标量运算符对常用的数学运算符进行了重载。并且支持类似numpy的广播特性。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># 幅值裁剪<br>x = torch.tensor([0.9,-0.8,100.0,-20.0,0.7])<br>y = torch.clamp(x,min=-1,max = 1)<br>z = torch.clamp(x,max = 1)<br>print(y)<br>print(z)<br>&#x27;&#x27;&#x27;<br><span class="hljs-code">tensor([ 0.9000, -0.8000,  1.0000, -1.0000,  0.7000])</span><br><span class="hljs-code">tensor([  0.9000,  -0.8000,   1.0000, -20.0000,   0.7000])</span><br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure><h5 id="向量运算"><a href="#向量运算" class="headerlink" title="向量运算"></a>向量运算</h5><p>向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-tag">a</span> = torch<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>)<span class="hljs-selector-class">.float</span>()<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.sum(a)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.mean(a)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.max(a)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.min(a)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.prod(a)</span></span>) #累乘<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.std(a)</span></span>)  #标准差<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.var(a)</span></span>)  #方差<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.median(a)</span></span>) #中位数<br></code></pre></td></tr></table></figure><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">#指定维度计算统计值<br><span class="hljs-selector-tag">b</span> = <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.view</span>(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(b)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.max(b,dim = <span class="hljs-number">0</span>)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.max(b,dim = <span class="hljs-number">1</span>)</span></span>)<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment">#torch.sort和torch.topk可以对张量排序</span><br><span class="hljs-attribute">a</span> = torch.tensor([[<span class="hljs-number">9</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">4</span>]]).float()<br><span class="hljs-attribute">print</span>(torch.topk(a,<span class="hljs-number">2</span>,dim = <span class="hljs-number">0</span>),<span class="hljs-string">&quot;\n&quot;</span>)<br><span class="hljs-attribute">print</span>(torch.topk(a,<span class="hljs-number">2</span>,dim = <span class="hljs-number">1</span>),<span class="hljs-string">&quot;\n&quot;</span>)<br><span class="hljs-attribute">print</span>(torch.sort(a,dim = <span class="hljs-number">1</span>),<span class="hljs-string">&quot;\n&quot;</span>)<br><br><span class="hljs-comment">#利用torch.topk可以在Pytorch中实现KNN算法</span><br></code></pre></td></tr></table></figure><h5 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h5><p>矩阵必须是二维的。类似torch.tensor([1,2,3])这样的不是矩阵。<br>矩阵运算包括：矩阵乘法，矩阵转置，矩阵逆，矩阵求迹，矩阵范数，矩阵行列式，矩阵求特征值，矩阵分解等运算。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs stylus">#矩阵乘法<br><span class="hljs-selector-tag">a</span> = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[1,2]</span>,<span class="hljs-selector-attr">[3,4]</span>])<br><span class="hljs-selector-tag">b</span> = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[2,0]</span>,<span class="hljs-selector-attr">[0,2]</span>])<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a@b)</span></span>  #等价于torch<span class="hljs-selector-class">.matmul</span>(<span class="hljs-selector-tag">a</span>,b) 或 torch<span class="hljs-selector-class">.mm</span>(<span class="hljs-selector-tag">a</span>,b)<br><br>#矩阵转置<br><span class="hljs-selector-tag">a</span> = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[1.0,2]</span>,<span class="hljs-selector-attr">[3,4]</span>])<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(a.t()</span></span>)<br><br>#矩阵逆，必须为浮点类型<br><span class="hljs-selector-tag">a</span> = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[1.0,2]</span>,<span class="hljs-selector-attr">[3,4]</span>])<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.inverse(a)</span></span>)<br><br>#矩阵求trace<br><span class="hljs-selector-tag">a</span> = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[1.0,2]</span>,<span class="hljs-selector-attr">[3,4]</span>])<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.trace(a)</span></span>)<br><br>#矩阵求范数<br><span class="hljs-selector-tag">a</span> = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[1.0,2]</span>,<span class="hljs-selector-attr">[3,4]</span>])<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.norm(a)</span></span>)<br><br>#矩阵行列式<br><span class="hljs-selector-tag">a</span> = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[1.0,2]</span>,<span class="hljs-selector-attr">[3,4]</span>])<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.det(a)</span></span>)<br><br>#矩阵特征值和特征向量<br><span class="hljs-selector-tag">a</span> = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[1.0,2]</span>,<span class="hljs-selector-attr">[-5,4]</span>],dtype = torch.<span class="hljs-attribute">float</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.eig(a,eigenvectors=True)</span></span>)<br><br>#两个特征值分别是 -<span class="hljs-number">2.5</span>+<span class="hljs-number">2.7839</span>j, <span class="hljs-number">2.5</span>-<span class="hljs-number">2.7839</span>j <br><br><br>#矩阵QR分解, 将一个方阵分解为一个正交矩阵q和上三角矩阵r<br>#QR分解实际上是对矩阵a实施Schmidt正交化得到<span class="hljs-selector-tag">q</span><br><span class="hljs-selector-tag">a</span>  = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[1.0,2.0]</span>,<span class="hljs-selector-attr">[3.0,4.0]</span>])<br><span class="hljs-selector-tag">q</span>,r = torch<span class="hljs-selector-class">.qr</span>(a)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(q,<span class="hljs-string">&quot;\n&quot;</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(r,<span class="hljs-string">&quot;\n&quot;</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(q@r)</span></span><br><br><br>#矩阵svd分解<br>#svd分解可以将任意一个矩阵分解为一个正交矩阵u,一个对角阵s和一个正交矩阵v<span class="hljs-selector-class">.t</span>()的乘积<br>#svd常用于矩阵压缩和降维<br>a=torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[1.0,2.0]</span>,<span class="hljs-selector-attr">[3.0,4.0]</span>,<span class="hljs-selector-attr">[5.0,6.0]</span>])<br><br>u,s,v = torch<span class="hljs-selector-class">.svd</span>(a)<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(u,<span class="hljs-string">&quot;\n&quot;</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(s,<span class="hljs-string">&quot;\n&quot;</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(v,<span class="hljs-string">&quot;\n&quot;</span>)</span></span><br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(u@torch.diag(s)</span></span>@v<span class="hljs-selector-class">.t</span>())<br><br>#利用svd分解可以在Pytorch中实现主成分分析降维<br></code></pre></td></tr></table></figure><h5 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h5><p>Pytorch的广播规则和numpy是一样的:</p><ul><li>1、如果张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度都一样。</li><li>2、如果两个张量在某个维度上的长度是相同的，或者其中一个张量在该维度上的长度为1，那么我们就说这两个张量在该维度上是相容的。</li><li>3、如果两个张量在所有维度上都是相容的，它们就能使用广播。</li><li>4、广播之后，每个维度的长度将取两个张量在该维度长度的较大值。</li><li>5、在任何一个维度上，如果一个张量的长度为1，另一个张量长度大于1，那么在该维度上，就好像是对第一个张量进行了复制。</li></ul><p>torch.broadcast_tensors可以将多个张量根据广播规则转换成相同的维度。</p><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记4-张量的结构操作</title>
    <link href="/2022/01/19/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%BC%A0%E9%87%8F%E7%9A%84%E7%BB%93%E6%9E%84%E6%93%8D%E4%BD%9C/"/>
    <url>/2022/01/19/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%BC%A0%E9%87%8F%E7%9A%84%E7%BB%93%E6%9E%84%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="张量的操作主要包括张量的结构操作和张量的数学运算"><a href="#张量的操作主要包括张量的结构操作和张量的数学运算" class="headerlink" title="张量的操作主要包括张量的结构操作和张量的数学运算"></a>张量的操作主要包括张量的结构操作和张量的数学运算</h2><span id="more"></span><p>张量结构操作诸如：张量创建，索引切片，维度变换，合并分割。<br>张量数学运算主要有：标量运算，向量运算，矩阵运算。另外我们会介绍张量运算的广播机制。</p><h5 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h5><p>张量创建的许多方法和numpy中创建array的方法很像</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">import numpy as np<br>import torch <br><br>a = torch.tensor([1,2,3],dtype = torch.float)<br>print(a)<br>&#x27;&#x27;&#x27;<br><span class="hljs-code">tensor([1., 2., 3.])</span><br>&#x27;&#x27;&#x27;<br><br>b = torch.arange(1,10,step = 2)<br>print(b)<br>&#x27;&#x27;&#x27;<br><span class="hljs-code">tensor([1, 3, 5, 7, 9])</span><br>&#x27;&#x27;&#x27;<br><br>c = torch.linspace(0.0, 2*3.14, 10)<br>print(c)<br>&#x27;&#x27;&#x27;<br><span class="hljs-code">tensor([0.0000, 0.6978, 1.3956, 2.0933, 2.7911, 3.4889, 4.1867, 4.8844, 5.5822,</span><br><span class="hljs-code">        6.2800])</span><br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure><p>另外还有 torch.ones()、torch.zero_like()、torch.zeros() 等方法创建</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># 均匀随机分布<br>torch.manual_seed(0)<br>minval, maxval = 0, 10<br>a = minval + (maxval - minval) * torch.rand([5])<br>print(a)<br>&#x27;&#x27;&#x27;<br><span class="hljs-code">tensor([4.9626, 7.6822, 0.8848, 1.3203, 3.0742])</span><br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># 正太分布随机<br>b = torch.normal(mean = torch.zeros(3,3), std = torch.ones(3,3))<br>print(b)<br>&#x27;&#x27;&#x27;<br><span class="hljs-code">tensor([[ 0.5507,  0.2704,  0.6472],</span><br><span class="hljs-code">        [ 0.2490, -0.3354,  0.4564],</span><br><span class="hljs-code">        [-0.6255,  0.4539, -1.3740]])</span><br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># 整数随机排列<br>d = torch.randperm(20)<br>print(d)<br>&#x27;&#x27;&#x27;<br><span class="hljs-code">tensor([ 3, 17,  9, 19,  1, 18,  4, 13, 15, 12,  0, 16,  7, 11,  2,  5,  8, 10,</span><br><span class="hljs-code">         6, 14])</span><br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure><p>此外还有 torch.eye() (单位矩阵) 、 torch.diag() (对角矩阵) 等</p><h5 id="索引切片"><a href="#索引切片" class="headerlink" title="索引切片"></a>索引切片</h5><p>张量的索引切片方式和numpy几乎是一样的。切片时支持缺省参数和省略号。可以通过索引和切片对部分元素进行修改。<br>此外，对于不规则的切片提取,可以使用 torch.index_select, torch.masked_select, torch.take<br>如果要通过修改张量的某些元素得到新的张量，可以使用 torch.where,torch.masked_fill,torch.index_fill</p><ul><li>torch.where可以理解为if的张量版本。</li><li>torch.index_fill的选取元素逻辑和torch.index_select相同。</li><li>torch.masked_fill的选取元素逻辑和torch.masked_select相同。</li></ul><h5 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h5><p>维度变换相关函数主要有 torch.reshape(或者调用张量的view方法), torch.squeeze, torch.unsqueeze, torch.transpose</p><ul><li>torch.reshape 可以改变张量的形状。</li><li>torch.squeeze 可以减少维度。</li><li>torch.unsqueeze 可以增加维度。</li><li>torch.transpose 可以交换维度。</li></ul><p>如果张量在某个维度上只有一个元素，利用torch.squeeze可以消除这个维度。<br>torch.unsqueeze的作用和torch.squeeze的作用相反。</p><p>torch.transpose可以交换张量的维度，torch.transpose常用于图片存储格式的变换上。<br>如果是二维的矩阵，通常会调用矩阵的转置方法 matrix.t()，等价于 torch.transpose(matrix,0,1)。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">minval=0<br>maxval=255<br># Batch,Height,Width,Channel<br>data = torch.floor(minval + (maxval-minval)*torch.rand([100,256,256,4])).int()<br>print(data.shape)<br><br># 转换成 Pytorch默认的图片格式 Batch,Channel,Height,Width <br># 需要交换两次<br>data<span class="hljs-emphasis">_t = torch.transpose(torch.transpose(data,1,2),1,3)</span><br><span class="hljs-emphasis">print(data_</span>t.shape)<br>&#x27;&#x27;&#x27;<br><span class="hljs-code">torch.Size([100, 256, 256, 4])</span><br><span class="hljs-code">torch.Size([100, 4, 256, 256])</span><br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure><h5 id="合并分割"><a href="#合并分割" class="headerlink" title="合并分割"></a>合并分割</h5><p>可以用torch.cat方法和torch.stack方法将多个张量合并，可以用torch.split方法把一个张量分割成多个张量。<br>torch.cat和torch.stack有略微的区别，torch.cat是连接，不会增加维度，而torch.stack是堆叠，会增加维度。<br>torch.split是torch.cat的逆运算，可以指定分割份数平均分割，也可以通过指定每份的记录数量进行分割</p><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记3-动态计算图</title>
    <link href="/2022/01/18/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03-%E5%8A%A8%E6%80%81%E8%AE%A1%E7%AE%97%E5%9B%BE/"/>
    <url>/2022/01/18/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03-%E5%8A%A8%E6%80%81%E8%AE%A1%E7%AE%97%E5%9B%BE/</url>
    
    <content type="html"><![CDATA[<h2 id="动态计算图"><a href="#动态计算图" class="headerlink" title="动态计算图"></a>动态计算图</h2><span id="more"></span><h5 id="Pytorch的动态计算图"><a href="#Pytorch的动态计算图" class="headerlink" title="Pytorch的动态计算图"></a>Pytorch的动态计算图</h5><p>包括：</p><ul><li>动态计算图简介</li><li>计算图中的Function</li><li>计算图和反向传播</li><li>叶子节点和非叶子节点</li><li>计算图在TensorBoard中的可视化</li></ul><h5 id="动态计算图简介"><a href="#动态计算图简介" class="headerlink" title="动态计算图简介"></a>动态计算图简介</h5><p>Pytorch的计算图由节点和边组成，节点表示张量或者Function，边表示张量和Function之间的依赖关系。<br>Pytorch中的计算图是动态图。这里的动态主要有两重含义。<br>第一层含义是：计算图的正向传播是立即执行的。无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到计算结果。<br>第二层含义是：计算图在反向传播后立即销毁。下次调用需要重新构建计算图。如果在程序中使用了backward方法执行了反向传播，或者利用torch.autograd.grad方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建。</p><h6 id="计算图的正向传播是立即执行的。"><a href="#计算图的正向传播是立即执行的。" class="headerlink" title="计算图的正向传播是立即执行的。"></a>计算图的正向传播是立即执行的。</h6><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">import torch <br>w = torch.tensor([[3.0,1.0]],requires<span class="hljs-emphasis">_grad=True)</span><br><span class="hljs-emphasis">b = torch.tensor([[3.0]],requires_grad=True)</span><br><span class="hljs-emphasis">X = torch.randn(10,2)</span><br><span class="hljs-emphasis">Y = torch.randn(10,1)</span><br><span class="hljs-emphasis">Y_hat = X@w.t() + b  # Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span><br><span class="hljs-emphasis">loss = torch.mean(torch.pow(Y_</span>hat-Y,2))<br><br>print(loss.data)<br>print(Y_hat.data)<br><br>&#x27;&#x27;&#x27;<br><span class="hljs-code">tensor(16.8885)</span><br><span class="hljs-code">tensor([[ 3.3509],</span><br><span class="hljs-code">        [-2.5233],</span><br><span class="hljs-code">        [ 5.1586],</span><br><span class="hljs-code">        [ 4.9135],</span><br><span class="hljs-code">        [ 1.0449],</span><br><span class="hljs-code">        [ 8.0712],</span><br><span class="hljs-code">        [ 5.0686],</span><br><span class="hljs-code">        [ 0.5840],</span><br><span class="hljs-code">        [-0.0614],</span><br><span class="hljs-code">        [ 2.7492]])</span><br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure><h6 id="计算图在反向传播后立即销毁。"><a href="#计算图在反向传播后立即销毁。" class="headerlink" title="计算图在反向传播后立即销毁。"></a>计算图在反向传播后立即销毁。</h6><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> torch <br><span class="hljs-attribute">w</span> = torch.tensor([[<span class="hljs-number">3</span>.<span class="hljs-number">0</span>,<span class="hljs-number">1</span>.<span class="hljs-number">0</span>]],requires_grad=True)<br><span class="hljs-attribute">b</span> = torch.tensor([[<span class="hljs-number">3</span>.<span class="hljs-number">0</span>]],requires_grad=True)<br><span class="hljs-attribute">X</span> = torch.randn(<span class="hljs-number">10</span>,<span class="hljs-number">2</span>)<br><span class="hljs-attribute">Y</span> = torch.randn(<span class="hljs-number">10</span>,<span class="hljs-number">1</span>)<br><span class="hljs-attribute">Y_hat</span> = X@w.t() + b  # Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关<br><span class="hljs-attribute">loss</span> = torch.mean(torch.pow(Y_hat-Y,<span class="hljs-number">2</span>))<br><br><span class="hljs-comment">#计算图在反向传播后立即销毁，如果需要保留计算图, 需要设置retain_graph = True</span><br><span class="hljs-attribute">loss</span>.backward()  #loss.backward(retain_graph = True) <br><br><span class="hljs-comment">#loss.backward() #如果再次执行反向传播将报错</span><br></code></pre></td></tr></table></figure><h5 id="计算图中的Function"><a href="#计算图中的Function" class="headerlink" title="计算图中的Function"></a>计算图中的Function</h5><p>计算图中的 张量我们已经比较熟悉了, 计算图中的另外一种节点是Function, 实际上就是 Pytorch中各种对张量操作的函数。<br>这些Function和我们Python中的函数有一个较大的区别，那就是它同时包括正向计算逻辑和反向传播的逻辑。<br>我们可以通过继承torch.autograd.Function来创建这种支持反向传播的Function</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyReLU</span>(<span class="hljs-params">torch.autograd.Function</span>):</span><br>   <br>    <span class="hljs-comment">#正向传播逻辑，可以用ctx存储一些值，供反向传播使用。</span><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">ctx, <span class="hljs-built_in">input</span></span>):</span><br>        ctx.save_for_backward(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">input</span>.clamp(<span class="hljs-built_in">min</span>=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment">#反向传播逻辑</span><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">ctx, grad_output</span>):</span><br>        <span class="hljs-built_in">input</span>, = ctx.saved_tensors<br>        grad_input = grad_output.clone()<br>        grad_input[<span class="hljs-built_in">input</span> &lt; <span class="hljs-number">0</span>] = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">return</span> grad_input<br><br><br>w = torch.tensor([[<span class="hljs-number">3.0</span>,<span class="hljs-number">1.0</span>]],requires_grad=<span class="hljs-literal">True</span>)<br>b = torch.tensor([[<span class="hljs-number">3.0</span>]],requires_grad=<span class="hljs-literal">True</span>)<br>X = torch.tensor([[-<span class="hljs-number">1.0</span>,-<span class="hljs-number">1.0</span>],[<span class="hljs-number">1.0</span>,<span class="hljs-number">1.0</span>]])<br>Y = torch.tensor([[<span class="hljs-number">2.0</span>,<span class="hljs-number">3.0</span>]])<br><br>relu = MyReLU.apply <span class="hljs-comment"># relu现在也可以具有正向传播和反向传播功能</span><br>Y_hat = relu(X@w.t() + b)<br>loss = torch.mean(torch.<span class="hljs-built_in">pow</span>(Y_hat-Y,<span class="hljs-number">2</span>))<br><br>loss.backward()<br><br><span class="hljs-built_in">print</span>(w.grad)<br><span class="hljs-built_in">print</span>(b.grad)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[4.5000, 4.5000]])</span><br><span class="hljs-string">tensor([[4.5000]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># Y<span class="hljs-emphasis">_hat的梯度函数即是我们自己所定义的 MyReLU.backward</span><br><span class="hljs-emphasis">print(Y_hat.grad_</span>fn)<br>&#x27;&#x27;&#x27;<br><span class="hljs-code">&lt;torch.autograd.function.MyReLUBackward object at 0x7efe582c7ba8&gt;</span><br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure><h5 id="计算图与反向传播"><a href="#计算图与反向传播" class="headerlink" title="计算图与反向传播"></a>计算图与反向传播</h5><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> torch <br><br><span class="hljs-attribute">x</span> = torch.tensor(<span class="hljs-number">3</span>.<span class="hljs-number">0</span>,requires_grad=True)<br><span class="hljs-attribute">y1</span> = x + <span class="hljs-number">1</span><br><span class="hljs-attribute">y2</span> = <span class="hljs-number">2</span>*x<br><span class="hljs-attribute">loss</span> = (y<span class="hljs-number">1</span>-y<span class="hljs-number">2</span>)**<span class="hljs-number">2</span><br><br><span class="hljs-attribute">loss</span>.backward()<br></code></pre></td></tr></table></figure><p>loss.backward()语句调用后，依次发生以下计算过程。</p><ul><li>1，loss自己的grad梯度赋值为1，即对自身的梯度为1。</li><li>2，loss根据其自身梯度以及关联的backward方法，计算出其对应的自变量即y1和y2的梯度，将该值赋值到y1.grad和y2.grad。</li><li>3，y2和y1根据其自身梯度以及关联的backward方法, 分别计算出其对应的自变量x的梯度，x.grad将其收到的多个梯度值累加。</li></ul><p>（注意，1,2,3步骤的求梯度顺序和对多个梯度值的累加规则恰好是求导链式法则的程序表述）<br>正因为求导链式法则衍生的梯度累加规则，张量的grad梯度不会自动清零，在需要的时候需要手动置零。</p><h5 id="叶子节点和非叶子节点"><a href="#叶子节点和非叶子节点" class="headerlink" title="叶子节点和非叶子节点"></a>叶子节点和非叶子节点</h5><p>执行下面代码，我们会发现 loss.grad并不是我们期望的1,而是 None。类似地 y1.grad 以及 y2.grad也是 None.<br>这是为什么呢？这是由于它们不是叶子节点张量。<br>在反向传播过程中，只有 is_leaf=True 的叶子节点，需要求导的张量的导数结果才会被最后保留下来。<br>那么什么是叶子节点张量呢？叶子节点张量需要满足两个条件。</p><ul><li>1，叶子节点张量是由用户直接创建的张量，而非由某个Function通过计算得到的张量。</li><li>2，叶子节点张量的 requires_grad属性必须为True.</li></ul><p>Pytorch设计这样的规则主要是为了节约内存或者显存空间，因为几乎所有的时候，用户只会关心他自己直接创建的张量的梯度。<br>所有依赖于叶子节点张量的张量, 其requires_grad 属性必定是True的，但其梯度值只在计算过程中被用到，不会最终存储到grad属性中。<br>如果需要保留中间计算结果的梯度到grad属性中，可以使用 retain_grad方法。<br>如果仅仅是为了调试代码查看梯度值，可以利用register_hook打印日志。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">import torch <br><br>x = torch.tensor(3.0,requires_grad=True)<br>y1 = x + 1<br>y2 = 2*x<br>loss = (y1-y2)**2<br><br>loss.backward()<br>print(&quot;loss.grad:&quot;, loss.grad)<br>print(&quot;y1.grad:&quot;, y1.grad)<br>print(&quot;y2.grad:&quot;, y2.grad)<br>print(x.grad)<br><br>&#x27;&#x27;&#x27;<br><span class="hljs-code">loss.grad: None</span><br><span class="hljs-code">y1.grad: None</span><br><span class="hljs-code">y2.grad: None</span><br><span class="hljs-code">tensor(4.)</span><br>&#x27;&#x27;&#x27;<br><br>print(x.is<span class="hljs-emphasis">_leaf)</span><br><span class="hljs-emphasis">print(y1.is_leaf)</span><br><span class="hljs-emphasis">print(y2.is_leaf)</span><br><span class="hljs-emphasis">print(loss.is_</span>leaf)<br><br>&#x27;&#x27;&#x27;<br><span class="hljs-code">True</span><br><span class="hljs-code">False</span><br><span class="hljs-code">False</span><br><span class="hljs-code">False</span><br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure><p>利用retain_grad可以保留非叶子节点的梯度值，利用register_hook可以查看非叶子节点的梯度值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <br><br><span class="hljs-comment">#正向传播</span><br>x = torch.tensor(<span class="hljs-number">3.0</span>,requires_grad=<span class="hljs-literal">True</span>)<br>y1 = x + <span class="hljs-number">1</span><br>y2 = <span class="hljs-number">2</span>*x<br>loss = (y1-y2)**<span class="hljs-number">2</span><br><br><span class="hljs-comment">#非叶子节点梯度显示控制</span><br>y1.register_hook(<span class="hljs-keyword">lambda</span> grad: <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;y1 grad: &#x27;</span>, grad))<br>y2.register_hook(<span class="hljs-keyword">lambda</span> grad: <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;y2 grad: &#x27;</span>, grad))<br>loss.retain_grad()<br><br><span class="hljs-comment">#反向传播</span><br>loss.backward()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;loss.grad:&quot;</span>, loss.grad)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x.grad:&quot;</span>, x.grad)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">y2 grad:  tensor(4.)</span><br><span class="hljs-string">y1 grad:  tensor(-4.)</span><br><span class="hljs-string">loss.grad: tensor(1.)</span><br><span class="hljs-string">x.grad: tensor(4.)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h5 id="计算图在TensorBoard中的可视化"><a href="#计算图在TensorBoard中的可视化" class="headerlink" title="计算图在TensorBoard中的可视化"></a>计算图在TensorBoard中的可视化</h5><p>可以利用 torch.utils.tensorboard 将计算图导出到 TensorBoard进行可视化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Net</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>(Net, self).__init__()<br>        self.w = nn.Parameter(torch.randn(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>))<br>        self.b = nn.Parameter(torch.zeros(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        y = x@self.w + self.b<br>        <span class="hljs-keyword">return</span> y<br><br>net = Net()<br><br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br>writer = SummaryWriter(<span class="hljs-string">&#x27;./data/tensorboard&#x27;</span>)<br>writer.add_graph(net,input_to_model = torch.rand(<span class="hljs-number">10</span>,<span class="hljs-number">2</span>))<br>writer.close()<br><br>%load_ext tensorboard<br><span class="hljs-comment">#%tensorboard --logdir ./data/tensorboard</span><br><br><span class="hljs-keyword">from</span> tensorboard <span class="hljs-keyword">import</span> notebook<br>notebook.<span class="hljs-built_in">list</span>()<br><br><span class="hljs-comment">#在tensorboard中查看模型</span><br>notebook.start(<span class="hljs-string">&quot;--logdir ./data/tensorboard&quot;</span>)<br></code></pre></td></tr></table></figure><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记2-自动微分机制</title>
    <link href="/2022/01/18/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E6%9C%BA%E5%88%B6/"/>
    <url>/2022/01/18/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="自动微分机制"><a href="#自动微分机制" class="headerlink" title="自动微分机制"></a>自动微分机制</h2><span id="more"></span><p>神经网络通常依赖反向传播求梯度来更新网络参数，求梯度过程通常是一件非常复杂而容易出错的事情。而深度学习框架可以帮助我们自动地完成这种求梯度运算。<br>Pytorch一般通过反向传播 backward 方法 实现这种求梯度计算。该方法求得的梯度将存在对应自变量张量的grad属性下。除此之外，也能够调用torch.autograd.grad 函数来实现求梯度计算。这就是Pytorch的自动微分机制。</p><h5 id="利用backward方法求导数"><a href="#利用backward方法求导数" class="headerlink" title="利用backward方法求导数"></a>利用backward方法求导数</h5><p>backward 方法通常在一个标量张量上调用，该方法求得的梯度将存在对应自变量张量的grad属性下。如果调用的张量非标量，则要传入一个和它同形状 的gradient参数张量。相当于用该gradient参数张量与调用张量作向量点乘，得到的标量结果再反向传播。</p><h6 id="标量的反向传播"><a href="#标量的反向传播" class="headerlink" title="标量的反向传播"></a>标量的反向传播</h6><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">import numpy as np <br>import torch <br><br># f(x) = a*x**2 <span class="hljs-code">+ b*x +</span> c的导数<br><br>x = torch.tensor(0.0,requires_grad = True) # x需要被求导<br>a = torch.tensor(1.0)<br>b = torch.tensor(-2.0)<br>c = torch.tensor(1.0)<br>y = a*torch.pow(x,2) <span class="hljs-code">+ b*x +</span> c <br><br>y.backward()<br>dy<span class="hljs-emphasis">_dx = x.grad</span><br><span class="hljs-emphasis">print(dy_</span>dx)<br><br>&#x27;&#x27;&#x27;<br><span class="hljs-code">tensor(-2.)</span><br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure><h6 id="非标量的反向传播"><a href="#非标量的反向传播" class="headerlink" title="非标量的反向传播"></a>非标量的反向传播</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br><span class="hljs-keyword">import</span> torch <br><br><span class="hljs-comment"># f(x) = a*x**2 + b*x + c</span><br><br>x = torch.tensor([[<span class="hljs-number">0.0</span>,<span class="hljs-number">0.0</span>],[<span class="hljs-number">1.0</span>,<span class="hljs-number">2.0</span>]],requires_grad = <span class="hljs-literal">True</span>) <span class="hljs-comment"># x需要被求导</span><br>a = torch.tensor(<span class="hljs-number">1.0</span>)<br>b = torch.tensor(-<span class="hljs-number">2.0</span>)<br>c = torch.tensor(<span class="hljs-number">1.0</span>)<br>y = a*torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>) + b*x + c <br><br>gradient = torch.tensor([[<span class="hljs-number">1.0</span>,<span class="hljs-number">1.0</span>],[<span class="hljs-number">1.0</span>,<span class="hljs-number">1.0</span>]])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x:\n&quot;</span>,x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y:\n&quot;</span>,y)<br>y.backward(gradient = gradient)<br>x_grad = x.grad<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_grad:\n&quot;</span>,x_grad)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">x:</span><br><span class="hljs-string"> tensor([[0., 0.],</span><br><span class="hljs-string">        [1., 2.]], requires_grad=True)</span><br><span class="hljs-string">y:</span><br><span class="hljs-string"> tensor([[1., 1.],</span><br><span class="hljs-string">        [0., 1.]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="hljs-string">x_grad:</span><br><span class="hljs-string"> tensor([[-2., -2.],</span><br><span class="hljs-string">        [ 0.,  2.]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h6 id="非标量的反向传播可以用标量的反向传播实现"><a href="#非标量的反向传播可以用标量的反向传播实现" class="headerlink" title="非标量的反向传播可以用标量的反向传播实现"></a>非标量的反向传播可以用标量的反向传播实现</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br><span class="hljs-keyword">import</span> torch <br><br><span class="hljs-comment"># f(x) = a*x**2 + b*x + c</span><br><br>x = torch.tensor([[<span class="hljs-number">0.0</span>,<span class="hljs-number">0.0</span>],[<span class="hljs-number">1.0</span>,<span class="hljs-number">2.0</span>]],requires_grad = <span class="hljs-literal">True</span>) <span class="hljs-comment"># x需要被求导</span><br>a = torch.tensor(<span class="hljs-number">1.0</span>)<br>b = torch.tensor(-<span class="hljs-number">2.0</span>)<br>c = torch.tensor(<span class="hljs-number">1.0</span>)<br>y = a*torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>) + b*x + c <br><br>gradient = torch.tensor([[<span class="hljs-number">1.0</span>,<span class="hljs-number">1.0</span>],[<span class="hljs-number">1.0</span>,<span class="hljs-number">1.0</span>]])<br>z = torch.<span class="hljs-built_in">sum</span>(y*gradient)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x:&quot;</span>,x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y:&quot;</span>,y)<br>z.backward()<br>x_grad = x.grad<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_grad:\n&quot;</span>,x_grad)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">x: tensor([[0., 0.],</span><br><span class="hljs-string">        [1., 2.]], requires_grad=True)</span><br><span class="hljs-string">y: tensor([[1., 1.],</span><br><span class="hljs-string">        [0., 1.]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="hljs-string">x_grad:</span><br><span class="hljs-string"> tensor([[-2., -2.],</span><br><span class="hljs-string">        [ 0.,  2.]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h5 id="利用autograd-grad方法求导数"><a href="#利用autograd-grad方法求导数" class="headerlink" title="利用autograd.grad方法求导数"></a>利用autograd.grad方法求导数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br><span class="hljs-keyword">import</span> torch <br><br><span class="hljs-comment"># f(x) = a*x**2 + b*x + c的导数</span><br><br>x = torch.tensor(<span class="hljs-number">0.0</span>,requires_grad = <span class="hljs-literal">True</span>) <span class="hljs-comment"># x需要被求导</span><br>a = torch.tensor(<span class="hljs-number">1.0</span>)<br>b = torch.tensor(-<span class="hljs-number">2.0</span>)<br>c = torch.tensor(<span class="hljs-number">1.0</span>)<br>y = a*torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>) + b*x + c<br><br><br><span class="hljs-comment"># create_graph 设置为 True 将允许创建更高阶的导数 </span><br>dy_dx = torch.autograd.grad(y,x,create_graph=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(dy_dx.data)<br><br><span class="hljs-comment"># 求二阶导数</span><br>dy2_dx2 = torch.autograd.grad(dy_dx,x)[<span class="hljs-number">0</span>] <br><br><span class="hljs-built_in">print</span>(dy2_dx2.data)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor(-2.)</span><br><span class="hljs-string">tensor(2.)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">import numpy as np <br>import torch <br><br>x1 = torch.tensor(1.0,requires<span class="hljs-emphasis">_grad = True) # x需要被求导</span><br><span class="hljs-emphasis">x2 = torch.tensor(2.0,requires_</span>grad = True)<br><br>y1 = x1*x2<br>y2 = x1+x2<br><br><br># 允许同时对多个自变量求导数<br>(dy1<span class="hljs-emphasis">_dx1,dy1_dx2) = torch.autograd.grad(outputs=y1,inputs = [x1,x2],retain_graph = True)</span><br><span class="hljs-emphasis">print(dy1_dx1,dy1_</span>dx2)<br><br># 如果有多个因变量，相当于把多个因变量的梯度结果求和<br>(dy12<span class="hljs-emphasis">_dx1,dy12_dx2) = torch.autograd.grad(outputs=[y1,y2],inputs = [x1,x2])</span><br><span class="hljs-emphasis">print(dy12_dx1,dy12_</span>dx2)<br><br>&#x27;&#x27;&#x27;<br><span class="hljs-code">tensor(2.) tensor(1.)</span><br><span class="hljs-code">tensor(3.) tensor(2.)</span><br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure><h5 id="利用自动微分和优化器求最小值"><a href="#利用自动微分和优化器求最小值" class="headerlink" title="利用自动微分和优化器求最小值"></a>利用自动微分和优化器求最小值</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br><span class="hljs-keyword">import</span> torch <br><br><span class="hljs-comment"># f(x) = a*x**2 + b*x + c的最小值</span><br><br>x = torch.tensor(<span class="hljs-number">0.0</span>,requires_grad = <span class="hljs-literal">True</span>) <span class="hljs-comment"># x需要被求导</span><br>a = torch.tensor(<span class="hljs-number">1.0</span>)<br>b = torch.tensor(-<span class="hljs-number">2.0</span>)<br>c = torch.tensor(<span class="hljs-number">1.0</span>)<br><br>optimizer = torch.optim.SGD(params=[x],lr = <span class="hljs-number">0.01</span>)<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f</span>(<span class="hljs-params">x</span>):</span><br>    result = a*torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>) + b*x + c <br>    <span class="hljs-keyword">return</span>(result)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">500</span>):<br>    optimizer.zero_grad()<br>    y = f(x)<br>    y.backward()<br>    optimizer.step()<br>   <br>    <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y=&quot;</span>,f(x).data,<span class="hljs-string">&quot;;&quot;</span>,<span class="hljs-string">&quot;x=&quot;</span>,x.data)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">y= tensor(0.) ; x= tensor(1.0000)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记1-张量数据结构</title>
    <link href="/2022/01/18/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E5%BC%A0%E9%87%8F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    <url>/2022/01/18/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E5%BC%A0%E9%87%8F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<h2 id="张量的数据类型、张量的维度、张量的尺寸、张量和numpy数组等"><a href="#张量的数据类型、张量的维度、张量的尺寸、张量和numpy数组等" class="headerlink" title="张量的数据类型、张量的维度、张量的尺寸、张量和numpy数组等"></a>张量的数据类型、张量的维度、张量的尺寸、张量和numpy数组等</h2><span id="more"></span><p>Pytorch的基本数据结构是张量Tensor。张量即多维数组。Pytorch的张量和numpy中的array很类似。</p><h5 id="张量的数据类型"><a href="#张量的数据类型" class="headerlink" title="张量的数据类型"></a>张量的数据类型</h5><p>张量的数据类型和numpy.array基本一一对应，但是不支持str类型。包括:</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs angelscript">torch.<span class="hljs-built_in">float</span>64(torch.<span class="hljs-built_in">double</span>),<br><br>torch.<span class="hljs-built_in">float</span>32(torch.<span class="hljs-built_in">float</span>),<br><br>torch.<span class="hljs-built_in">float</span>16,<br><br>torch.<span class="hljs-built_in">int</span>64(torch.long),<br><br>torch.<span class="hljs-built_in">int</span>32(torch.<span class="hljs-built_in">int</span>),<br><br>torch.<span class="hljs-built_in">int</span>16,<br><br>torch.<span class="hljs-built_in">int</span>8,<br><br>torch.<span class="hljs-built_in">uint</span>8,<br><br>torch.<span class="hljs-built_in">bool</span><br></code></pre></td></tr></table></figure><p>一般神经网络建模使用的都是torch.float32类型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch <br><br><span class="hljs-comment"># 自动推断数据类型</span><br>i = torch.tensor(<span class="hljs-number">1</span>);<span class="hljs-built_in">print</span>(i,i.dtype)<br>x = torch.tensor(<span class="hljs-number">2.0</span>);<span class="hljs-built_in">print</span>(x,x.dtype)<br>b = torch.tensor(<span class="hljs-literal">True</span>);<span class="hljs-built_in">print</span>(b,b.dtype)<br><span class="hljs-string">&#x27;&#x27;&#x27; # 输出</span><br><span class="hljs-string">tensor(1) torch.int64</span><br><span class="hljs-string">tensor(2.) torch.float32</span><br><span class="hljs-string">tensor(True) torch.bool</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># 指定数据类型</span><br>i = torch.tensor(<span class="hljs-number">1</span>,dtype = torch.int32);<span class="hljs-built_in">print</span>(i,i.dtype)<br>x = torch.tensor(<span class="hljs-number">2.0</span>,dtype = torch.double);<span class="hljs-built_in">print</span>(x,x.dtype)<br>‘’‘<br>tensor(<span class="hljs-number">1</span>, dtype=torch.int32) torch.int32<br>tensor(<span class="hljs-number">2.</span>, dtype=torch.float64) torch.float64<br>’‘’<br><br><span class="hljs-comment"># 使用特定类型构造函数</span><br>i = torch.IntTensor(<span class="hljs-number">1</span>);<span class="hljs-built_in">print</span>(i,i.dtype)<br>x = torch.Tensor(np.array(<span class="hljs-number">2.0</span>));<span class="hljs-built_in">print</span>(x,x.dtype) <span class="hljs-comment">#等价于torch.FloatTensor</span><br>b = torch.BoolTensor(np.array([<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>])); <span class="hljs-built_in">print</span>(b,b.dtype)<br>‘’‘<br>tensor([<span class="hljs-number">1266789664</span>], dtype=torch.int32) torch.int32<br>tensor(<span class="hljs-number">2.</span>) torch.float32<br>tensor([ <span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>,  <span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>]) torch.<span class="hljs-built_in">bool</span><br>’‘’<br><br><span class="hljs-comment"># 不同类型进行转换</span><br>i = torch.tensor(<span class="hljs-number">1</span>); <span class="hljs-built_in">print</span>(i,i.dtype)<br>x = i.<span class="hljs-built_in">float</span>(); <span class="hljs-built_in">print</span>(x,x.dtype) <span class="hljs-comment">#调用 float方法转换成浮点类型</span><br>y = i.<span class="hljs-built_in">type</span>(torch.<span class="hljs-built_in">float</span>); <span class="hljs-built_in">print</span>(y,y.dtype) <span class="hljs-comment">#使用type函数转换成浮点类型</span><br>z = i.type_as(x);<span class="hljs-built_in">print</span>(z,z.dtype) <span class="hljs-comment">#使用type_as方法转换成某个Tensor相同类型</span><br>‘’‘<br>tensor(<span class="hljs-number">1</span>) torch.int64<br>tensor(<span class="hljs-number">1.</span>) torch.float32<br>tensor(<span class="hljs-number">1.</span>) torch.float32<br>tensor(<span class="hljs-number">1.</span>) torch.float32<br>’‘’<br></code></pre></td></tr></table></figure><h5 id="张量的维度"><a href="#张量的维度" class="headerlink" title="张量的维度"></a>张量的维度</h5><p>不同类型的数据可以用不同维度(dimension)的张量来表示。标量为0维张量，向量为1维张量，矩阵为2维张量。彩色图像有rgb三个通道，可以表示为3维张量。视频还有时间维，可以表示为4维张量。<br>可以简单地总结为：有几层中括号，就是多少维的张量。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scalar = torch.tensor(True)<br>print(scalar)<br>print(scalar.dim())  # 标量，0维张量<br>&#x27;&#x27;&#x27;<br><span class="hljs-code">tensor(True)</span><br><span class="hljs-code">0</span><br>&#x27;&#x27;&#x27;<br><br>vector = torch.tensor([1.0,2.0,3.0,4.0]) #向量，1维张量<br>print(vector)<br>print(vector.dim())<br>&#x27;&#x27;&#x27;<br><span class="hljs-code">tensor([1., 2., 3., 4.])</span><br><span class="hljs-code">1</span><br>&#x27;&#x27;&#x27;<br><br>matrix = torch.tensor([[1.0,2.0],[3.0,4.0]]) #矩阵, 2维张量<br>print(matrix)<br>print(matrix.dim())<br>&#x27;&#x27;&#x27;<br><span class="hljs-code">tensor([[1., 2.],</span><br><span class="hljs-code">        [3., 4.]])</span><br><span class="hljs-code">2</span><br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure><h5 id="张量的尺寸"><a href="#张量的尺寸" class="headerlink" title="张量的尺寸"></a>张量的尺寸</h5><p>可以使用 shape属性或者 size()方法查看张量在每一维的长度.可以使用 view 方法改变张量的尺寸。<br>如果view方法改变尺寸失败，可以使用reshape方法.</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"># 使用view可以改变张量尺寸<br>vector = torch.arange(0,12)<br>print(vector)<br>print(vector.shape)<br><br>matrix34 = vector.view(3,4)<br>print(matrix34)<br>print(matrix34.shape)<br><br>matrix43 = vector.view(4,-1) #-1表示该位置长度由程序自动推断<br>print(matrix43)<br>print(matrix43.shape)<br><br>&#x27;&#x27;&#x27;<br><span class="hljs-code">tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span><br><span class="hljs-code">torch.Size([12])</span><br><span class="hljs-code">tensor([[ 0,  1,  2,  3],</span><br><span class="hljs-code">        [ 4,  5,  6,  7],</span><br><span class="hljs-code">        [ 8,  9, 10, 11]])</span><br><span class="hljs-code">torch.Size([3, 4])</span><br><span class="hljs-code">tensor([[ 0,  1,  2],</span><br><span class="hljs-code">        [ 3,  4,  5],</span><br><span class="hljs-code">        [ 6,  7,  8],</span><br><span class="hljs-code">        [ 9, 10, 11]])</span><br><span class="hljs-code">torch.Size([4, 3])</span><br>&#x27;&#x27;&#x27;<br><br># 有些操作会让张量存储结构扭曲，直接使用view会失败，可以用reshape方法<br>matrix26 = torch.arange(0,12).view(2,6)<br>print(matrix26)<br>print(matrix26.shape)<br><br># 转置操作让张量存储结构扭曲<br>matrix62 = matrix26.t()<br>print(matrix62.is_contiguous())<br><br># 直接使用view方法会失败，可以使用reshape方法<br>#matrix34 = matrix62.view(3,4) #error!<br>matrix34 = matrix62.reshape(3,4) #等价于matrix34 = matrix62.contiguous().view(3,4)<br>print(matrix34)<br><br>&#x27;&#x27;&#x27;<br><span class="hljs-code">tensor([[ 0,  1,  2,  3,  4,  5],</span><br><span class="hljs-code">        [ 6,  7,  8,  9, 10, 11]])</span><br><span class="hljs-code">torch.Size([2, 6])</span><br><span class="hljs-code">False</span><br><span class="hljs-code">tensor([[ 0,  6,  1,  7],</span><br><span class="hljs-code">        [ 2,  8,  3,  9],</span><br><span class="hljs-code">        [ 4, 10,  5, 11]])</span><br>&#x27;&#x27;&#x27;<br></code></pre></td></tr></table></figure><h5 id="张量和numpy数组"><a href="#张量和numpy数组" class="headerlink" title="张量和numpy数组"></a>张量和numpy数组</h5><p>可以用numpy方法从Tensor得到numpy数组，也可以用torch.from_numpy从numpy数组得到Tensor。这两种方法关联的Tensor和numpy数组是共享数据内存的。如果改变其中一个，另外一个的值也会发生改变。如果有需要，可以用张量的clone方法拷贝张量，中断这种关联。<br>此外，还可以使用item方法从标量张量得到对应的Python数值。使用tolist方法从张量得到对应的Python数值列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#torch.from_numpy函数从numpy数组得到Tensor</span><br>arr = np.zeros(<span class="hljs-number">3</span>)<br>tensor = torch.from_numpy(arr)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;before add 1:&quot;</span>)<br><span class="hljs-built_in">print</span>(arr)<br><span class="hljs-built_in">print</span>(tensor)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nafter add 1:&quot;</span>)<br>np.add(arr,<span class="hljs-number">1</span>, out = arr) <span class="hljs-comment">#给 arr增加1，tensor也随之改变</span><br><span class="hljs-built_in">print</span>(arr)<br><span class="hljs-built_in">print</span>(tensor)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">before add 1:</span><br><span class="hljs-string">[0. 0. 0.]</span><br><span class="hljs-string">tensor([0., 0., 0.], dtype=torch.float64)</span><br><span class="hljs-string"></span><br><span class="hljs-string">after add 1:</span><br><span class="hljs-string">[1. 1. 1.]</span><br><span class="hljs-string">tensor([1., 1., 1.], dtype=torch.float64)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># numpy方法从Tensor得到numpy数组</span><br>tensor = torch.zeros(<span class="hljs-number">3</span>)<br>arr = tensor.numpy()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;before add 1:&quot;</span>)<br><span class="hljs-built_in">print</span>(tensor)<br><span class="hljs-built_in">print</span>(arr)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nafter add 1:&quot;</span>)<br><span class="hljs-comment">#使用带下划线的方法表示计算结果会返回给调用 张量</span><br>tensor.add_(<span class="hljs-number">1</span>) <span class="hljs-comment">#给 tensor增加1，arr也随之改变 </span><br><span class="hljs-comment">#或： torch.add(tensor,1,out = tensor)</span><br><span class="hljs-built_in">print</span>(tensor)<br><span class="hljs-built_in">print</span>(arr)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">before add 1:</span><br><span class="hljs-string">tensor([0., 0., 0.])</span><br><span class="hljs-string">[0. 0. 0.]</span><br><span class="hljs-string"></span><br><span class="hljs-string">after add 1:</span><br><span class="hljs-string">tensor([1., 1., 1.])</span><br><span class="hljs-string">[1. 1. 1.]</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># 可以用clone() 方法拷贝张量，中断这种关联</span><br>tensor = torch.zeros(<span class="hljs-number">3</span>)<br><br><span class="hljs-comment">#使用clone方法拷贝张量, 拷贝后的张量和原始张量内存独立</span><br>arr = tensor.clone().numpy() <span class="hljs-comment"># 也可以使用tensor.data.numpy()</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;before add 1:&quot;</span>)<br><span class="hljs-built_in">print</span>(tensor)<br><span class="hljs-built_in">print</span>(arr)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nafter add 1:&quot;</span>)<br><span class="hljs-comment">#使用 带下划线的方法表示计算结果会返回给调用 张量</span><br>tensor.add_(<span class="hljs-number">1</span>) <span class="hljs-comment">#给 tensor增加1，arr不再随之改变</span><br><span class="hljs-built_in">print</span>(tensor)<br><span class="hljs-built_in">print</span>(arr)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">before add 1:</span><br><span class="hljs-string">tensor([0., 0., 0.])</span><br><span class="hljs-string">[0. 0. 0.]</span><br><span class="hljs-string"></span><br><span class="hljs-string">after add 1:</span><br><span class="hljs-string">tensor([1., 1., 1.])</span><br><span class="hljs-string">[0. 0. 0.]</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># item方法和tolist方法可以将张量转换成Python数值和数值列表</span><br>scalar = torch.tensor(<span class="hljs-number">1.0</span>)<br>s = scalar.item()<br><span class="hljs-built_in">print</span>(s)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(s))<br><br>tensor = torch.rand(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>t = tensor.tolist()<br><span class="hljs-built_in">print</span>(t)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(t))<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">1.0</span><br><span class="hljs-string">&lt;class &#x27;float&#x27;&gt;</span><br><span class="hljs-string">[[0.5526873469352722, 0.46957558393478394], [0.6724914312362671, 0.26923561096191406]]</span><br><span class="hljs-string">&lt;class &#x27;list&#x27;&gt;</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>搬运自：</p><ul><li><a href="https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1">https://www.heywhale.com/home/competition/61bff9a84b63a700179b7f8d/content/1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第一个python包</title>
    <link href="/2021/12/29/%E7%AC%AC%E4%B8%80%E4%B8%AApython%E5%8C%85/"/>
    <url>/2021/12/29/%E7%AC%AC%E4%B8%80%E4%B8%AApython%E5%8C%85/</url>
    
    <content type="html"><![CDATA[<h2 id="Senior-Data-Structure-Tools–SDStools"><a href="#Senior-Data-Structure-Tools–SDStools" class="headerlink" title="Senior Data Structure Tools–SDStools"></a>Senior Data Structure Tools–SDStools</h2><span id="more"></span><p>python标准库中没有链表、树、图等高级数据结构，所以整理了一些网上的代码到这个库中。</p><h5 id="链表："><a href="#链表：" class="headerlink" title="链表："></a>链表：</h5><ul><li><a href="https://zhuanlan.zhihu.com/p/60057180">https://zhuanlan.zhihu.com/p/60057180</a></li><li><a href="https://jackkuo666.github.io/Data_Structure_with_Python_book/chapter3/section1.html">https://jackkuo666.github.io/Data_Structure_with_Python_book/chapter3/section1.html</a></li></ul><h5 id="如何发布包到pypi"><a href="#如何发布包到pypi" class="headerlink" title="如何发布包到pypi"></a>如何发布包到pypi</h5><p>我的项目目录结构如下：<br><img src="/img/article/py.jpg"></p><p>打包主要就是setup的编写</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs clean">setup(<br>    name=NAME,<br>    version=about[<span class="hljs-string">&#x27;__version__&#x27;</span>],<br>    description=DESCRIPTION,<br>    long_description=long_description,<br>    long_description_content_type=<span class="hljs-string">&#x27;text/markdown&#x27;</span>,<br>    author=AUTHOR,<br>    author_email=EMAIL,<br>    python_requires=REQUIRES_PYTHON,<br>    url=URL,<br>    packages=find_packages(),                ## 必须，如果需要打包test文件夹或其他可参考下面格式进行添加<br>    # packages=find_packages(exclude=[<span class="hljs-string">&quot;tests&quot;</span>, <span class="hljs-string">&quot;*.tests&quot;</span>, <span class="hljs-string">&quot;*.tests.*&quot;</span>, <span class="hljs-string">&quot;tests.*&quot;</span>]),<br>    # If your package is a single <span class="hljs-keyword">module</span>, use this instead <span class="hljs-keyword">of</span> <span class="hljs-string">&#x27;packages&#x27;</span>:<br>    # py_modules=[<span class="hljs-string">&#x27;SDStools&#x27;</span>],<br><br>    # entry_points=&#123;<br>    #     <span class="hljs-string">&#x27;console_scripts&#x27;</span>: [<span class="hljs-string">&#x27;mycli=mymodule:cli&#x27;</span>],<br>    # &#125;,<br>    install_requires=REQUIRED,<br>    extras_require=EXTRAS,<br>    # package_data=&#123;<br>    #     # include json and pkl files<br>    #     <span class="hljs-string">&#x27;&#x27;</span>: [<span class="hljs-string">&#x27;*.json&#x27;</span>, <span class="hljs-string">&#x27;models/*.pkl&#x27;</span>, <span class="hljs-string">&#x27;models/*.json&#x27;</span>],<br>    # &#125;,<br>    include_package_data=<span class="hljs-literal">True</span>,<br>    license=<span class="hljs-string">&#x27;MIT&#x27;</span>,<br>    classifiers=[<br>        # Trove classifiers<br>        # Full list: https:<span class="hljs-comment">//pypi.python.org/pypi?%3Aaction=list_classifiers</span><br>        <span class="hljs-string">&#x27;License :: OSI Approved :: MIT License&#x27;</span>,<br>        <span class="hljs-string">&#x27;Programming Language :: Python&#x27;</span>,<br>        <span class="hljs-string">&#x27;Programming Language :: Python :: 3&#x27;</span>,<br>        <span class="hljs-string">&#x27;Programming Language :: Python :: 3.6&#x27;</span>,<br>        <span class="hljs-string">&#x27;Programming Language :: Python :: Implementation :: CPython&#x27;</span>,<br>        <span class="hljs-string">&#x27;Programming Language :: Python :: Implementation :: PyPy&#x27;</span><br>    ],<br>    # $ setup.py publish support.<br>    cmdclass=&#123;<br>        <span class="hljs-string">&#x27;upload&#x27;</span>: UploadCommand,<br>    &#125;,<br>)<br></code></pre></td></tr></table></figure><p>编写完可以进行本地打包安装测试：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">python setup.py <span class="hljs-keyword">build </span>    <span class="hljs-comment"># 执行构建, 会将包的内容构建到 build 文件夹下。</span><br></code></pre></td></tr></table></figure><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">python setup.py <span class="hljs-keyword">install</span>  <span class="hljs-comment"># 会将包直接安装到当前解释器的 site-packages 下，安装完成后即可以使用 pip list 命令查看到。</span><br></code></pre></td></tr></table></figure><p>如果没什么问题的话就可以提交到pypi了。</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">python</span> setup.<span class="hljs-keyword">py</span> sdist  ## 打包<br>twine upload dist/*    ## 发布<br># <span class="hljs-keyword">python</span> setup.<span class="hljs-keyword">py</span> upload   ## 如果setup.<span class="hljs-keyword">py</span>里有upload命令也可一键执行打包发布<br></code></pre></td></tr></table></figure><p>参考：</p><ul><li><a href="https://www.jiqizhixin.com/articles/19060901">https://www.jiqizhixin.com/articles/19060901</a></li><li><a href="https://zhuanlan.zhihu.com/p/66603015">https://zhuanlan.zhihu.com/p/66603015</a></li><li><a href="https://zhuanlan.zhihu.com/p/66603015">https://zhuanlan.zhihu.com/p/66603015</a></li></ul><h5 id="源码地址"><a href="#源码地址" class="headerlink" title="源码地址"></a>源码地址</h5><ul><li><a href="https://github.com/shubihu/SDSTools">https://github.com/shubihu/SDSTools</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LeetCode</title>
    <link href="/2021/12/28/leetcode/"/>
    <url>/2021/12/28/leetcode/</url>
    
    <content type="html"><![CDATA[<h2 id="力扣笔记"><a href="#力扣笔记" class="headerlink" title="力扣笔记"></a>力扣笔记</h2><span id="more"></span><h5 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h5><p>给定一个字符串 s ，其中包含字母顺序打乱的用英文单词表示的若干数字（0-9）。按 升序 返回原始的数字。例如：输入：s = “owoztneoer”，输出：”012”。<br>原题地址：<a href="https://leetcode-cn.com/problems/reconstruct-original-digits-from-english/">https://leetcode-cn.com/problems/reconstruct-original-digits-from-english/</a></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs vim">### 我的方案，使用了递归，，但依然很惨，没通过力扣的检验（超出时间限制）<br><br>def originalDigits(s):<br>    en_num = &#123;<span class="hljs-string">&#x27;zero&#x27;</span>:<span class="hljs-number">0</span>, <span class="hljs-string">&#x27;one&#x27;</span>:<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;two&#x27;</span>:<span class="hljs-number">2</span>, <span class="hljs-string">&#x27;three&#x27;</span>:<span class="hljs-number">3</span>,<span class="hljs-string">&#x27;four&#x27;</span>:<span class="hljs-number">4</span>,<span class="hljs-string">&#x27;five&#x27;</span>:<span class="hljs-number">5</span>,<span class="hljs-string">&#x27;six&#x27;</span>:<span class="hljs-number">6</span>,<span class="hljs-string">&#x27;seven&#x27;</span>:<span class="hljs-number">7</span>,<br>    <span class="hljs-string">&#x27;eight&#x27;</span>:<span class="hljs-number">8</span>,<span class="hljs-string">&#x27;nine&#x27;</span>:<span class="hljs-number">9</span>&#125;<br><br>    ss = []<br>    <span class="hljs-keyword">for</span> <span class="hljs-keyword">k</span>, v in en_num.<span class="hljs-built_in">items</span>():<br>        <span class="hljs-keyword">l</span> = <span class="hljs-built_in">len</span>(<span class="hljs-keyword">k</span>)<br>        <span class="hljs-keyword">c</span> = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i in <span class="hljs-keyword">k</span>:<br>            <span class="hljs-keyword">if</span> i in <span class="hljs-variable">s:</span><br>                <span class="hljs-keyword">c</span> += <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">c</span> == <span class="hljs-variable">l:</span><br>            ss.<span class="hljs-keyword">append</span>(str(v))<br>            <span class="hljs-keyword">for</span> <span class="hljs-keyword">j</span> in <span class="hljs-keyword">k</span>:<br>                s = s.replace(<span class="hljs-keyword">j</span>, <span class="hljs-string">&#x27;&#x27;</span>, <span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">if</span> <span class="hljs-variable">s:</span><br>        ss.<span class="hljs-keyword">append</span>(originalDigits(s))<br>        ss.<span class="hljs-keyword">sort</span>()<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;&#x27;</span>.<span class="hljs-keyword">join</span>(ss)<br>    <span class="hljs-keyword">else</span>:<br>        ss.<span class="hljs-keyword">sort</span>()<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;&#x27;</span>.<span class="hljs-keyword">join</span>(ss)<br></code></pre></td></tr></table></figure><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs prolog">## 力扣方案<br><br>class <span class="hljs-symbol">Solution</span>:<br>    def originalDigits(self, s: str) -&gt; str:<br>        c = <span class="hljs-symbol">Counter</span>(s)<br><br>        cnt = [<span class="hljs-number">0</span>] * <span class="hljs-number">10</span><br>        cnt[<span class="hljs-number">0</span>] = c[<span class="hljs-string">&quot;z&quot;</span>]<br>        cnt[<span class="hljs-number">2</span>] = c[<span class="hljs-string">&quot;w&quot;</span>]<br>        cnt[<span class="hljs-number">4</span>] = c[<span class="hljs-string">&quot;u&quot;</span>]<br>        cnt[<span class="hljs-number">6</span>] = c[<span class="hljs-string">&quot;x&quot;</span>]<br>        cnt[<span class="hljs-number">8</span>] = c[<span class="hljs-string">&quot;g&quot;</span>]<br><br>        cnt[<span class="hljs-number">3</span>] = c[<span class="hljs-string">&quot;h&quot;</span>] - cnt[<span class="hljs-number">8</span>]<br>        cnt[<span class="hljs-number">5</span>] = c[<span class="hljs-string">&quot;f&quot;</span>] - cnt[<span class="hljs-number">4</span>]<br>        cnt[<span class="hljs-number">7</span>] = c[<span class="hljs-string">&quot;s&quot;</span>] - cnt[<span class="hljs-number">6</span>]<br>        <br>        cnt[<span class="hljs-number">1</span>] = c[<span class="hljs-string">&quot;o&quot;</span>] - cnt[<span class="hljs-number">0</span>] - cnt[<span class="hljs-number">2</span>] - cnt[<span class="hljs-number">4</span>]<br><br>        cnt[<span class="hljs-number">9</span>] = c[<span class="hljs-string">&quot;i&quot;</span>] - cnt[<span class="hljs-number">5</span>] - cnt[<span class="hljs-number">6</span>] - cnt[<span class="hljs-number">8</span>]<br><br>        return <span class="hljs-string">&quot;&quot;</span>.join(str(x) * cnt[x] for x in range(<span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><p>如果单看这个代码的话我依然看不懂，还是要看解释，解释在上面的网址里都有。反正这个题确实挺考验智商的吧，我这智商还是洗洗睡了。</p><h5 id="题目描述：-1"><a href="#题目描述：-1" class="headerlink" title="题目描述："></a>题目描述：</h5><p>无重复字符串的排列组合。编写一种方法，计算某字符串的所有排列组合，字符串每个字符均不相同。<br>例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。<br>原题地址：<a href="https://leetcode-cn.com/problems/permutation-i-lcci/">https://leetcode-cn.com/problems/permutation-i-lcci/</a><br>python的itertools包中的permutations函数可以实现。</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs maxima">### itertools.<span class="hljs-built_in">permutations</span>函数源码如下<br><br>def <span class="hljs-built_in">permutation</span>(iterable, r=None):<br>    #  r:<span class="hljs-built_in">length</span> <span class="hljs-built_in">permutations</span> of elements <span class="hljs-keyword">in</span> the iterable<br>    # <span class="hljs-built_in">permutations</span>(&#x27;ABCD&#x27;, <span class="hljs-number">2</span>) --&gt; AB AC AD BA BC BD CA CB CD DA DB DC<br>    # <span class="hljs-built_in">permutations</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)) --&gt; <span class="hljs-number">012</span> <span class="hljs-number">021</span> <span class="hljs-number">102</span> <span class="hljs-number">120</span> <span class="hljs-number">201</span> <span class="hljs-number">210</span><br>    pool = tuple(iterable)<br>    n = len(pool)<br>    r = n <span class="hljs-keyword">if</span> r <span class="hljs-built_in">is</span> None <span class="hljs-keyword">else</span> r<br>    <span class="hljs-keyword">if</span> r &gt; n:<br>        <span class="hljs-built_in">return</span><br>    <span class="hljs-built_in">indices</span> = list(<span class="hljs-built_in">range</span>(n))<br>    cycles = list(<span class="hljs-built_in">range</span>(n, n-r, -<span class="hljs-number">1</span>))<br>    yield tuple(pool[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">indices</span>[:r])<br>    <span class="hljs-keyword">while</span> n:<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> reversed(<span class="hljs-built_in">range</span>(r)):<br>            cycles[i] -= <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> cycles[i] == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">indices</span>[i:] = <span class="hljs-built_in">indices</span>[i+<span class="hljs-number">1</span>:] + <span class="hljs-built_in">indices</span>[i:i+<span class="hljs-number">1</span>]<br>                cycles[i] = n - i<br>            <span class="hljs-keyword">else</span>:<br>                j = cycles[i]<br>                <span class="hljs-built_in">indices</span>[i], <span class="hljs-built_in">indices</span>[-j] = <span class="hljs-built_in">indices</span>[-j], <span class="hljs-built_in">indices</span>[i]<br>                yield tuple(pool[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">indices</span>[:r])<br>                <span class="hljs-built_in">break</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-built_in">return</span><br></code></pre></td></tr></table></figure><p>源码还是挺复杂的，，，，，</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs go">### 力扣方法，也是递归<br><br>def permutation(S):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(S)==<span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> [S]<br>        ans=[]<br>        <span class="hljs-keyword">for</span> i in <span class="hljs-keyword">range</span>(<span class="hljs-built_in">len</span>(S)):<br>            s=S[:i]+S[i+<span class="hljs-number">1</span>:]              <br>            <span class="hljs-keyword">for</span> <span class="hljs-keyword">string</span> in permutation(s):<br>                ans.<span class="hljs-built_in">append</span>(S[i]+<span class="hljs-keyword">string</span>)<br>        <span class="hljs-keyword">return</span> ans<br></code></pre></td></tr></table></figure><h5 id="题目描述：-2"><a href="#题目描述：-2" class="headerlink" title="题目描述："></a>题目描述：</h5><p>给定两个单词 word1 和 word2，请计算出 word1 与 word2 的最长公共字串长度。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">word1</span> = &#x27;abcfdg&#x27;<br><span class="hljs-attribute">word2</span> = &#x27;bcfdkhi&#x27;<br><br><span class="hljs-attribute">def</span> comm_len(word<span class="hljs-number">1</span>, word<span class="hljs-number">2</span>):<br>    <span class="hljs-attribute">c</span> = <span class="hljs-number">0</span><br>    <span class="hljs-attribute">for</span> i in range(len(word<span class="hljs-number">1</span>)):<br>        <span class="hljs-attribute">if</span> word[i-c:i+<span class="hljs-number">1</span>] in word<span class="hljs-number">2</span>:<br>            <span class="hljs-attribute">c</span> += <span class="hljs-number">1</span><br><br>    <span class="hljs-attribute">return</span> c<br><br></code></pre></td></tr></table></figure><h5 id="题目描述：-3"><a href="#题目描述：-3" class="headerlink" title="题目描述："></a>题目描述：</h5><p>快速排序</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">arr = <span class="hljs-literal">[<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">12</span>, <span class="hljs-number">32</span>, <span class="hljs-number">198</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">15</span>, <span class="hljs-number">112</span>, <span class="hljs-number">132</span>]</span><br><br>def quick<span class="hljs-constructor">_sort(<span class="hljs-params">arr</span>)</span>:<br>    <span class="hljs-keyword">if</span> len(arr) &lt;= <span class="hljs-number">1</span>:<br>        return arr<br>    tmp = arr<span class="hljs-literal">[<span class="hljs-number">0</span>]</span><br>    left = <span class="hljs-literal">[<span class="hljs-identifier">i</span> <span class="hljs-identifier">for</span> <span class="hljs-identifier">i</span> <span class="hljs-identifier">in</span> <span class="hljs-identifier">arr</span>[<span class="hljs-number">1</span>:]</span> <span class="hljs-keyword">if</span> i &lt; tmp]<br>    right = <span class="hljs-literal">[<span class="hljs-identifier">i</span> <span class="hljs-identifier">for</span> <span class="hljs-identifier">i</span> <span class="hljs-identifier">in</span> <span class="hljs-identifier">arr</span>[<span class="hljs-number">1</span>:]</span> <span class="hljs-keyword">if</span> i &gt; tmp]<br>    return quick<span class="hljs-constructor">_sort(<span class="hljs-params">left</span>)</span> + <span class="hljs-literal">[<span class="hljs-identifier">tmp</span>]</span> + quick<span class="hljs-constructor">_sort(<span class="hljs-params">right</span>)</span><br><br>print(quick<span class="hljs-constructor">_sort(<span class="hljs-params">arr</span>)</span>)<br></code></pre></td></tr></table></figure><h5 id="题目描述：-4"><a href="#题目描述：-4" class="headerlink" title="题目描述："></a>题目描述：</h5><p>归并排序应用</p><ul><li><p>1、将两个有序数组合并成一个有序数组</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-keyword">a</span> = [<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">5</span>,<span class="hljs-number">7</span>]<br>b = [<span class="hljs-number">2</span>,<span class="hljs-number">6</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>]<br><br>def <span class="hljs-built_in">merge</span>(left, <span class="hljs-literal">right</span>):<br>    <span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27;合并操作，将两个有序数组left[]和right[]合并成一个大的有序数组&#x27;</span><span class="hljs-string">&#x27;&#x27;</span><br>    <span class="hljs-comment">#left与right的下标指针</span><br>    l, r = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>    <span class="hljs-built_in">result</span> = []<br>    <span class="hljs-keyword">while</span> l&lt;<span class="hljs-built_in">len</span>(left) <span class="hljs-keyword">and</span> r&lt;<span class="hljs-built_in">len</span>(<span class="hljs-literal">right</span>):<br>        <span class="hljs-keyword">if</span> left[l] &lt; <span class="hljs-literal">right</span>[r]:<br>            <span class="hljs-built_in">result</span>.append(left[l])<br>            l += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-built_in">result</span>.append(<span class="hljs-literal">right</span>[r])<br>            r += <span class="hljs-number">1</span><br>    <span class="hljs-built_in">result</span> += left[l:]<br>    <span class="hljs-built_in">result</span> += <span class="hljs-literal">right</span>[r:]<br>    <span class="hljs-literal">return</span> <span class="hljs-built_in">result</span><br><br>print(<span class="hljs-built_in">merge</span>(<span class="hljs-keyword">a</span>, b))<br></code></pre></td></tr></table></figure></li><li><ol start="2"><li>每个 SNP 位点用一个长度为 2 的 list 表示，第一个元素为染色体编号（chr，范围为<br>1~22），第二个元素为染色体上的位置（pos）。写一个 python 函数，输入两个正序（按<br>chr 和 pos 排序）的 SNP 位点 list，输出一个合并且去重的正序 SNP 位点 list。不能使用<br>sorted 函数、pandas 库，要求时间复杂度尽可能低。<br>例如：<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs lua">Input: list1 = <span class="hljs-string">[[1,230], [1,4000], [2,500]]</span>, list2 = <span class="hljs-string">[[2,320], [6,70]]</span><br>Output: <span class="hljs-string">[[1,230], [1,4000], [2,320], [2,500], [6,70]]</span><br></code></pre></td></tr></table></figure></li></ol></li></ul><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">list1</span> =<span class="hljs-meta"> [[1,230], [1,4000], [2,500]]</span><br><span class="hljs-attribute">list2</span> =<span class="hljs-meta"> [[2,320], [6,70]]</span><br><br><span class="hljs-attribute">def</span> merge(list<span class="hljs-number">1</span>, list<span class="hljs-number">2</span>):<br>    <span class="hljs-attribute">l</span>, r = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>    <span class="hljs-attribute">result</span> =<span class="hljs-meta"> []</span><br>    <span class="hljs-attribute">while</span> l&lt;len(list<span class="hljs-number">1</span>) and r&lt;len(list<span class="hljs-number">2</span>):<br>        <span class="hljs-attribute">if</span> list<span class="hljs-number">1</span>[l][<span class="hljs-number">0</span>] &lt; list<span class="hljs-number">2</span>[r][<span class="hljs-number">0</span>]:<br>            <span class="hljs-attribute">if</span> list<span class="hljs-number">1</span>[l] not in result:<br>                <span class="hljs-attribute">result</span>.append(list<span class="hljs-number">1</span>[l])<br>            <span class="hljs-attribute">l</span> += <span class="hljs-number">1</span><br>        <span class="hljs-attribute">else</span>:<br>            <span class="hljs-attribute">if</span> list<span class="hljs-number">2</span>[r] not in result:<br>                <span class="hljs-attribute">result</span>.append(list<span class="hljs-number">2</span>[r])<br>            <span class="hljs-attribute">r</span> += <span class="hljs-number">1</span><br><br>    <span class="hljs-attribute">result</span> += list<span class="hljs-number">1</span>[l:]<br>    <span class="hljs-attribute">result</span> += list<span class="hljs-number">2</span>[r:]<br>    <span class="hljs-attribute">return</span> result<br><br><span class="hljs-attribute">print</span>(merge(list<span class="hljs-number">1</span>, list<span class="hljs-number">2</span>))<br></code></pre></td></tr></table></figure><h5 id="题目描述：-5"><a href="#题目描述：-5" class="headerlink" title="题目描述："></a>题目描述：</h5><p>输出单向链表中倒数第k个结点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Node</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, val=<span class="hljs-number">0</span></span>):</span><br>        self.val = val<br>        self.<span class="hljs-built_in">next</span> = <span class="hljs-literal">None</span><br><br><br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>    <span class="hljs-keyword">try</span>:<br>        l, s, k, head = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">input</span>()), <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">int</span>, <span class="hljs-built_in">input</span>().split())), <span class="hljs-built_in">int</span>(<span class="hljs-built_in">input</span>()), Node()<br>        <span class="hljs-keyword">while</span> k:<br>            head.<span class="hljs-built_in">next</span> = Node(s.pop())<br>            head = head.<span class="hljs-built_in">next</span><br>            k -= <span class="hljs-number">1</span><br>        <span class="hljs-built_in">print</span>(head.val)<br>    <span class="hljs-keyword">except</span>:<br>        <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure><h5 id="题目描述：-6"><a href="#题目描述：-6" class="headerlink" title="题目描述："></a>题目描述：</h5><p>给定两个单词 word1 和 word2，请计算出将 word1 转换成 word2 所使用的最少操作数。<br>参考：<a href="https://www.jianshu.com/p/9a53f32cf62b">https://www.jianshu.com/p/9a53f32cf62b</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">editDistance</span>(<span class="hljs-params">str1, str2</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    计算字符串str1和str2的编辑距离</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    edit = [[i+j <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(str2)+<span class="hljs-number">1</span>)] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(str1)+<span class="hljs-number">1</span>)]<br>    <span class="hljs-built_in">print</span>(edit)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-built_in">len</span>(str1)+<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-built_in">len</span>(str2)+<span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">if</span> str1[i-<span class="hljs-number">1</span>] == str2[j-<span class="hljs-number">1</span>]:<br>                d = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">else</span>:<br>                d = <span class="hljs-number">1</span><br>            edit[i][j] = <span class="hljs-built_in">min</span>(edit[i-<span class="hljs-number">1</span>][j]+<span class="hljs-number">1</span>,edit[i][j-<span class="hljs-number">1</span>]+<span class="hljs-number">1</span>,edit[i-<span class="hljs-number">1</span>][j-<span class="hljs-number">1</span>]+d)<br>    <span class="hljs-keyword">return</span> edit[<span class="hljs-built_in">len</span>(str1)][<span class="hljs-built_in">len</span>(str2)]<br><br>editDistance(<span class="hljs-string">&#x27;python&#x27;</span>, <span class="hljs-string">&#x27;pyton&#x27;</span>)<br></code></pre></td></tr></table></figure><h5 id="题目描述：-7"><a href="#题目描述：-7" class="headerlink" title="题目描述："></a>题目描述：</h5><ul><li><p>最长回文字串</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs stylus">s = <span class="hljs-selector-tag">input</span>()<br>win_len = <span class="hljs-number">2</span><br>s_list = <span class="hljs-selector-attr">[]</span><br><span class="hljs-keyword">for</span> <span class="hljs-selector-tag">i</span> <span class="hljs-keyword">in</span> range(len(s)):<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(i+<span class="hljs-number">1</span>, len(s) + <span class="hljs-number">1</span>):<br>        ss = s<span class="hljs-selector-attr">[i:j]</span><br>        <span class="hljs-keyword">if</span> ss == ss<span class="hljs-selector-attr">[::-1]</span>:<br>            s_list<span class="hljs-selector-class">.append</span>(len(ss))<br>            <br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(max(s_list)</span></span>)<br></code></pre></td></tr></table></figure></li><li><p>输入两个 DNA 序列，输出它们的最长公共序列。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Input</span>: seq<span class="hljs-number">1</span> = “AGGCT”, seq<span class="hljs-number">2</span> = “GGCA”<br><span class="hljs-attribute">Output</span>: “GGC”<br></code></pre></td></tr></table></figure></li></ul><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">seq1</span> = &#x27;AGGCT&#x27;<br><span class="hljs-attribute">seq2</span> = &#x27;GGCA&#x27;<br><br><span class="hljs-attribute">def</span> comm_len(seq<span class="hljs-number">1</span>, seq<span class="hljs-number">2</span>):<br>    <span class="hljs-attribute">c</span> = <span class="hljs-number">0</span><br>    <span class="hljs-attribute">seq_list</span> =<span class="hljs-meta"> []</span><br>    <span class="hljs-attribute">for</span> i in range(len(seq<span class="hljs-number">1</span>)):<br>        <span class="hljs-attribute">tmp</span> = seq<span class="hljs-number">1</span>[i-c:i+<span class="hljs-number">1</span>]<br>        <span class="hljs-attribute">if</span> tmp in seq<span class="hljs-number">2</span>:<br>            <span class="hljs-attribute">c</span> += <span class="hljs-number">1</span><br>            <span class="hljs-attribute">seq_list</span>.append(tmp)<br><br>          <br>    <span class="hljs-attribute">return</span> seq_list[-<span class="hljs-number">1</span>]<br><br><span class="hljs-attribute">print</span>(comm_len(seq<span class="hljs-number">1</span>, seq<span class="hljs-number">2</span>))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>leetcode</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VIP视频解析手机端</title>
    <link href="/2021/12/24/VIP%E8%A7%86%E9%A2%91%E8%A7%A3%E6%9E%90%E6%89%8B%E6%9C%BA%E7%AB%AF/"/>
    <url>/2021/12/24/VIP%E8%A7%86%E9%A2%91%E8%A7%A3%E6%9E%90%E6%89%8B%E6%9C%BA%E7%AB%AF/</url>
    
    <content type="html"><![CDATA[<h2 id="薅各大厂视频羊毛——手机端"><a href="#薅各大厂视频羊毛——手机端" class="headerlink" title="薅各大厂视频羊毛——手机端"></a>薅各大厂视频羊毛——手机端</h2><span id="more"></span><p>以前都是用电脑端Chrome浏览器安装油猴插件及其相应的脚本看视频，但是如果想用电视看视频，用电脑投屏终究是有点麻烦。找了许久还是让我找到了手机端可以使用脚本的浏览器。<br>使用方法也可以直接跳转该地址：<br><a href="https://blog.csdn.net/qq_37759464/article/details/121903038?utm_source=app&amp;app_version=4.20.0&amp;code=app_1562916241&amp;uLinkId=usr1mkqgl919blen">https://blog.csdn.net/qq_37759464/article/details/121903038?utm_source=app&amp;app_version=4.20.0&amp;code=app_1562916241&amp;uLinkId=usr1mkqgl919blen</a></p><p>个人推荐的话：安卓可以使用Via浏览器，真的很简约，官网也一样的简约（<a href="https://viayoo.com/zh-cn/%EF%BC%89%EF%BC%8C%E8%80%8C%E4%B8%94%E4%BA%B2%E6%B5%8B%E5%8F%AF%E4%BB%A5%E5%AE%9E%E7%8E%B0VIP%E8%A7%86%E9%A2%91%E8%A7%A3%E6%9E%90%E3%80%82%E8%8B%B9%E6%9E%9CIOS%E7%AB%AF%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8Alook%EF%BC%8C%E4%B8%8D%E8%BF%87%E8%BF%99%E4%B8%AA%E8%BD%AF%E4%BB%B6%E4%B8%8D%E6%98%AF%E5%85%8D%E8%B4%B9%E7%9A%84%EF%BC%88%E7%8E%B0%E5%9C%A8%E7%96%AB%E6%83%85%E6%90%9E%E5%8D%8A%E4%BB%B7%E6%B4%BB%E5%8A%A86%E5%85%83%EF%BC%8C%E4%B8%8D%E8%BF%87%E6%94%AF%E4%BB%98%E5%AE%9D%E6%9C%89%E6%97%B6%E5%80%99%E4%BC%9A%E6%9C%89%E8%8B%B9%E6%9E%9CAppstore%E6%B6%88%E8%B4%B9%E5%88%B8%EF%BC%8C%E6%AF%94%E5%A6%82%E4%BF%BA%E4%BB%8A%E5%A4%A9%E9%A2%86%E4%BA%869%E5%85%83%EF%BC%8C%E6%9E%9C%E6%96%AD%E5%85%A5%E6%89%8B%E7%99%BD%E6%8D%A1%E4%BA%86%E8%BF%99%E4%B8%AA%E6%B5%8F%E8%A7%88%E5%99%A8%EF%BC%89%E3%80%82">https://viayoo.com/zh-cn/），而且亲测可以实现VIP视频解析。苹果IOS端可以使用Alook，不过这个软件不是免费的（现在疫情搞半价活动6元，不过支付宝有时候会有苹果Appstore消费券，比如俺今天领了9元，果断入手白捡了这个浏览器）。</a></p><p>安装好浏览器后就是安装脚本了，上面的地址里有详细的方法，脚本的地址在此：<br><a href="https://greasyfork.org/zh-CN/scripts/435698-%E5%85%A8%E7%BD%91vip%E8%A7%86%E9%A2%91%E8%87%AA%E5%8A%A8%E8%A7%A3%E6%9E%90%E6%92%AD%E6%94%BE%E5%99%A8-%E5%B7%B2%E9%80%82%E9%85%8D%E6%89%8B%E6%9C%BA%E3%80%82">https://greasyfork.org/zh-CN/scripts/435698-%E5%85%A8%E7%BD%91vip%E8%A7%86%E9%A2%91%E8%87%AA%E5%8A%A8%E8%A7%A3%E6%9E%90%E6%92%AD%E6%94%BE%E5%99%A8-%E5%B7%B2%E9%80%82%E9%85%8D%E6%89%8B%E6%9C%BA。</a></p><p>说下我安装过程中遇到的问题吧，就是全选复制那块，不知道为啥就是不能全选，没办法只能慢慢拖拽光标进行全选然后复制（脚本是真长）。</p><p>手机投屏的话，安卓可以用乐播投屏，苹果的话直接用自带的屏幕镜像进行投屏就OK了。</p><p>当然了如果有钱开VIP就可以直接忽略本文啦，不过爱奇艺又要涨价了。</p>]]></content>
    
    
    <categories>
      
      <category>TroubleShoot</category>
      
    </categories>
    
    
    <tags>
      
      <tag>薅羊毛</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>JupyterNotebook远程云服务器</title>
    <link href="/2021/11/30/jupyter/"/>
    <url>/2021/11/30/jupyter/</url>
    
    <content type="html"><![CDATA[<h2 id="搭建Jupyter-Notebook远程云服务器"><a href="#搭建Jupyter-Notebook远程云服务器" class="headerlink" title="搭建Jupyter Notebook远程云服务器"></a>搭建Jupyter Notebook远程云服务器</h2><span id="more"></span><h5 id="安装Jupyter"><a href="#安装Jupyter" class="headerlink" title="安装Jupyter"></a>安装Jupyter</h5><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs verilog">pip install Jupyter<br>jupyter notebook --<span class="hljs-keyword">generate</span>-<span class="hljs-keyword">config</span><br></code></pre></td></tr></table></figure><h5 id="设置密码用于设置服务器配置，以及登录Jupyter。打开Python终端，输入以下："><a href="#设置密码用于设置服务器配置，以及登录Jupyter。打开Python终端，输入以下：" class="headerlink" title="设置密码用于设置服务器配置，以及登录Jupyter。打开Python终端，输入以下："></a>设置密码用于设置服务器配置，以及登录Jupyter。打开Python终端，输入以下：</h5><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">jupyter notebook password</span><br></code></pre></td></tr></table></figure><h5 id="设置服务器配置文件"><a href="#设置服务器配置文件" class="headerlink" title="设置服务器配置文件"></a>设置服务器配置文件</h5><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arcade">vim ~<span class="hljs-regexp">/.jupyter/</span>jupyter_notebook_config.py<br></code></pre></td></tr></table></figure><p>在末尾增加以下几行配置信息</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">c.NotebookApp.ip</span> = <span class="hljs-string">&#x27;*&#x27;</span> <span class="hljs-comment">#所有绑定服务器的IP都能访问，若想只在特定ip访问，输入ip地址即可</span><br><span class="hljs-attr">c.NotebookApp.port</span> = <span class="hljs-number">8888</span> <span class="hljs-comment">#将端口设置为自己喜欢的吧，默认是8888</span><br><span class="hljs-attr">c.NotebookApp.open_browser</span> = <span class="hljs-literal">False</span> <span class="hljs-comment">#我们并不想在服务器上直接打开Jupyter Notebook，所以设置成False</span><br><span class="hljs-attr">c.NotebookApp.notebook_dir</span> = <span class="hljs-string">&#x27;/root/jupyter_projects&#x27;</span> <span class="hljs-comment">#这里是设置Jupyter的根目录，若不设置将默认root的根目录，不安全</span><br><span class="hljs-attr">c.NotebookApp.allow_root</span> = <span class="hljs-literal">True</span> <span class="hljs-comment"># 为了安全，Jupyter默认不允许以root权限启动jupyter </span><br></code></pre></td></tr></table></figure><h5 id="启动Jupyter-远程服务器"><a href="#启动Jupyter-远程服务器" class="headerlink" title="启动Jupyter 远程服务器"></a>启动Jupyter 远程服务器</h5><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">jupyter notebook</span><br></code></pre></td></tr></table></figure><p>至此，Jupyter远程服务器以搭建完毕。在本地浏览器上，输入 ip地址:8888，将会打开远程Jupyter。接下来就可以像在本地一样使用服务器上的Jupyter。</p><p>如果出现不能连接的情况，多半是防火墙的问题。<br>简单粗暴一劳永逸的就是关闭防火墙，当然也可进行设置。参考 <a href="https://www.codeleading.com/article/10462087431/">https://www.codeleading.com/article/10462087431/</a></p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arduino">systemctl stop firewalld.service<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>jupyter</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch模型转paddle模型踩坑记录</title>
    <link href="/2021/11/29/pytorch2paddle/"/>
    <url>/2021/11/29/pytorch2paddle/</url>
    
    <content type="html"><![CDATA[<h2 id="如题"><a href="#如题" class="headerlink" title="如题"></a>如题</h2><span id="more"></span><h5 id="踩坑1"><a href="#踩坑1" class="headerlink" title="踩坑1"></a>踩坑1</h5><p>网上有很多使用x2paddle把pytorch转paddle的文章，不过大不部分也都是采用的迂回路线，就是先转ONNX，再转paddle，试了下水，果然没有那么简单的事情，一直报错，最后好像报了个 model not support，，，，遂放弃。</p><h5 id="踩坑2"><a href="#踩坑2" class="headerlink" title="踩坑2"></a>踩坑2</h5><p>使用工具不行只有一步一步慢慢转，这也是最开始使用的方法，起初报错没解决才找到x2paddle的，没想到又回归到最原始的方法了。<br>转换的过程一直卡在网络这块，所以就先把网络这块拿出来记录下。</p><h6 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h6><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs clean">######################### torch 版  ############################<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> SeqNet(nn.Module):<br>    def __init__(self):<br>        super(SeqNet, self).__init__()<br>        # input <br>        self.conv1 = nn.Conv1d(<span class="hljs-number">12</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>)<br>        self.conv2 = nn.Conv1d(<span class="hljs-number">12</span>, <span class="hljs-number">10</span>, <span class="hljs-number">200</span>)<br>        self.conv3 = nn.Conv1d(<span class="hljs-number">12</span>, <span class="hljs-number">10</span>, <span class="hljs-number">500</span>)<br>        self.conv4 = nn.Conv1d(<span class="hljs-number">12</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1000</span>)<br>        self.pooling = nn.MaxPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">200</span>))<br>        self.fc1 = nn.Linear(<span class="hljs-number">900</span>, <span class="hljs-number">64</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">1</span>)<br><br>    def forward(self, x):<br>        batch_size = x.size(<span class="hljs-number">0</span>)<br>        <br>        out1 = self.pooling(F.relu(self.conv1(x)))<br>        out2 = self.pooling(F.relu(self.conv2(x)))<br>        out3 = self.pooling(F.relu(self.conv3(x)))<br>        out4 = self.pooling(F.relu(self.conv4(x)))<br><br>        out = torch.cat([out1, out2, out3, out4], <span class="hljs-number">2</span>)<br>        out = out.view(batch_size, <span class="hljs-number">-1</span>)<br>        out = self.fc1(out)<br>        out = F.relu(out)<br>        # out = F.dropout(out, p=<span class="hljs-number">0.2</span>)<br>        out = self.fc2(out)<br>        return out<br></code></pre></td></tr></table></figure><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs clean">######################### paddle 版  ############################<br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> paddle.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> SeqNet(nn.Layer):<br>    def __init__(self):<br>        super(SeqNet, self).__init__()<br>        # input <br>        self.conv1 = nn.Conv1D(<span class="hljs-number">12</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>)<br>        self.conv2 = nn.Conv1D(<span class="hljs-number">12</span>, <span class="hljs-number">10</span>, <span class="hljs-number">200</span>)<br>        self.conv3 = nn.Conv1D(<span class="hljs-number">12</span>, <span class="hljs-number">10</span>, <span class="hljs-number">500</span>)<br>        self.conv4 = nn.Conv1D(<span class="hljs-number">12</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1000</span>)<br>        # self.pooling = nn.MaxPool2D((<span class="hljs-number">1</span>, <span class="hljs-number">200</span>))   <br>        ### torch版的 nn.MaxPool2D 输入数剧格式为（NCHW 或 CHW）,paddle版的 nn.MaxPool2D 输入数据格式只有 NCHW<br>        ### N代表batch_size， C代表channel，H代表高度，W代表宽度<br>        ### 所以这里用 paddle 的 nn.MaxPool1D 替换了 torch 的 nn.MaxPool2D<br>        self.pooling = nn.MaxPool1D(<span class="hljs-number">200</span>)<br>        self.fc1 = nn.Linear(<span class="hljs-number">900</span>, <span class="hljs-number">64</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">1</span>)<br><br>    def forward(self, x):<br>        ### torch.tensor.size 对应 paddle.tensor.shape<br>        batch_size = x.shape[<span class="hljs-number">0</span>]   <br>        <br>        out1 = self.pooling(F.relu(self.conv1(x)))<br>        out2 = self.pooling(F.relu(self.conv2(x)))<br>        out3 = self.pooling(F.relu(self.conv3(x)))<br>        out4 = self.pooling(F.relu(self.conv4(x)))<br>        <br>        ### torch.cat 对应 paddle.concat<br>        # out = torch.cat([out1, out2, out3, out4], <span class="hljs-number">2</span>)  <br>        out = paddle.concat([out1, out2, out3, out4], <span class="hljs-number">2</span>)<br>        ### torch.tensor.view 对应 paddle.tensor.reshape<br>        # out = out.view(batch_size, <span class="hljs-number">-1</span>)<br>        out = paddle.reshape(out, [batch_size, <span class="hljs-number">-1</span>])<br>        out = self.fc1(out)<br>        out = F.relu(out)<br>        # out = F.dropout(out, p=<span class="hljs-number">0.2</span>)<br>        out = self.fc2(out)<br><br>        return out<br></code></pre></td></tr></table></figure><h6 id="对于自定义数据集-paddle和pytorch实现的方法类似"><a href="#对于自定义数据集-paddle和pytorch实现的方法类似" class="headerlink" title="对于自定义数据集 paddle和pytorch实现的方法类似"></a>对于自定义数据集 paddle和pytorch实现的方法类似</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> paddle.io <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyDataset</span>(<span class="hljs-params">Dataset</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    步骤一：继承paddle.io.Dataset类</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, mode=<span class="hljs-string">&#x27;train&#x27;</span></span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        步骤二：实现构造函数，定义数据读取方式，划分训练和测试数据集</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(MyDataset, self).__init__()<br><br>        <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;train&#x27;</span>:<br>            self.data = [<br>                [<span class="hljs-string">&#x27;traindata1&#x27;</span>, <span class="hljs-string">&#x27;label1&#x27;</span>],<br>                [<span class="hljs-string">&#x27;traindata2&#x27;</span>, <span class="hljs-string">&#x27;label2&#x27;</span>],<br>                [<span class="hljs-string">&#x27;traindata3&#x27;</span>, <span class="hljs-string">&#x27;label3&#x27;</span>],<br>                [<span class="hljs-string">&#x27;traindata4&#x27;</span>, <span class="hljs-string">&#x27;label4&#x27;</span>],<br>            ]<br>        <span class="hljs-keyword">else</span>:<br>            self.data = [<br>                [<span class="hljs-string">&#x27;testdata1&#x27;</span>, <span class="hljs-string">&#x27;label1&#x27;</span>],<br>                [<span class="hljs-string">&#x27;testdata2&#x27;</span>, <span class="hljs-string">&#x27;label2&#x27;</span>],<br>                [<span class="hljs-string">&#x27;testdata3&#x27;</span>, <span class="hljs-string">&#x27;label3&#x27;</span>],<br>                [<span class="hljs-string">&#x27;testdata4&#x27;</span>, <span class="hljs-string">&#x27;label4&#x27;</span>],<br>            ]<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span>(<span class="hljs-params">self, index</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        步骤三：实现__getitem__方法，定义指定index时如何获取数据，并返回单条数据（训练数据，对应的标签）</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        data = self.data[index][<span class="hljs-number">0</span>]<br>        label = self.data[index][<span class="hljs-number">1</span>]<br><br>        <span class="hljs-keyword">return</span> data, label<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        步骤四：实现__len__方法，返回数据集总数目</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.data)<br></code></pre></td></tr></table></figure><h6 id="还有就是训练这块"><a href="#还有就是训练这块" class="headerlink" title="还有就是训练这块"></a>还有就是训练这块</h6><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs clean">######################### torch 版  ############################<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br>model = SeqNet()<br>model.to(device)<br>optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="hljs-number">1e-4</span> ,weight_decay=<span class="hljs-number">5e-4</span>)<br>scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epoch)<br>criterion = nn.BCEWithLogitsLoss()<br><br>for i, (inputs, labels) <span class="hljs-keyword">in</span> (enumerate(trainloader)):<br>    inputs = inputs.to(device)<br>    labels = labels.float().to(device)<br><br>    out_linear = model(inputs).to(device)<br>    loss = criterion(out_linear, labels.unsqueeze(<span class="hljs-number">1</span>))<br><br>    optimizer.zero_grad()<br>    loss.backward()<br>    optimizer.step()<br></code></pre></td></tr></table></figure><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs clean">######################### paddle 版  ############################<br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> paddle.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> paddle.optimizer <span class="hljs-keyword">as</span> optim<br><br>model = SeqNet()<br>model.to(device)<br>optimizer = optim.AdamW(learning_rate=<span class="hljs-number">1e-4</span>, parameters=model.parameters(),weight_decay=<span class="hljs-number">5e-4</span>)<br>### optimizer = optim.Adam(parameters=model.parameters(), learning_rate=<span class="hljs-number">1e-4</span>)<br>### paddle 版CosineAnnealingDecay接収的是 learning_rate参数<br>scheduler = optim.lr.CosineAnnealingDecay(<span class="hljs-number">1e-4</span>, T_max=max_epoch)<br>criterion = nn.BCEWithLogitsLoss()<br><br>for i, (inputs, labels) <span class="hljs-keyword">in</span> (enumerate(trainloader)):<br>    # inputs = inputs.to(device)<br>    inputs = inputs.cuda()<br>    # labels = labels.float().to(device)<br>    labels = labels.cuda()<br>    # labels = paddle.reshape(labels, (<span class="hljs-number">30</span>, <span class="hljs-number">1</span>))<br>    labels = paddle.cast(labels, dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)  ## 转换数据类型<br><br>    out_linear = model(inputs)<br>    out_linear = paddle.reshape(out_linear, (batch_size,))<br>    loss = criterion(out_linear, labels)<br>    # loss = criterion(out_linear, labels.unsqueeze(<span class="hljs-number">1</span>))<br><br>    # optimizer.zero_grad()<br>    loss.backward()<br>    optimizer.step()<br>    optimizer.clear_grad()<br></code></pre></td></tr></table></figure><p>其余剩下就是一些小问题了，直接运行debug改就好了。<br>pytorch 完整版地址：<a href="https://github.com/shubihu/coggle_learn/blob/main/baseline/pytorch.ipynb">https://github.com/shubihu/coggle_learn/blob/main/baseline/pytorch.ipynb</a><br>paddle 完整版地址：<a href="https://github.com/shubihu/coggle_learn/blob/main/baseline/paddle.ipynb">https://github.com/shubihu/coggle_learn/blob/main/baseline/paddle.ipynb</a><br>aistudio上项目的地址为：<a href="https://aistudio.baidu.com/aistudio/projectdetail/2724787?contributionType=1">https://aistudio.baidu.com/aistudio/projectdetail/2724787?contributionType=1</a></p>]]></content>
    
    
    <categories>
      
      <category>TroubleShoot</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TroubleShoot</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>mac-iTerm2</title>
    <link href="/2021/11/15/mac-iTerm2/"/>
    <url>/2021/11/15/mac-iTerm2/</url>
    
    <content type="html"><![CDATA[<h2 id="mac-air-m1-终端配置记录"><a href="#mac-air-m1-终端配置记录" class="headerlink" title="mac air m1 终端配置记录"></a>mac air m1 终端配置记录</h2><span id="more"></span><h3 id="安装Homebrew"><a href="#安装Homebrew" class="headerlink" title="安装Homebrew"></a>安装Homebrew</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/bin/</span>bash -c <span class="hljs-string">&quot;$(curl -fsSL https://cdn.jsdelivr.net/gh/ineo6/homebrew-install/install.sh)&quot;</span><br></code></pre></td></tr></table></figure><p>将以上命令粘贴至终端。脚本内置 中科大镜像，所以能让Homebrew安装的更快。</p><h3 id="安装-oh-my-zsh"><a href="#安装-oh-my-zsh" class="headerlink" title="安装 oh-my-zsh"></a>安装 oh-my-zsh</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sh -c <span class="hljs-string">&quot;<span class="hljs-subst">$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)</span>&quot;</span><br></code></pre></td></tr></table></figure><h3 id="配置-oh-my-zsh"><a href="#配置-oh-my-zsh" class="headerlink" title="配置 oh-my-zsh"></a>配置 oh-my-zsh</h3><p>参考 <a href="https://www.dazhuanlan.com/lyuuawa0508/topics/1599354">https://www.dazhuanlan.com/lyuuawa0508/topics/1599354</a><br>注：其中有些命令可能因为版本问题不一致，主要是cask相关，按照提示修改即可<br>比如 安装 iTerm2 的命令是现在这样</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">brew</span> tap homebrew/cask<br><span class="hljs-attribute">brew</span> install iterm<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><h3 id="brew-安装-nvm"><a href="#brew-安装-nvm" class="headerlink" title="brew 安装 nvm"></a>brew 安装 nvm</h3><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs clean">brew install nvm<br>mkdir ~/.nvm<br>vi ~/.zshrc<br>#################### 将下面内容添加到 ~/.zshrc 中 #############################<br><span class="hljs-keyword">export</span> NVM_DIR=<span class="hljs-string">&quot;$HOME/.nvm&quot;</span><br>[ -s <span class="hljs-string">&quot;/opt/homebrew/opt/nvm/nvm.sh&quot;</span> ] &amp;&amp; . <span class="hljs-string">&quot;/opt/homebrew/opt/nvm/nvm.sh&quot;</span> # This loads nvm<br>[ -s <span class="hljs-string">&quot;/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm&quot;</span> ] &amp;&amp; . <span class="hljs-string">&quot;/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm&quot;</span> # This loads nvm bash_completion<br>###############################################################################<br>source ~/.zshrc<br></code></pre></td></tr></table></figure><h3 id="github-加速"><a href="#github-加速" class="headerlink" title="github 加速"></a>github 加速</h3><p>参考 <a href="https://brew.idayer.com/guide/github">https://brew.idayer.com/guide/github</a></p>]]></content>
    
    
    <categories>
      
      <category>Mac</category>
      
    </categories>
    
    
    <tags>
      
      <tag>玩</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>科学上网</title>
    <link href="/2021/11/07/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/"/>
    <url>/2021/11/07/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/</url>
    
    <content type="html"><![CDATA[<h2 id="如题"><a href="#如题" class="headerlink" title="如题"></a>如题</h2><span id="more"></span><h3 id="Mark一下以防不时之需-https-lncn-org"><a href="#Mark一下以防不时之需-https-lncn-org" class="headerlink" title="Mark一下以防不时之需 https://lncn.org/"></a>Mark一下以防不时之需 <a href="https://lncn.org/">https://lncn.org/</a></h3><p>连接该网站方法如下图所示<br><img src="/img/article/ssr.jpg"></p><h3 id="Window-客户端地址"><a href="#Window-客户端地址" class="headerlink" title="Window 客户端地址"></a>Window 客户端地址</h3><p><a href="https://github.com/shadowsocksrr/shadowsocksr-csharp/releases">https://github.com/shadowsocksrr/shadowsocksr-csharp/releases</a></p><h3 id="Mac-OS-客户端地址"><a href="#Mac-OS-客户端地址" class="headerlink" title="Mac OS 客户端地址"></a>Mac OS 客户端地址</h3><p><a href="https://github.com/wzdnzd/ShadowsocksX-NG-R/releases">https://github.com/wzdnzd/ShadowsocksX-NG-R/releases</a></p>]]></content>
    
    
    <categories>
      
      <category>TroubleShoot</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科学上网</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>核酸检测机构地图微信小程序开发</title>
    <link href="/2021/10/20/%E6%A0%B8%E9%85%B8%E6%A3%80%E6%B5%8B%E6%9C%BA%E6%9E%84%E5%9C%B0%E5%9B%BE%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91/"/>
    <url>/2021/10/20/%E6%A0%B8%E9%85%B8%E6%A3%80%E6%B5%8B%E6%9C%BA%E6%9E%84%E5%9C%B0%E5%9B%BE%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91/</url>
    
    <content type="html"><![CDATA[<h2 id="小程序开发学习"><a href="#小程序开发学习" class="headerlink" title="小程序开发学习"></a>小程序开发学习</h2><span id="more"></span><h5 id="开发（玩）背景"><a href="#开发（玩）背景" class="headerlink" title="开发（玩）背景"></a>开发（玩）背景</h5><p>最近需要做核酸检测，于是想找下附件的机构，但是找了好多都是列表形式的（心里嘀咕了句怎么连附近都看不了，就想着自己做下），不过最后还是找到了带附近功能的小程序，竟然还是国务院客户端。果然还是国家想的周到。虽然找到了这样的小程序自己已经没必要再造轮子了，但是本着学习（玩）的心态还是想着如何重现一下。</p><h5 id="这里也推广下国务院客户端小程序"><a href="#这里也推广下国务院客户端小程序" class="headerlink" title="这里也推广下国务院客户端小程序"></a>这里也推广下国务院客户端小程序</h5><p><img src="/img/article/wechat/%E5%9B%BD%E5%8A%A1%E9%99%A2.jpg"></p><h5 id="最后也算基本实现了这样的功能，贴两张图做个对比吧。"><a href="#最后也算基本实现了这样的功能，贴两张图做个对比吧。" class="headerlink" title="最后也算基本实现了这样的功能，贴两张图做个对比吧。"></a>最后也算基本实现了这样的功能，贴两张图做个对比吧。</h5><p>左边的是国务院客户端，右边是自己做的<br><img src="/img/article/wechat/%E6%A0%B8%E9%85%B8%E6%A3%80%E6%B5%8B.jpg"></p><h5 id="最后再说下开发过程踩过的坑吧"><a href="#最后再说下开发过程踩过的坑吧" class="headerlink" title="最后再说下开发过程踩过的坑吧"></a>最后再说下开发过程踩过的坑吧</h5><p>最主要的坑就是标记在地图上的marker（核酸机构）不显示的问题，首先是经纬度，经度（longitude），维度（latitude），最开始标记点一直没显示就是因为自己把经纬度写反了（关键是后台都不报错）导致一直不显示。其次是经纬度的赋值需是数字类型，字符串类型也是不行的。最后就是服务器域名的配置问题，有些域名比如 <a href="https://apis.map.qq.com/">https://apis.map.qq.com</a> 以及 wx.request 的地址都需要进行配置。当然了还有就是文件路径什么的尽量不要用中文命名，反正奇奇怪怪的bug就是这样产生的。<br>作为一个前端小白做这个花了一个多星期时间，才勉强做出这样的功能，真是令人头秃啊。</p><h5 id="完整代码地址-https-github-com-shubihu-Korok-Mask"><a href="#完整代码地址-https-github-com-shubihu-Korok-Mask" class="headerlink" title="完整代码地址 https://github.com/shubihu/Korok-Mask"></a>完整代码地址 <a href="https://github.com/shubihu/Korok-Mask">https://github.com/shubihu/Korok-Mask</a></h5><p>扫描下方二维码直达，第一次加载有点慢，，，，没办法，用的免费的服务器，慢应该是正常的。<br><img src="/img/article/wechat/%E5%85%8B%E6%B4%9B%E6%A0%BC.jpg"><br>参考</p><ul><li><a href="https://www.ruanyifeng.com/blog/2020/10/wechat-miniprogram-tutorial-part-one.html">https://www.ruanyifeng.com/blog/2020/10/wechat-miniprogram-tutorial-part-one.html</a></li><li><a href="https://github.com/zwz888mm/zhang">https://github.com/zwz888mm/zhang</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>微信小程序</category>
      
    </categories>
    
    
    <tags>
      
      <tag>核酸检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MongoDB基础</title>
    <link href="/2021/10/18/MongoDB%E5%9F%BA%E7%A1%80/"/>
    <url>/2021/10/18/MongoDB%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h2 id="MongoDB-基础命令"><a href="#MongoDB-基础命令" class="headerlink" title="MongoDB 基础命令"></a>MongoDB 基础命令</h2><span id="more"></span><h5 id="启动本地服务端"><a href="#启动本地服务端" class="headerlink" title="启动本地服务端"></a>启动本地服务端</h5><p>进入mongodb bin目录下打开命令行执行 mongod 启动服务端(存储引擎参数 –storageEngine=mmapv1)</p><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs taggerscript">.<span class="hljs-symbol">\m</span>ongod.exe --storageEngine=mmapv1 --dbpath E:<span class="hljs-symbol">\D</span>esktop<span class="hljs-symbol">\J</span>ava<span class="hljs-symbol">\J</span>avaSoftware<span class="hljs-symbol">\m</span>ongoDB<span class="hljs-symbol">\d</span>ata\<br></code></pre></td></tr></table></figure><h5 id="启动本地客户端"><a href="#启动本地客户端" class="headerlink" title="启动本地客户端"></a>启动本地客户端</h5><p>进入mongodb bin目录下打开命令行执行 mongo 启动客户端</p><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs livescript">.<span class="hljs-string">\mongo.exe</span><br></code></pre></td></tr></table></figure><h5 id="查看数据库"><a href="#查看数据库" class="headerlink" title="查看数据库"></a>查看数据库</h5><figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dart"><span class="hljs-keyword">show</span> dbs<br></code></pre></td></tr></table></figure><h5 id="切换数据库（无需新建，直接引用）"><a href="#切换数据库（无需新建，直接引用）" class="headerlink" title="切换数据库（无需新建，直接引用）"></a>切换数据库（无需新建，直接引用）</h5><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs actionscript"><span class="hljs-keyword">use</span> demo<br></code></pre></td></tr></table></figure><h5 id="插入数据-以创建一个雇员信息表为例"><a href="#插入数据-以创建一个雇员信息表为例" class="headerlink" title="插入数据(以创建一个雇员信息表为例)"></a>插入数据(以创建一个雇员信息表为例)</h5><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.Employee</span><span class="hljs-selector-class">.save</span>(&#123;<span class="hljs-selector-tag">code</span>:<span class="hljs-string">&#x27;E01&#x27;</span>, name:<span class="hljs-string">&#x27;Jacky&#x27;</span>&#125;)<br></code></pre></td></tr></table></figure><h5 id="查看数据"><a href="#查看数据" class="headerlink" title="查看数据"></a>查看数据</h5><figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dart"><span class="hljs-keyword">show</span> collections<br></code></pre></td></tr></table></figure><h5 id="查找数据"><a href="#查找数据" class="headerlink" title="查找数据"></a>查找数据</h5><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Employee</span>.</span></span>find<span class="hljs-literal">()</span><br></code></pre></td></tr></table></figure><h5 id="格式化输出查找数据"><a href="#格式化输出查找数据" class="headerlink" title="格式化输出查找数据"></a>格式化输出查找数据</h5><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.Employee</span><span class="hljs-selector-class">.find</span>()<span class="hljs-selector-class">.pretty</span>()<br></code></pre></td></tr></table></figure><h5 id="添加不同格式数据"><a href="#添加不同格式数据" class="headerlink" title="添加不同格式数据"></a>添加不同格式数据</h5><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.Employee</span><span class="hljs-selector-class">.save</span>(&#123;<span class="hljs-selector-tag">code</span>:<span class="hljs-string">&#x27;E02&#x27;</span>, name:<span class="hljs-string">&#x27;Jim&#x27;</span>, email:<span class="hljs-string">&#x27;test@email.com&#x27;</span>&#125;)<br></code></pre></td></tr></table></figure><p>启动mongodb时，提示Unclean shutdown detected mongodb，解决方法:</p><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs taggerscript">.<span class="hljs-symbol">\m</span>ongod.exe --repair --dbpath E:<span class="hljs-symbol">\D</span>esktop<span class="hljs-symbol">\J</span>ava<span class="hljs-symbol">\J</span>avaSoftware<span class="hljs-symbol">\m</span>ongoDB<span class="hljs-symbol">\d</span>ata\<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>数据库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MongoDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>iPhone 快捷指令自动打开低电量模式</title>
    <link href="/2021/09/27/iPhone%E5%BF%AB%E6%8D%B7%E6%8C%87%E4%BB%A4%E8%87%AA%E5%8A%A8%E6%89%93%E5%BC%80%E4%BD%8E%E7%94%B5%E9%87%8F%E6%A8%A1%E5%BC%8F/"/>
    <url>/2021/09/27/iPhone%E5%BF%AB%E6%8D%B7%E6%8C%87%E4%BB%A4%E8%87%AA%E5%8A%A8%E6%89%93%E5%BC%80%E4%BD%8E%E7%94%B5%E9%87%8F%E6%A8%A1%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h2 id="如题"><a href="#如题" class="headerlink" title="如题"></a>如题</h2><span id="more"></span><p>iPhone的电量一直是个软肋，但是其低电量模式续航还真是很可观的。当然一般想起来用低电量模式的时候都是电量剩余不多了的时候，尤其是在外面玩耍的时候，都不能好好地扣手机了。</p><h5 id="具体步骤如下："><a href="#具体步骤如下：" class="headerlink" title="具体步骤如下："></a>具体步骤如下：</h5><p>打开快捷指令，添加自动化，选择 电池电量 ，然后选择 低于50%（这里可以修改，反正我修改成了100%，让手机在99%以下都处于低电量模式以此来延长续航），然后 下一步 ，点击 添加操作，在搜素框搜索低电量，选择设定低电量模式脚本 ，然后再下一步，关掉运行前询问就完成了。</p><h5 id="1"><a href="#1" class="headerlink" title="1"></a>1</h5><p><img src="/img/article/iphone1/1.jpg"></p><h5 id="2"><a href="#2" class="headerlink" title="2"></a>2</h5><p><img src="/img/article/iphone1/2.jpg"></p><h5 id="3"><a href="#3" class="headerlink" title="3"></a>3</h5><p><img src="/img/article/iphone1/3.jpg"></p>]]></content>
    
    
    <categories>
      
      <category>iPhone</category>
      
    </categories>
    
    
    <tags>
      
      <tag>玩</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pyecharts 不同颜色绘制正负柱状图</title>
    <link href="/2021/09/27/Pyecharts%E4%B8%8D%E5%90%8C%E9%A2%9C%E8%89%B2%E7%BB%98%E5%88%B6%E6%AD%A3%E8%B4%9F%E6%9F%B1%E7%8A%B6%E5%9B%BE/"/>
    <url>/2021/09/27/Pyecharts%E4%B8%8D%E5%90%8C%E9%A2%9C%E8%89%B2%E7%BB%98%E5%88%B6%E6%AD%A3%E8%B4%9F%E6%9F%B1%E7%8A%B6%E5%9B%BE/</url>
    
    <content type="html"><![CDATA[<h2 id="如题"><a href="#如题" class="headerlink" title="如题"></a>如题</h2><span id="more"></span><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import akshare as ak<br>import pyecharts.options as opts<br><span class="hljs-keyword">from</span> pyecharts.charts import Bar, Line<br><span class="hljs-keyword">from</span> pyecharts.commons.utils import JsCode<br><br>fund_em_info_df = ak.fund_em_open_fund_info(<span class="hljs-attribute">fund</span>=<span class="hljs-string">&quot;006008&quot;</span>, <span class="hljs-attribute">indicator</span>=<span class="hljs-string">&quot;单位净值走势&quot;</span>)<br><br>fund_name = <span class="hljs-string">&#x27;诺安积极配置混合C&#x27;</span><br>x_data = fund_em_info_df[<span class="hljs-string">&#x27;净值日期&#x27;</span>].tolist()<br>y_data = fund_em_info_df[<span class="hljs-string">&#x27;单位净值&#x27;</span>].tolist()<br>z_data = fund_em_info_df[<span class="hljs-string">&#x27;日增长率&#x27;</span>].tolist()<br><br>background_color_js = (<br>    <span class="hljs-string">&quot;new echarts.graphic.LinearGradient(0, 0, 0, 1, &quot;</span><br>    <span class="hljs-string">&quot;[&#123;offset: 0, color: &#x27;#c86589&#x27;&#125;, &#123;offset: 1, color: &#x27;#06a7ff&#x27;&#125;], false)&quot;</span><br>)<br>area_color_js = (<br>    <span class="hljs-string">&quot;new echarts.graphic.LinearGradient(0, 0, 0, 1, &quot;</span><br>    <span class="hljs-string">&quot;[&#123;offset: 0, color: &#x27;#eb64fb&#x27;&#125;, &#123;offset: 1, color: &#x27;#3fbbff0d&#x27;&#125;], false)&quot;</span><br>)<br><br><br>bar = (<br>    Bar(<span class="hljs-attribute">init_opts</span>=opts.InitOpts(bg_color=JsCode(background_color_js), <span class="hljs-attribute">width</span>=<span class="hljs-string">&#x27;700px&#x27;</span>, <span class="hljs-attribute">height</span>=<span class="hljs-string">&#x27;450px&#x27;</span>))     ## width, height修改画布大小<br>    .add_xaxis(<span class="hljs-attribute">xaxis_data</span>=x_data)<br>    .add_yaxis(<br>        <span class="hljs-attribute">series_name</span>=<span class="hljs-string">&quot;&quot;</span>,<br>        <span class="hljs-attribute">y_axis</span>=z_data,<br>        <span class="hljs-attribute">label_opts</span>=opts.LabelOpts(is_show=False),<br>        <span class="hljs-attribute">itemstyle_opts</span>=opts.ItemStyleOpts(<br>            ### 调用js代码绘制不同颜色<br>            <span class="hljs-attribute">color</span>=JsCode(<br>                <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">                    function(params) &#123;</span><br><span class="hljs-string">                        var colorList;</span><br><span class="hljs-string">                        if (params.data &gt;= 0) &#123;</span><br><span class="hljs-string">                          colorList = &#x27;#FF4500&#x27;;</span><br><span class="hljs-string">                        &#125; else &#123;</span><br><span class="hljs-string">                          colorList = &#x27;#14b143&#x27;;</span><br><span class="hljs-string">                        &#125;</span><br><span class="hljs-string">                        return colorList;</span><br><span class="hljs-string">                    &#125;</span><br><span class="hljs-string">                    &quot;</span><span class="hljs-string">&quot;&quot;</span><br>                )<br>            )<br>        )<br>    .set_global_opts(<br>        <span class="hljs-attribute">title_opts</span>=opts.TitleOpts(<br>            <span class="hljs-attribute">title</span>=fund_name,<br>            <span class="hljs-attribute">pos_bottom</span>=<span class="hljs-string">&quot;90%&quot;</span>,<br>            <span class="hljs-attribute">pos_left</span>=<span class="hljs-string">&quot;center&quot;</span>,<br>            <span class="hljs-attribute">title_textstyle_opts</span>=opts.TextStyleOpts(color=&quot;#fff&quot;, <span class="hljs-attribute">font_size</span>=16),<br>        ),<br>        <span class="hljs-attribute">xaxis_opts</span>=opts.AxisOpts(<br>            <span class="hljs-attribute">type_</span>=<span class="hljs-string">&quot;category&quot;</span>,<br>            <span class="hljs-attribute">boundary_gap</span>=<span class="hljs-literal">False</span>,<br>            <span class="hljs-attribute">axislabel_opts</span>=opts.LabelOpts(margin=30, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;#ffffff63&quot;</span>),<br>            <span class="hljs-attribute">axisline_opts</span>=opts.AxisLineOpts(is_show=False),<br>            <span class="hljs-attribute">axistick_opts</span>=opts.AxisTickOpts(<br>                <span class="hljs-attribute">is_show</span>=<span class="hljs-literal">True</span>,<br>                <span class="hljs-attribute">length</span>=25,<br>                <span class="hljs-attribute">linestyle_opts</span>=opts.LineStyleOpts(color=&quot;#ffffff1f&quot;),<br>            ),<br>            <span class="hljs-attribute">splitline_opts</span>=opts.SplitLineOpts(<br>                <span class="hljs-attribute">is_show</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">linestyle_opts</span>=opts.LineStyleOpts(color=&quot;#ffffff1f&quot;)<br>            ),<br>        ),<br>        <span class="hljs-attribute">yaxis_opts</span>=opts.AxisOpts(<br>            <span class="hljs-attribute">type_</span>=<span class="hljs-string">&quot;value&quot;</span>,<br>            <span class="hljs-attribute">position</span>=<span class="hljs-string">&quot;left&quot;</span>,<br>            <span class="hljs-attribute">axislabel_opts</span>=opts.LabelOpts(margin=20, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;#ffffff63&quot;</span>),<br>            <span class="hljs-attribute">axisline_opts</span>=opts.AxisLineOpts(<br>                <span class="hljs-attribute">linestyle_opts</span>=opts.LineStyleOpts(width=2, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;#fff&quot;</span>)<br>            ),<br>            <span class="hljs-attribute">axistick_opts</span>=opts.AxisTickOpts(<br>                <span class="hljs-attribute">is_show</span>=<span class="hljs-literal">True</span>,<br>                <span class="hljs-attribute">length</span>=15,<br>                <span class="hljs-attribute">linestyle_opts</span>=opts.LineStyleOpts(color=&quot;#ffffff1f&quot;),<br>            ),<br>            <span class="hljs-attribute">splitline_opts</span>=opts.SplitLineOpts(<br>                <span class="hljs-attribute">is_show</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">linestyle_opts</span>=opts.LineStyleOpts(color=&quot;#ffffff1f&quot;)<br>            ),<br>        ),<br><span class="hljs-comment">#         legend_opts=opts.LegendOpts(is_show=True),</span><br>        datazoom_opts=[opts.DataZoomOpts(), opts.DataZoomOpts(<span class="hljs-attribute">type_</span>=<span class="hljs-string">&quot;inside&quot;</span>)]    ## 时间轴显示并可同通过鼠标滑动<br>    )<br>)<br><br><br>line = (<br>    Line(<span class="hljs-attribute">init_opts</span>=opts.InitOpts(bg_color=JsCode(background_color_js)))<br>    .add_xaxis(<span class="hljs-attribute">xaxis_data</span>=x_data)<br>    .add_yaxis(<br>        <span class="hljs-attribute">series_name</span>=<span class="hljs-string">&quot;&quot;</span>,<br>        y_axis=[round(i * 10, 2) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> y_data],<br>        <span class="hljs-attribute">is_smooth</span>=<span class="hljs-literal">True</span>,<br>        <span class="hljs-attribute">is_symbol_show</span>=<span class="hljs-literal">True</span>,<br>        <span class="hljs-attribute">symbol</span>=<span class="hljs-string">&quot;circle&quot;</span>,<br>        <span class="hljs-attribute">symbol_size</span>=6,<br>        <span class="hljs-attribute">linestyle_opts</span>=opts.LineStyleOpts(color=&quot;#fff&quot;),<br>        <span class="hljs-attribute">label_opts</span>=opts.LabelOpts(is_show=True, <span class="hljs-attribute">position</span>=<span class="hljs-string">&quot;top&quot;</span>, <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;white&quot;</span>),<br>        <span class="hljs-attribute">itemstyle_opts</span>=opts.ItemStyleOpts(<br>            <span class="hljs-attribute">color</span>=<span class="hljs-string">&quot;red&quot;</span>, <span class="hljs-attribute">border_color</span>=<span class="hljs-string">&quot;#fff&quot;</span>, <span class="hljs-attribute">border_width</span>=3<br>        ),<br>        <span class="hljs-attribute">tooltip_opts</span>=opts.TooltipOpts(is_show=False),<br>        <span class="hljs-attribute">areastyle_opts</span>=opts.AreaStyleOpts(color=JsCode(area_color_js), <span class="hljs-attribute">opacity</span>=1),<br>    )<br>)<br><br>bar.overlap(line)        ## 混合柱状图和线图<br>bar.render_notebook()<br><br></code></pre></td></tr></table></figure><p>结果如下</p><iframe src="/img/bar_line.html" width="100%" height="500" name="topFrame" scrolling="yes"  noresize="noresize" frameborder="0" id="topFrame"></iframe><p>参考</p><ul><li><a href="https://gallery.pyecharts.org/#/Candlestick/professional_kline_chart">https://gallery.pyecharts.org/#/Candlestick/professional_kline_chart</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pyecharts</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch 学习入门</title>
    <link href="/2021/09/16/Pytorch-learning/"/>
    <url>/2021/09/16/Pytorch-learning/</url>
    
    <content type="html"><![CDATA[<h2 id="Pytorch-Learning-Note"><a href="#Pytorch-Learning-Note" class="headerlink" title="Pytorch Learning Note"></a>Pytorch Learning Note</h2><span id="more"></span><h5 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h5><p>Module：创建一个行为类似于函数的可调用对象，但也可以包含状态（例如神经网络层权重）。 它知道其中包含的 Parameter ，并且可以将其所有坡度归零，遍历它们以进行权重更新等。<br>Parameter：张量的包装器，用于告知 Module 具有在反向传播期间需要更新的权重。 仅更新具有require_grad属性集的张量<br>functional：一个模块（通常按照惯例导入到 F 名称空间中），其中包含激活函数，损失函数等。 以及卷积和线性层等层的无状态版本。</p><h5 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h5><p>包含诸如 SGD 的优化程序，这些优化程序在后退步骤</p><h5 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h5><p>更新 Parameter 的权重。 具有__len__和__getitem__的对象，包括 Pytorch 提供的类，例如 TensorDataset</p><h5 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h5><p>获取任何 Dataset 并创建一个迭代器，该迭代器返回批量数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path<br><span class="hljs-keyword">import</span> requests<br><br>DATA_PATH = Path(<span class="hljs-string">&quot;data&quot;</span>)<br>PATH = DATA_PATH / <span class="hljs-string">&quot;mnist&quot;</span><br><br>PATH.mkdir(parents=<span class="hljs-literal">True</span>, exist_ok=<span class="hljs-literal">True</span>)<br><br>URL = <span class="hljs-string">&quot;https://github.com/pytorch/tutorials/raw/master/_static/&quot;</span><br>FILENAME = <span class="hljs-string">&quot;mnist.pkl.gz&quot;</span><br><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> (PATH / FILENAME).exists():<br>        content = requests.get(URL + FILENAME).content<br>        (PATH / FILENAME).<span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;wb&quot;</span>).write(content)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> gzip<br><br><span class="hljs-keyword">with</span> gzip.<span class="hljs-built_in">open</span>((PATH / FILENAME).as_posix(), <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=<span class="hljs-string">&quot;latin-1&quot;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>pyplot.imshow(x_train[<span class="hljs-number">0</span>].reshape((<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)), cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br><span class="hljs-built_in">print</span>(x_train.shape)<br></code></pre></td></tr></table></figure><pre><code>(50000, 784)</code></pre><!-- ![png](output_2_1.png) --><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>x_train, y_train, x_valid, y_valid = <span class="hljs-built_in">map</span>(<br>    torch.tensor, (x_train, y_train, x_valid, y_valid)<br>)<br>n, c = x_train.shape<br>x_train, x_train.shape, y_train.<span class="hljs-built_in">min</span>(), y_train.<span class="hljs-built_in">max</span>()<br><span class="hljs-built_in">print</span>(x_train, y_train)<br><span class="hljs-built_in">print</span>(x_train.shape)<br><span class="hljs-built_in">print</span>(y_train.<span class="hljs-built_in">min</span>(), y_train.<span class="hljs-built_in">max</span>())<br></code></pre></td></tr></table></figure><pre><code>tensor([[0., 0., 0.,  ..., 0., 0., 0.],        [0., 0., 0.,  ..., 0., 0., 0.],        [0., 0., 0.,  ..., 0., 0., 0.],        ...,        [0., 0., 0.,  ..., 0., 0., 0.],        [0., 0., 0.,  ..., 0., 0., 0.],        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])torch.Size([50000, 784])tensor(0) tensor(9)</code></pre><h5 id="从0构建神经网络线性模型"><a href="#从0构建神经网络线性模型" class="headerlink" title="从0构建神经网络线性模型"></a>从0构建神经网络线性模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><br>weights = torch.randn(<span class="hljs-number">784</span>, <span class="hljs-number">10</span>) / math.sqrt(<span class="hljs-number">784</span>)<br>weights.requires_grad_()<br>bias = torch.zeros(<span class="hljs-number">10</span>, requires_grad=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment">## softmax激活函数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">log_softmax</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> x - x.exp().<span class="hljs-built_in">sum</span>(-<span class="hljs-number">1</span>).log().unsqueeze(-<span class="hljs-number">1</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model</span>(<span class="hljs-params">xb</span>):</span><br>    <span class="hljs-keyword">return</span> log_softmax(xb @ weights + bias)  <span class="hljs-comment">#  @代表点积运算</span><br><br>bs = <span class="hljs-number">64</span>  <span class="hljs-comment"># batch size</span><br><br>xb = x_train[<span class="hljs-number">0</span>:bs]  <span class="hljs-comment"># a mini-batch from x</span><br>preds = model(xb)  <span class="hljs-comment"># predictions</span><br>preds[<span class="hljs-number">0</span>], preds.shape<br><span class="hljs-built_in">print</span>(preds[<span class="hljs-number">0</span>], preds.shape)<br><br><span class="hljs-comment">## 损失函数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">nll</span>(<span class="hljs-params"><span class="hljs-built_in">input</span>, target</span>):</span><br>    <span class="hljs-keyword">return</span> -<span class="hljs-built_in">input</span>[<span class="hljs-built_in">range</span>(target.shape[<span class="hljs-number">0</span>]), target].mean()<br><br>loss_func = nll<br><br>yb = y_train[<span class="hljs-number">0</span>:bs]<br><span class="hljs-built_in">print</span>(loss_func(preds, yb))<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">accuracy</span>(<span class="hljs-params">out, yb</span>):</span><br>    preds = torch.argmax(out, dim=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> (preds == yb).<span class="hljs-built_in">float</span>().mean()<br><br><span class="hljs-built_in">print</span>(accuracy(preds, yb))<br><br><span class="hljs-keyword">from</span> IPython.core.debugger <span class="hljs-keyword">import</span> set_trace<br><br>lr = <span class="hljs-number">0.5</span>  <span class="hljs-comment"># learning rate</span><br>epochs = <span class="hljs-number">2</span>  <span class="hljs-comment"># how many epochs to train for</span><br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>((n - <span class="hljs-number">1</span>) // bs + <span class="hljs-number">1</span>):<br>        <span class="hljs-comment">#         set_trace()</span><br>        start_i = i * bs<br>        end_i = start_i + bs<br>        xb = x_train[start_i:end_i]<br>        yb = y_train[start_i:end_i]<br>        pred = model(xb)<br>        loss = loss_func(pred, yb)<br><br>        loss.backward()<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            weights -= weights.grad * lr<br>            bias -= bias.grad * lr<br>            weights.grad.zero_()<br>            bias.grad.zero_()<br>            <br><span class="hljs-built_in">print</span>(loss_func(model(xb), yb), accuracy(model(xb), yb))<br><br></code></pre></td></tr></table></figure><pre><code>tensor([-2.5487, -2.8346, -2.7262, -2.1794, -2.1199, -2.1041, -1.9327, -2.1947,        -2.5637, -2.2133], grad_fn=&lt;SelectBackward&gt;) torch.Size([64, 10])tensor(2.3308, grad_fn=&lt;NegBackward&gt;)tensor(0.1094)tensor(0.0806, grad_fn=&lt;NegBackward&gt;) tensor(1.)</code></pre><h5 id="使用torch-nn-functional-重构"><a href="#使用torch-nn-functional-重构" class="headerlink" title="使用torch.nn.functional 重构"></a>使用torch.nn.functional 重构</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br>loss_func = F.cross_entropy<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model</span>(<span class="hljs-params">xb</span>):</span><br>    <span class="hljs-keyword">return</span> xb @ weights + bias<br><br><span class="hljs-built_in">print</span>(loss_func(model(xb), yb), accuracy(model(xb), yb))<br><br></code></pre></td></tr></table></figure><pre><code>tensor(0.0806, grad_fn=&lt;NllLossBackward&gt;) tensor(1.)</code></pre><h5 id="使用nn-Module重构"><a href="#使用nn-Module重构" class="headerlink" title="使用nn.Module重构"></a>使用nn.Module重构</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Mnist_Logistic</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.weights = nn.Parameter(torch.randn(<span class="hljs-number">784</span>, <span class="hljs-number">10</span>) / math.sqrt(<span class="hljs-number">784</span>))<br>        self.bias = nn.Parameter(torch.zeros(<span class="hljs-number">10</span>))<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, xb</span>):</span><br>        <span class="hljs-keyword">return</span> xb @ self.weights + self.bias<br>    <br>model = Mnist_Logistic()<br><span class="hljs-built_in">print</span>(loss_func(model(xb), yb))<br><br>    <br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span>():</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>((n - <span class="hljs-number">1</span>) // bs + <span class="hljs-number">1</span>):<br>            start_i = i * bs<br>            end_i = start_i + bs<br>            xb = x_train[start_i:end_i]<br>            yb = y_train[start_i:end_i]<br>            pred = model(xb)<br>            loss = loss_func(pred, yb)<br><br>            loss.backward()<br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters():<br>                    p -= p.grad * lr<br>                model.zero_grad()<br><br>fit()<br><span class="hljs-built_in">print</span>(loss_func(model(xb), yb))<br><br></code></pre></td></tr></table></figure><pre><code>tensor(2.4222, grad_fn=&lt;NllLossBackward&gt;)tensor(0.0817, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h5 id="使用nn-Linear重构"><a href="#使用nn-Linear重构" class="headerlink" title="使用nn.Linear重构"></a>使用nn.Linear重构</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Mnist_Logistic</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.lin = nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, xb</span>):</span><br>        <span class="hljs-keyword">return</span> self.lin(xb)<br>    <br>model = Mnist_Logistic()<br><span class="hljs-built_in">print</span>(loss_func(model(xb), yb))<br>fit()<br><br><span class="hljs-built_in">print</span>(loss_func(model(xb), yb))<br></code></pre></td></tr></table></figure><pre><code>tensor(2.3090, grad_fn=&lt;NllLossBackward&gt;)tensor(0.0824, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h5 id="使用optim重构"><a href="#使用optim重构" class="headerlink" title="使用optim重构"></a>使用optim重构</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_model</span>():</span><br>    model = Mnist_Logistic()<br>    <span class="hljs-keyword">return</span> model, optim.SGD(model.parameters(), lr=lr)<br><br>model, opt = get_model()<br><span class="hljs-built_in">print</span>(loss_func(model(xb), yb))<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>((n - <span class="hljs-number">1</span>) // bs + <span class="hljs-number">1</span>):<br>        start_i = i * bs<br>        end_i = start_i + bs<br>        xb = x_train[start_i:end_i]<br>        yb = y_train[start_i:end_i]<br>        pred = model(xb)<br>        loss = loss_func(pred, yb)<br><br>        loss.backward()<br>        opt.step()<br>        opt.zero_grad()<br><br><span class="hljs-built_in">print</span>(loss_func(model(xb), yb))<br></code></pre></td></tr></table></figure><pre><code>tensor(2.2990, grad_fn=&lt;NllLossBackward&gt;)tensor(0.0805, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h5 id="使用Dataset重构"><a href="#使用Dataset重构" class="headerlink" title="使用Dataset重构"></a>使用Dataset重构</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> TensorDataset<br><br>train_ds = TensorDataset(x_train, y_train)<br>xb,yb = train_ds[i*bs : i*bs+bs]<br>model, opt = get_model()<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>((n - <span class="hljs-number">1</span>) // bs + <span class="hljs-number">1</span>):<br>        xb, yb = train_ds[i * bs: i * bs + bs]<br>        pred = model(xb)<br>        loss = loss_func(pred, yb)<br><br>        loss.backward()<br>        opt.step()<br>        opt.zero_grad()<br><br><span class="hljs-built_in">print</span>(loss_func(model(xb), yb))<br></code></pre></td></tr></table></figure><pre><code>tensor(0.0817, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h5 id="使用DataLoader重构"><a href="#使用DataLoader重构" class="headerlink" title="使用DataLoader重构"></a>使用DataLoader重构</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br>train_ds = TensorDataset(x_train, y_train)<br>train_dl = DataLoader(train_ds, batch_size=bs)<br><br>model, opt = get_model()<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    <span class="hljs-keyword">for</span> xb, yb <span class="hljs-keyword">in</span> train_dl:<br>        pred = model(xb)<br>        loss = loss_func(pred, yb)<br><br>        loss.backward()<br>        opt.step()<br>        opt.zero_grad()<br><br><span class="hljs-built_in">print</span>(loss_func(model(xb), yb))<br></code></pre></td></tr></table></figure><pre><code>tensor(0.0803, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h5 id="添加验证"><a href="#添加验证" class="headerlink" title="添加验证"></a>添加验证</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">train_ds = TensorDataset(x_train, y_train)<br>train_dl = DataLoader(train_ds, batch_size=bs, shuffle=<span class="hljs-literal">True</span>)<br><br>valid_ds = TensorDataset(x_valid, y_valid)<br>valid_dl = DataLoader(valid_ds, batch_size=bs * <span class="hljs-number">2</span>)<br><br>model, opt = get_model()<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    model.train()<br>    <span class="hljs-keyword">for</span> xb, yb <span class="hljs-keyword">in</span> train_dl:<br>        pred = model(xb)<br>        loss = loss_func(pred, yb)<br><br>        loss.backward()<br>        opt.step()<br>        opt.zero_grad()<br><br>    model.<span class="hljs-built_in">eval</span>()<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        valid_loss = <span class="hljs-built_in">sum</span>(loss_func(model(xb), yb) <span class="hljs-keyword">for</span> xb, yb <span class="hljs-keyword">in</span> valid_dl)<br><br>    <span class="hljs-built_in">print</span>(epoch, valid_loss / <span class="hljs-built_in">len</span>(valid_dl))<br></code></pre></td></tr></table></figure><pre><code>0 tensor(0.3093)1 tensor(0.3198)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## 创建fit()和get_data()</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">loss_batch</span>(<span class="hljs-params">model, loss_func, xb, yb, opt=<span class="hljs-literal">None</span></span>):</span><br>    loss = loss_func(model(xb), yb)<br><br>    <span class="hljs-keyword">if</span> opt <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        loss.backward()<br>        opt.step()<br>        opt.zero_grad()<br><br>    <span class="hljs-keyword">return</span> loss.item(), <span class="hljs-built_in">len</span>(xb)<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span>(<span class="hljs-params">epochs, model, loss_func, opt, train_dl, valid_dl</span>):</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        model.train()<br>        <span class="hljs-keyword">for</span> xb, yb <span class="hljs-keyword">in</span> train_dl:<br>            loss_batch(model, loss_func, xb, yb, opt)<br><br>        model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            losses, nums = <span class="hljs-built_in">zip</span>(<br>                *[loss_batch(model, loss_func, xb, yb) <span class="hljs-keyword">for</span> xb, yb <span class="hljs-keyword">in</span> valid_dl]<br>            )<br>        val_loss = np.<span class="hljs-built_in">sum</span>(np.multiply(losses, nums)) / np.<span class="hljs-built_in">sum</span>(nums)<br><br>        <span class="hljs-built_in">print</span>(epoch, val_loss)<br>        <br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_data</span>(<span class="hljs-params">train_ds, valid_ds, bs</span>):</span><br>    <span class="hljs-keyword">return</span> (<br>        DataLoader(train_ds, batch_size=bs, shuffle=<span class="hljs-literal">True</span>),<br>        DataLoader(valid_ds, batch_size=bs * <span class="hljs-number">2</span>),<br>    )<br><br>train_dl, valid_dl = get_data(train_ds, valid_ds, bs)<br>model, opt = get_model()<br>fit(epochs, model, loss_func, opt, train_dl, valid_dl)<br></code></pre></td></tr></table></figure><pre><code>0 0.33136114755868911 0.35820939881801606</code></pre><h5 id="切换到-CNN"><a href="#切换到-CNN" class="headerlink" title="切换到 CNN"></a>切换到 CNN</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Mnist_CNN</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br>        self.conv2 = nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br>        self.conv3 = nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">10</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, xb</span>):</span><br>        xb = xb.view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br>        xb = F.relu(self.conv1(xb))<br>        xb = F.relu(self.conv2(xb))<br>        xb = F.relu(self.conv3(xb))<br>        xb = F.avg_pool2d(xb, <span class="hljs-number">4</span>)<br>        <span class="hljs-keyword">return</span> xb.view(-<span class="hljs-number">1</span>, xb.size(<span class="hljs-number">1</span>))<br><br>lr = <span class="hljs-number">0.1</span><br><br>model = Mnist_CNN()<br>opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="hljs-number">0.9</span>)<br><br>fit(epochs, model, loss_func, opt, train_dl, valid_dl)<br></code></pre></td></tr></table></figure><pre><code>0 0.29366704518795011 0.21561954822540283</code></pre><h5 id="nn-Sequential-Sequential对象以顺序方式运行其中包含的每个模块。"><a href="#nn-Sequential-Sequential对象以顺序方式运行其中包含的每个模块。" class="headerlink" title="nn.Sequential   Sequential对象以顺序方式运行其中包含的每个模块。"></a>nn.Sequential   Sequential对象以顺序方式运行其中包含的每个模块。</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Lambda</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, func</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.func = func<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-keyword">return</span> self.func(x)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">preprocess</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br><br>model = nn.Sequential(<br>    Lambda(preprocess),<br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>),<br>    nn.ReLU(),<br>    nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>),<br>    nn.ReLU(),<br>    nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">10</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>),<br>    nn.ReLU(),<br>    nn.AvgPool2d(<span class="hljs-number">4</span>),<br>    Lambda(<span class="hljs-keyword">lambda</span> x: x.view(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)),<br>)<br><br>opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="hljs-number">0.9</span>)<br><br>fit(epochs, model, loss_func, opt, train_dl, valid_dl)<br></code></pre></td></tr></table></figure><pre><code>0 0.40379241023063661 0.25595326462984086</code></pre><h5 id="包装DataLoader"><a href="#包装DataLoader" class="headerlink" title="包装DataLoader"></a>包装DataLoader</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">preprocess</span>(<span class="hljs-params">x, y</span>):</span><br>    <span class="hljs-keyword">return</span> x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>), y<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WrappedDataLoader</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, dl, func</span>):</span><br>        self.dl = dl<br>        self.func = func<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.dl)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__iter__</span>(<span class="hljs-params">self</span>):</span><br>        batches = <span class="hljs-built_in">iter</span>(self.dl)<br>        <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> batches:<br>            <span class="hljs-keyword">yield</span> (self.func(*b))<br><br>train_dl, valid_dl = get_data(train_ds, valid_ds, bs)<br>train_dl = WrappedDataLoader(train_dl, preprocess)<br>valid_dl = WrappedDataLoader(valid_dl, preprocess)<br><br>model = nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>),<br>    nn.ReLU(),<br>    nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>),<br>    nn.ReLU(),<br>    nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">10</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>),<br>    nn.ReLU(),<br>    nn.AdaptiveAvgPool2d(<span class="hljs-number">1</span>),<br>    Lambda(<span class="hljs-keyword">lambda</span> x: x.view(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)),<br>)<br><br>opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="hljs-number">0.9</span>)<br><br>fit(epochs, model, loss_func, opt, train_dl, valid_dl)<br><br></code></pre></td></tr></table></figure><pre><code>0 0.313964178180694551 0.2551067463874817</code></pre><h5 id="使用GPU，，，如果有"><a href="#使用GPU，，，如果有" class="headerlink" title="使用GPU，，，如果有"></a>使用GPU，，，如果有</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(torch.cuda.is_available())<br>dev = torch.device(<br>    <span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">preprocess</span>(<span class="hljs-params">x, y</span>):</span><br>    <span class="hljs-keyword">return</span> x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>).to(dev), y.to(dev)<br><br>train_dl, valid_dl = get_data(train_ds, valid_ds, bs)<br>train_dl = WrappedDataLoader(train_dl, preprocess)<br>valid_dl = WrappedDataLoader(valid_dl, preprocess)<br><br>model.to(dev)<br>opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="hljs-number">0.9</span>)<br>fit(epochs, model, loss_func, opt, train_dl, valid_dl)<br><br></code></pre></td></tr></table></figure><pre><code>False0 0.223737240695953381 0.2494806985616684</code></pre>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>iPhone 快捷指令上下班半自动打卡</title>
    <link href="/2021/09/10/iPhone/"/>
    <url>/2021/09/10/iPhone/</url>
    
    <content type="html"><![CDATA[<h2 id="如题"><a href="#如题" class="headerlink" title="如题"></a>如题</h2><span id="more"></span><p>打工人有时候会忘记打卡，为了能尽量少忘记，有时候会设定闹钟来提醒。在iPhone上还有个稍微半自动化的应用也可以实现，就是快捷指令。为啥是半自动化呢，因为息屏状态下是无法执行的，只是会有提醒，需要解锁后点击才能执行。当然了在不息屏的状态下是完全可以实现自动化的，但是也不可能一直不息屏吧。iPhone的那点点电量就不说了。<br>还有个前提就是提前设置好打卡APP的快捷打卡功能。</p><h6 id="快捷指令APP（一般自带，不小心删了可以去APP-Store重新下载）"><a href="#快捷指令APP（一般自带，不小心删了可以去APP-Store重新下载）" class="headerlink" title="快捷指令APP（一般自带，不小心删了可以去APP Store重新下载）"></a>快捷指令APP（一般自带，不小心删了可以去APP Store重新下载）</h6><p>打开快捷指令app，点击自动化，点击创建个人自动化<br><img src="/img/article/iphone/0.jpg"></p><h5 id="选择特定时间"><a href="#选择特定时间" class="headerlink" title="选择特定时间"></a>选择特定时间</h5><p>这个页面还有其它事件，比如到达、离开等。一般现在企业微信或者钉钉亦或者是公司自己的打卡APP都是基于距离进行定位打卡，所以这里也可以选择到达或者离开，这里以特定时间为例，其它事件可以自行摸索。<br>然后设置上下班时间，可以选择每天，每周（可选周一至周五），按心情选择。<br><img src="/img/article/iphone/1.jpg"></p><h5 id="点击下一步，点击添加操作。"><a href="#点击下一步，点击添加操作。" class="headerlink" title="点击下一步，点击添加操作。"></a>点击下一步，点击添加操作。</h5><p><img src="/img/article/iphone/2.jpg"></p><h5 id="搜索输入：打开app，点击打开app，点击选择。"><a href="#搜索输入：打开app，点击打开app，点击选择。" class="headerlink" title="搜索输入：打开app，点击打开app，点击选择。"></a>搜索输入：打开app，点击打开app，点击选择。</h5><p><img src="/img/article/iphone/3.jpg"></p><h5 id="搜索输入企业微信，点击企业微信，点击下一步。"><a href="#搜索输入企业微信，点击企业微信，点击下一步。" class="headerlink" title="搜索输入企业微信，点击企业微信，点击下一步。"></a>搜索输入企业微信，点击企业微信，点击下一步。</h5><p><img src="/img/article/iphone/4.jpg"></p><h5 id="将运行前询问关闭，点击不询问，点击完成。"><a href="#将运行前询问关闭，点击不询问，点击完成。" class="headerlink" title="将运行前询问关闭，点击不询问，点击完成。"></a>将运行前询问关闭，点击不询问，点击完成。</h5><p><img src="/img/article/iphone/5.jpg"></p><h5 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h5><p>iPhone的快捷指令有点像编程，所以功能不止这些，网上还有如何实现敲击背面显示健康码等，有兴趣的可以搜来看看。</p>]]></content>
    
    
    <categories>
      
      <category>iPhone</category>
      
    </categories>
    
    
    <tags>
      
      <tag>玩</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python 监控</title>
    <link href="/2021/09/02/python%E7%9B%91%E6%8E%A7/"/>
    <url>/2021/09/02/python%E7%9B%91%E6%8E%A7/</url>
    
    <content type="html"><![CDATA[<h2 id="使用python监控电脑键盘、鼠标并拍照录像"><a href="#使用python监控电脑键盘、鼠标并拍照录像" class="headerlink" title="使用python监控电脑键盘、鼠标并拍照录像"></a>使用python监控电脑键盘、鼠标并拍照录像</h2><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keyboard<br><span class="hljs-keyword">from</span> cv2 <span class="hljs-keyword">import</span> cv2<br><span class="hljs-comment"># from pynput.mouse import Listener</span><br><span class="hljs-keyword">import</span> pyautogui <span class="hljs-keyword">as</span> pag    <span class="hljs-comment">#监听鼠标</span><br><span class="hljs-comment"># from pynput.keyboard import Key, Listener</span><br><span class="hljs-keyword">from</span> threading <span class="hljs-keyword">import</span> Thread<br><br>x1, y1 = pag.position()<br><span class="hljs-comment"># print(x1, y1)</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">camera</span>():</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    拍照</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    cap = cv2.VideoCapture(<span class="hljs-number">0</span>)<br>    ret,frame = cap.read() <span class="hljs-comment">#读取摄像头内容</span><br>    cv2.imwrite(<span class="hljs-string">&quot;./test.jpg&quot;</span>,frame) <span class="hljs-comment">#保存到磁盘</span><br>    <span class="hljs-comment">#释放摄像头</span><br>    cap.release()<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">record_video</span>():</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    录制视频</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    cap = cv2.VideoCapture(<span class="hljs-number">0</span>)<br>    fps = <span class="hljs-number">30</span><br>    size=(<span class="hljs-built_in">int</span>(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),<span class="hljs-built_in">int</span>(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))<br>    videoWriter=cv2.VideoWriter(<span class="hljs-string">&#x27;./test.avi&#x27;</span>,cv2.VideoWriter_fourcc(<span class="hljs-string">&#x27;X&#x27;</span>,<span class="hljs-string">&#x27;V&#x27;</span>,<span class="hljs-string">&#x27;I&#x27;</span>,<span class="hljs-string">&#x27;D&#x27;</span>),fps,size)<br>    success,frame = cap.read()<br>    numFrameRemaining = <span class="hljs-number">5</span> * fps    <span class="hljs-comment">#摄像头捕获持续时间</span><br>    <span class="hljs-keyword">while</span> success <span class="hljs-keyword">and</span> numFrameRemaining &gt; <span class="hljs-number">0</span>:<br>        videoWriter.write(frame)<br>        success,frame = cap.read()<br>        numFrameRemaining -= <span class="hljs-number">1</span><br><br>    cap.release()<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">display_video</span>():</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    实时窗口</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    face_locations = []<br>    cap = cv2.VideoCapture(<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        <span class="hljs-comment"># Grab a single frame of video</span><br>        ret, frame = cap.read()<br><br>        <span class="hljs-comment"># Convert the image from BGR color (whichOpenCV uses) to RGB color (which face_recognition uses)</span><br>        rgb_frame = frame[:, :, ::-<span class="hljs-number">1</span>]<br><br>        <span class="hljs-comment"># Find all the faces in the current frameof video</span><br>        face_locations = face_recognition.face_locations(rgb_frame)<br><br>        <span class="hljs-comment"># Display the results</span><br>        <span class="hljs-keyword">for</span> top, right, bottom, left <span class="hljs-keyword">in</span> face_locations:<br>            <span class="hljs-comment"># Draw a box around the face</span><br>            cv2.rectangle(frame, (left, top),(right, bottom), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">2</span>)<br><br>        <span class="hljs-comment"># Display the resulting image</span><br>        cv2.imshow(<span class="hljs-string">&#x27;Video&#x27;</span>, frame)<br><br>        <span class="hljs-comment"># Hit &#x27;q&#x27; on the keyboard to quit!</span><br>        <span class="hljs-keyword">if</span> cv2.waitKey(<span class="hljs-number">1</span>) &amp; <span class="hljs-number">0xFF</span> == <span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;q&#x27;</span>):<br>            <span class="hljs-keyword">break</span><br><br>    <span class="hljs-comment"># Release handle tothe webcam</span><br>    cap.release()<br>    cv2.destroyAllWindows()<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">display_video2</span>():</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    实时检测</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment">#存储知道人名列表</span><br>    known_names=[<span class="hljs-string">&#x27;yahaha1&#x27;</span>, <span class="hljs-string">&#x27;yahaha2&#x27;</span>] <br>    <span class="hljs-comment">#存储知道的特征值</span><br>    known_faces=[]<br><br>    image1 =face_recognition.load_image_file(<span class="hljs-string">&quot;yahaha2.jpg&quot;</span>)<br>    face_encoding1 =face_recognition.face_encodings(image1)<br><br>    image2 =face_recognition.load_image_file(<span class="hljs-string">&quot;yahaha1.jpg&quot;</span>)<br>    face_encoding2 =face_recognition.face_encodings(image1)<br><br>    <span class="hljs-keyword">if</span> face_encoding1 <span class="hljs-keyword">and</span> face_encoding2:<br>        face_encoding1 = face_encoding1[<span class="hljs-number">0</span>]<br>        face_encoding2 = face_encoding2[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">else</span>:<br>        sys.exit()<br><br>    known_faces = [face_encoding1, face_encoding2]<br><br>    cap = cv2.VideoCapture(<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        <span class="hljs-comment"># Grab a single frame of video</span><br>        ret, frame = cap.read()<br>        <span class="hljs-comment"># Convert the image from BGR color (whichOpenCV uses) to RGB color (which face_recognition uses)</span><br>        rgb_frame = frame[:, :, ::-<span class="hljs-number">1</span>]<br><br>        <span class="hljs-comment"># Find all the faces and face encodings inthe current frame of video</span><br>        face_locations =face_recognition.face_locations(rgb_frame)  <span class="hljs-comment"># 如有gpu可添加参数model=&#x27;cnn&#x27;提升精度</span><br>        face_encodings =face_recognition.face_encodings(rgb_frame, face_locations)<br><br>        face_names = []<br>        <span class="hljs-keyword">for</span> face_encoding <span class="hljs-keyword">in</span> face_encodings:<br>            <span class="hljs-comment"># See if the face is a match for theknown face(s)</span><br>            matches =face_recognition.compare_faces(known_faces, face_encoding, tolerance=<span class="hljs-number">0.60</span>)<br><br>            name = <span class="hljs-literal">None</span><br>            <span class="hljs-comment"># if match[0]:</span><br>            <span class="hljs-comment">#     name = &quot;Yahaha&quot;</span><br>            <span class="hljs-built_in">print</span>(matches)<br>            <span class="hljs-keyword">if</span> <span class="hljs-literal">True</span> <span class="hljs-keyword">in</span> matches:<br>                first_match_index = matches.index(<span class="hljs-literal">True</span>)<br>                name = known_names[first_match_index]<br>            <span class="hljs-keyword">else</span>:<br>                name = <span class="hljs-string">&#x27;Unkonwn&#x27;</span><br><br>            face_names.append(name)<br><br>        <span class="hljs-comment"># Label the results</span><br>        <span class="hljs-keyword">for</span> (top, right, bottom, left), name <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(face_locations, face_names):<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> name:<br>                <span class="hljs-keyword">continue</span><br><br>            <span class="hljs-comment"># Draw a box around the face</span><br>            cv2.rectangle(frame, (left, top),(right, bottom), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">2</span>)<br>            <span class="hljs-comment"># Draw a label with a name below theface</span><br>            cv2.rectangle(frame, (left, bottom -<span class="hljs-number">25</span>), (right, bottom), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), cv2.FILLED)<br>            font = cv2.FONT_HERSHEY_DUPLEX<br>            cv2.putText(frame, name, (left + <span class="hljs-number">6</span>,bottom - <span class="hljs-number">6</span>), font, <span class="hljs-number">0.5</span>, (<span class="hljs-number">255</span>, <span class="hljs-number">255</span>, <span class="hljs-number">255</span>), <span class="hljs-number">1</span>)<br><br>        cv2.imshow(<span class="hljs-string">&#x27;Video&#x27;</span>, frame)<br><br>        <span class="hljs-comment"># Hit &#x27;q&#x27; on the keyboard to quit!</span><br>        <span class="hljs-keyword">if</span> cv2.waitKey(<span class="hljs-number">1</span>) &amp; <span class="hljs-number">0xFF</span> == <span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;q&#x27;</span>):<br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-comment"># All done!</span><br>    cap.release()<br>    cv2.destroyAllWindows()<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">proof</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-comment"># print(x)</span><br>    <span class="hljs-comment"># record_video()</span><br>    camera()<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">monitor_keyboard</span>():</span><br>    keyboard.hook(proof)<br>    <span class="hljs-comment">#按下任何按键时，都会调用proof，其中一定会传一个值，就是键盘事件</span><br>    keyboard.wait()<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">monitor_mouse</span>():</span><br>    x2, y2 = pag.position()<br>    <span class="hljs-keyword">while</span> x1 == x2:<br>        x2, y2 = pag.position()<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># record_video()</span><br>        camera()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    k = Thread(target=monitor_keyboard, args=())<br>    m = Thread(target=monitor_mouse, args=())<br>    k.start()<br>    m.start()<br>    k.join()<br>    m.join()<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>玩</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>We Are SVIP</title>
    <link href="/2021/08/20/WeAreSVIP/"/>
    <url>/2021/08/20/WeAreSVIP/</url>
    
    <content type="html"><![CDATA[<h2 id="薅各大厂视频羊毛"><a href="#薅各大厂视频羊毛" class="headerlink" title="薅各大厂视频羊毛"></a>薅各大厂视频羊毛</h2><span id="more"></span><p>周末了又可以追剧了，最近在看扫黑风暴，但是吧，现在各大视频网站都是需要VIP才能看，有的甚至更可恶，还要超前点播。这嘴脸真的是穷凶极恶。都是大厂了，说好的回报社会呢，，，就这样回报社会呢。人家（我没有）都已经忍痛购买你的VIP了，还不满足。算了，你有张良计，我有过墙梯。<br>今天就给大家分享个可以薅他们VIP的插件——油猴，确切的说是各路大神开发的脚本，油猴只是个脚本管家。如果能上google的话就方便很多，直接搜索油猴插件进行安装。但是国内吧，，，，可能很多人都无法上谷歌。这里就介绍下本地安装油猴插件的方法。</p><h5 id="No-1"><a href="#No-1" class="headerlink" title="No.1"></a>No.1</h5><p>首先先下载下油猴，链接在此：<a href="https://pan.baidu.com/s/1FQZBBjqr2s8f-9idizxI6w">https://pan.baidu.com/s/1FQZBBjqr2s8f-9idizxI6w</a> ;提取码：<span  style="color: #519D9E; ">8fmu</span><br>下载完成后直接解压到该文件夹。<br>然后打开我们的谷歌浏览器，在搜索地址栏输入：<span  style="color: #519D9E; ">chrome://extensions/</span>，进入扩展程序界面，<span  style="color: #519D9E; ">打开右上角的开发者模式</span>。接着选择<span  style="color: #519D9E; ">左上角的加载已解压的扩展程序</span>，然后选择我们刚刚解压过的油猴的目录即可，到这里油猴插件就完成了。接下来就是安装脚本了。</p><h5 id="No-2"><a href="#No-2" class="headerlink" title="No.2"></a>No.2</h5><p>然后进入这个网站：<a href="https://greasyfork.org/zh-CN">https://greasyfork.org/zh-CN</a> ，搜索VIP，会出现很多个脚本，自行选择就好。我选择的是 <a href="https://greasyfork.org/zh-CN/scripts/370634-%E6%87%92%E4%BA%BA%E4%B8%93%E7%94%A8-%E5%85%A8%E7%BD%91vip%E8%A7%86%E9%A2%91%E5%85%8D%E8%B4%B9%E7%A0%B4%E8%A7%A3%E5%8E%BB%E5%B9%BF%E5%91%8A-%E5%85%A8%E7%BD%91%E9%9F%B3%E4%B9%90%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BD-%E7%99%BE%E5%BA%A6%E7%BD%91%E7%9B%98%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BD%E7%AD%89%E5%A4%9A%E5%90%88%E4%B8%80%E7%89%88-%E9%95%BF%E6%9C%9F%E6%9B%B4%E6%96%B0-%E6%94%BE%E5%BF%83%E4%BD%BF%E7%94%A8">https://greasyfork.org/zh-CN/scripts/370634-%E6%87%92%E4%BA%BA%E4%B8%93%E7%94%A8-%E5%85%A8%E7%BD%91vip%E8%A7%86%E9%A2%91%E5%85%8D%E8%B4%B9%E7%A0%B4%E8%A7%A3%E5%8E%BB%E5%B9%BF%E5%91%8A-%E5%85%A8%E7%BD%91%E9%9F%B3%E4%B9%90%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BD-%E7%99%BE%E5%BA%A6%E7%BD%91%E7%9B%98%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BD%E7%AD%89%E5%A4%9A%E5%90%88%E4%B8%80%E7%89%88-%E9%95%BF%E6%9C%9F%E6%9B%B4%E6%96%B0-%E6%94%BE%E5%BF%83%E4%BD%BF%E7%94%A8</a> 这个，觉得还是比较好用的。之后点击安装就大功告成啦。</p><h4 id="No-3"><a href="#No-3" class="headerlink" title="No.3"></a>No.3</h4><p>利器有了，就可以去各大视频网站薅一波了，超前点播也可以薅哦。油猴这个插件及其脚本的功能不仅限于此，有兴趣可以自行摸索。</p>]]></content>
    
    
    <categories>
      
      <category>TroubleShoot</category>
      
    </categories>
    
    
    <tags>
      
      <tag>薅羊毛</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>水一篇</title>
    <link href="/2021/08/20/%E6%B0%B4%E4%B8%80%E7%AF%87/"/>
    <url>/2021/08/20/%E6%B0%B4%E4%B8%80%E7%AF%87/</url>
    
    <content type="html"><![CDATA[<h2 id="如题"><a href="#如题" class="headerlink" title="如题"></a>如题</h2><span id="more"></span><p>虽然参照这篇文章设置了，但还是站点地图显示无法获取，不知道是不是时间的问题，等等吧。</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><span class="hljs-comment">## 创建sitemap.xml</span><br>npm install hexo-generator-sitemap <span class="hljs-comment">--save</span><br></code></pre></td></tr></table></figure><iframe src="https://www.gongsunqi.xyz/2021/08/14/%E8%AE%A9%E8%87%AA%E5%B7%B1%E9%80%9A%E8%BF%87Hexo%E5%BB%BA%E7%AB%8B%E7%9A%84%E5%8D%9A%E5%AE%A2%E8%A2%AB%E8%B0%B7%E6%AD%8C%E6%90%9C%E7%B4%A2%E5%88%B0/" width="100%" height="500" name="topFrame" scrolling="yes"  noresize="noresize" frameborder="0" id="topFrame"></iframe>]]></content>
    
    
    <categories>
      
      <category>Working</category>
      
    </categories>
    
    
    <tags>
      
      <tag>划水摸鱼</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Trouble No Shoot</title>
    <link href="/2021/08/20/TroubleNoShoot/"/>
    <url>/2021/08/20/TroubleNoShoot/</url>
    
    <content type="html"><![CDATA[<h2 id="升级hexo遇到的问题"><a href="#升级hexo遇到的问题" class="headerlink" title="升级hexo遇到的问题"></a>升级hexo遇到的问题</h2><span id="more"></span><p>hexo的一个插件需要5.0以上的版本，看了下自己安装的是4.3的版本，所以想着升级一下。查了半天也没找到有效的方法。之后又查看了node的版本看着也很低，想升级的心又来了。折腾了半天愣是没升级成功，还把系统搞坏了，apt、dpkg这些也都没法用了。网上的资料有时候也不能盲目跟着做，还是用root账户删的，真是细思极恐，这要是公司的生产环境，这估计是要被祭天的。估计我也是仗着这是自己电脑里的子系统才敢这么胡作非为。系统坏了，本来想挽救一下的，发现越挽救问题越大。顺放弃。。。于是重新卸载Linux子系统，再重新安装，前后没花10分钟。果然还是微软baba的子系统安装卸载方便啊。<br>系统重新安装了，很多东西就要重新配置，比如github的免密提交等，这里也简单记录下。</p><h5 id="首先配置github及生成ssh秘钥，执行"><a href="#首先配置github及生成ssh秘钥，执行" class="headerlink" title="首先配置github及生成ssh秘钥，执行"></a>首先配置github及生成ssh秘钥，执行</h5><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs verilog">git <span class="hljs-keyword">config</span> --<span class="hljs-keyword">global</span> user<span class="hljs-variable">.email</span> <span class="hljs-string">&quot;you@example.com&quot;</span>    ## 我的 git <span class="hljs-keyword">config</span> --<span class="hljs-keyword">global</span> user<span class="hljs-variable">.email</span> <span class="hljs-string">&quot;jrwjb@sina.com&quot;</span>   <br>git <span class="hljs-keyword">config</span> --<span class="hljs-keyword">global</span> user<span class="hljs-variable">.name</span> <span class="hljs-string">&quot;Your Name&quot;</span>  ## 我的 git <span class="hljs-keyword">config</span> --<span class="hljs-keyword">global</span> user<span class="hljs-variable">.name</span> <span class="hljs-string">&quot;shubihu&quot;</span><br>ssh-keygen        ## 一路回车即可<br></code></pre></td></tr></table></figure><p>执行完后会在家目录的.ssh下生成下面几个文件</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clean">id_rsa   ## 私钥<br>id_rsa.pub  ## 共钥<br></code></pre></td></tr></table></figure><p>然后把公钥的内容添加到github上即可。<br><img src="/img/article/gitssh.jpg"></p><h5 id="回到最开始的问题，升级hexo、node。"><a href="#回到最开始的问题，升级hexo、node。" class="headerlink" title="回到最开始的问题，升级hexo、node。"></a>回到最开始的问题，升级hexo、node。</h5><p>因为是新系统，所以相对简单些，直接安装新版的node，可以从官网下载最新的稳定版进行安装，不过我嫌麻烦懒得去下载，所以参考了这篇文章进行安装。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment">## 使用nvm进行安装</span><br>curl -o- https:<span class="hljs-regexp">//</span>raw.githubusercontent.com<span class="hljs-regexp">/nvm-sh/</span>nvm<span class="hljs-regexp">/v0.35.3/i</span>nstall.sh | bash<br>nvm install node<br></code></pre></td></tr></table></figure><!-- <iframe src="https://www.myfreax.com/how-to-install-node-js-on-ubuntu-18-04/" width="100%" height="500" name="topFrame" scrolling="yes"  noresize="noresize" frameborder="0" id="topFrame"></iframe> --><h5 id="最后是升级hexo"><a href="#最后是升级hexo" class="headerlink" title="最后是升级hexo"></a>最后是升级hexo</h5><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs coffeescript"><span class="hljs-comment"># 使用淘宝源的 cnpm 替换 npm</span><br><span class="hljs-built_in">npm</span> install -g cnpm --registry=https://registry.<span class="hljs-built_in">npm</span>.taobao.org<br><br>cnpm install -g cnpm                 <span class="hljs-comment"># 升级 npm</span><br>cnpm cache clean -f                 <span class="hljs-comment"># 清除 npm 缓存</span><br><br>===更新 hexo: 进入 blog 目录，执行如下命令=== <br><span class="hljs-comment"># 更新 package.json 中的 hexo 及个插件版本</span><br>cnpm install -g <span class="hljs-built_in">npm</span>-check           <span class="hljs-comment"># 检查之前安装的插件，都有哪些是可以升级的 </span><br>cnpm install -g <span class="hljs-built_in">npm</span>-upgrade         <span class="hljs-comment"># 升级系统中的插件</span><br><span class="hljs-built_in">npm</span>-check<br><span class="hljs-built_in">npm</span>-upgrade<br><br><span class="hljs-comment"># 更新 hexo 及所有插件</span><br>cnpm update<br><br><span class="hljs-comment"># 确认 hexo 已经更新</span><br>hexo -v<br></code></pre></td></tr></table></figure><!-- <iframe src="https://xmuli.tech/posts/cb1e6c4f/" width="100%" height="500" name="topFrame" scrolling="yes"  noresize="noresize" frameborder="0" id="topFrame"></iframe> --><p>参考</p><ul><li><a href="https://xmuli.tech/posts/cb1e6c4f">https://xmuli.tech/posts/cb1e6c4f</a></li><li><a href="https://www.myfreax.com/how-to-install-node-js-on-ubuntu-18-04">https://www.myfreax.com/how-to-install-node-js-on-ubuntu-18-04</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>TroubleShoot</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TroubleShoot</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python Sort Algorithm</title>
    <link href="/2021/08/16/PythonSortAlgorithm/"/>
    <url>/2021/08/16/PythonSortAlgorithm/</url>
    
    <content type="html"><![CDATA[<h2 id="Python常用排序算法"><a href="#Python常用排序算法" class="headerlink" title="Python常用排序算法"></a>Python常用排序算法</h2><span id="more"></span><h5 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sql">def quick_sort(<span class="hljs-keyword">array</span>):<br>    if len(<span class="hljs-keyword">array</span>) <span class="hljs-operator">&lt;=</span> <span class="hljs-number">1</span>:  # 递归跳出条件<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">array</span><br>    pivot <span class="hljs-operator">=</span> <span class="hljs-keyword">array</span>[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">left</span> <span class="hljs-operator">=</span> [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-keyword">array</span>[<span class="hljs-number">1</span>:] if i <span class="hljs-operator">&lt;</span> pivot]<br>    <span class="hljs-keyword">right</span> <span class="hljs-operator">=</span> [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-keyword">array</span>[<span class="hljs-number">1</span>:] if i <span class="hljs-operator">&gt;=</span> pivot]<br>    <span class="hljs-keyword">return</span> quick_sort(<span class="hljs-keyword">left</span>) <span class="hljs-operator">+</span> [pivot] <span class="hljs-operator">+</span> quick_sort(<span class="hljs-keyword">right</span>)<br></code></pre></td></tr></table></figure><h5 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h5><figure class="highlight sas"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sas">def bubble_sort(<span class="hljs-meta">array</span>):<br>    for i <span class="hljs-meta">in</span><span class="hljs-meta"> range(</span>l<span class="hljs-meta">en(</span><span class="hljs-meta">array</span>) - 1):<br>        for j <span class="hljs-meta">in</span><span class="hljs-meta"> range(</span>l<span class="hljs-meta">en(</span><span class="hljs-meta">array</span>) - i -1): # 已排序好的部分不需再遍历<br>            <span class="hljs-meta">if</span> <span class="hljs-meta">array</span>[j] &gt; <span class="hljs-meta">array</span>[j+1]:<br>                <span class="hljs-meta">array</span>[j], <span class="hljs-meta">array</span>[j+1] = <span class="hljs-meta">array</span>[j+1], <span class="hljs-meta">array</span>[j]<br>    <span class="hljs-meta">return</span> <span class="hljs-meta">array</span><br></code></pre></td></tr></table></figure><h5 id="桶排"><a href="#桶排" class="headerlink" title="桶排"></a>桶排</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs sql">def bucker_sort(<span class="hljs-keyword">array</span>):<br>    <span class="hljs-keyword">result</span> <span class="hljs-operator">=</span> []<br>    minVal, maxVal <span class="hljs-operator">=</span> <span class="hljs-built_in">min</span>(<span class="hljs-keyword">array</span>), <span class="hljs-built_in">max</span>(<span class="hljs-keyword">array</span>)<br>    bucket <span class="hljs-operator">=</span> [<span class="hljs-number">0</span>] <span class="hljs-operator">*</span> (maxVal <span class="hljs-operator">-</span> minVal <span class="hljs-operator">+</span> <span class="hljs-number">1</span>)  # 所需的桶数<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-keyword">array</span>:<br>        bucket[i <span class="hljs-operator">-</span> minVal] <span class="hljs-operator">+</span><span class="hljs-operator">=</span> <span class="hljs-number">1</span>     # 每个数字出现的次数<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-keyword">range</span>(len(bucket)):<br>        if bucket[i]:<br>            <span class="hljs-keyword">result</span> <span class="hljs-operator">+</span><span class="hljs-operator">=</span> [i <span class="hljs-operator">+</span> minVal] <span class="hljs-operator">*</span> bucket[i]<br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">result</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux One Line Command</title>
    <link href="/2021/08/13/OneLineCommand/"/>
    <url>/2021/08/13/OneLineCommand/</url>
    
    <content type="html"><![CDATA[<h2 id="Linux-常用命令"><a href="#Linux-常用命令" class="headerlink" title="Linux 常用命令"></a>Linux 常用命令</h2><span id="more"></span><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs clean">sh -n ##判断是否有语法错误<br>sh -x ##执行详细过程<br>## 修改目录颜色<br>dircolors -p &gt; ~/.dircolors   ## 编辑 ~/.dircolors 修改<br>## 忽略大小写<br>echo <span class="hljs-string">&#x27;set completion-ignore-case on&#x27;</span> &gt; ~/.inputrc<br></code></pre></td></tr></table></figure><h5 id="Linux-两个文件求交集、并集、差集"><a href="#Linux-两个文件求交集、并集、差集" class="headerlink" title="Linux 两个文件求交集、并集、差集"></a>Linux 两个文件求交集、并集、差集</h5><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-built_in">sort</span> <span class="hljs-keyword">a</span>.txt b.txt | uniq -d   <span class="hljs-comment">### 交集</span><br><span class="hljs-built_in">sort</span> <span class="hljs-keyword">a</span>.txt b.txt | uniq   <span class="hljs-comment">###并集 </span><br><span class="hljs-built_in">sort</span> <span class="hljs-keyword">a</span>.txt b.txt b.txt | uniq -u  <span class="hljs-comment">## 差集 a-b</span><br><span class="hljs-built_in">sort</span> b.txt <span class="hljs-keyword">a</span>.txt <span class="hljs-keyword">a</span>.txt | uniq -u  <span class="hljs-comment">## 差集 b-a</span><br></code></pre></td></tr></table></figure><p>使用sort可以将文件进行排序，可以使用sort后面的玲玲，例如 -n 按照数字格式排序，例如 -i 忽略大小写，例如使用-r 为逆序输出等<br>uniq为删除文件中重复的行，得到文件中唯一的行，后面的命令 -d 表示的是输出出现次数大于1的内容 -u表示的是输出出现次数为1的内容，那么对于上述的求交集并集差集的命令做如下的解释：</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-selector-tag">sort</span> <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.txt</span> <span class="hljs-selector-tag">b</span><span class="hljs-selector-class">.txt</span> | <span class="hljs-selector-tag">uniq</span> <span class="hljs-selector-tag">-d</span> #将<span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.txt</span> <span class="hljs-selector-tag">b</span><span class="hljs-selector-class">.txt</span>文件进行排序，<span class="hljs-selector-tag">uniq</span>使得两个文件中的内容为唯一的，使用<span class="hljs-selector-tag">-d</span>输出两个文件中次数大于<span class="hljs-selector-tag">1</span>的内容，即是得到交集<br><span class="hljs-selector-tag">sort</span> <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.txt</span> <span class="hljs-selector-tag">b</span><span class="hljs-selector-class">.txt</span> | <span class="hljs-selector-tag">uniq</span>  #将<span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.txt</span> <span class="hljs-selector-tag">b</span><span class="hljs-selector-class">.txt</span>文件进行排序，<span class="hljs-selector-tag">uniq</span>使得两个文件中的内容为唯一的，即可得到两个文件的并集<br><span class="hljs-selector-tag">sort</span> <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.txt</span> <span class="hljs-selector-tag">b</span><span class="hljs-selector-class">.txt</span> <span class="hljs-selector-tag">b</span><span class="hljs-selector-class">.txt</span> | <span class="hljs-selector-tag">uniq</span> <span class="hljs-selector-tag">-u</span> #将两个文件排序，最后输出<span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.txt</span> <span class="hljs-selector-tag">b</span><span class="hljs-selector-class">.txt</span> <span class="hljs-selector-tag">b</span><span class="hljs-selector-class">.txt</span>文件中只出现过一次的内容，因为有两个<span class="hljs-selector-tag">b</span><span class="hljs-selector-class">.txt</span>所以只会输出只在<span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.txt</span>出现过一次的内容，即是<span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.txt-b</span><span class="hljs-selector-class">.txt</span>差集<br>#对于<span class="hljs-selector-tag">b</span><span class="hljs-selector-class">.txt-a</span><span class="hljs-selector-class">.txt</span>为同理<br></code></pre></td></tr></table></figure><h5 id="grep-命令是常用的搜索文本内容的，要找交集，如下即可："><a href="#grep-命令是常用的搜索文本内容的，要找交集，如下即可：" class="headerlink" title="grep 命令是常用的搜索文本内容的，要找交集，如下即可："></a>grep 命令是常用的搜索文本内容的，要找交集，如下即可：</h5><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">grep <span class="hljs-operator">-F</span> <span class="hljs-operator">-f</span> a.txt b.txt | <span class="hljs-built_in">sort</span> | uniq<br></code></pre></td></tr></table></figure><h5 id="差集"><a href="#差集" class="headerlink" title="差集:"></a>差集:</h5><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">grep -F -v -f <span class="hljs-keyword">a</span>.txt b.txt | <span class="hljs-built_in">sort</span> | uniq<br>grep -F -v -f b.txt <span class="hljs-keyword">a</span>.txt | <span class="hljs-built_in">sort</span> | uniq<br><span class="hljs-comment">#第一行结果为b-a；第二行为a-b。注意顺序很重要</span><br></code></pre></td></tr></table></figure><h5 id="根据id提取fastq"><a href="#根据id提取fastq" class="headerlink" title="根据id提取fastq"></a>根据id提取fastq</h5><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clean">grep -f id -A <span class="hljs-number">3</span> BC01.fq &gt; test.fq   ### -f 参数为ID文件<br></code></pre></td></tr></table></figure><h5 id="批量重命名文件"><a href="#批量重命名文件" class="headerlink" title="批量重命名文件"></a>批量重命名文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#只更改户后缀</span><br>rename <span class="hljs-string">&#x27;s/.txt/.log/&#x27;</span> *.txt   <span class="hljs-comment">#### 把txt后缀改为log</span><br><span class="hljs-comment">#小写变大写</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> `ls`;<span class="hljs-keyword">do</span> mv -f <span class="hljs-variable">$i</span> `<span class="hljs-built_in">echo</span> <span class="hljs-variable">$i</span> | tr a-z A-Z`;<span class="hljs-keyword">done</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> `ls`;<span class="hljs-keyword">do</span> mv -f <span class="hljs-variable">$i</span> `<span class="hljs-built_in">echo</span> <span class="hljs-variable">$i</span> | sed <span class="hljs-string">&#x27;s/..../..../&#x27;</span>`;<span class="hljs-keyword">done</span>  <span class="hljs-comment">##使用sed替换q</span><br>rename <span class="hljs-string">&#x27;s/small/large/&#x27;</span> image_*.png<br></code></pre></td></tr></table></figure><h5 id="删除空行"><a href="#删除空行" class="headerlink" title="删除空行"></a>删除空行</h5><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs vim">sed -i <span class="hljs-string">&#x27;/^$/d&#x27;</span> <span class="hljs-keyword">file</span><br><span class="hljs-keyword">grep</span> -v <span class="hljs-string">&#x27;^$&#x27;</span> <span class="hljs-keyword">file</span>   或  sed <span class="hljs-string">&#x27;/^$/d&#x27;</span> <span class="hljs-keyword">file</span> 或 sed -n <span class="hljs-string">&#x27;/./p&#x27;</span> <span class="hljs-keyword">file</span><br>awk <span class="hljs-string">&#x27;/./&#123;print&#125;&#x27;</span> <span class="hljs-keyword">file</span> 或  <span class="hljs-keyword">tr</span> -s <span class="hljs-string">&#x27;n&#x27;</span><br>#删除最后一列<br>sed -r -<span class="hljs-keyword">e</span> <span class="hljs-string">&#x27;s/\t[^\t]*$//g&#x27;</span> <span class="hljs-keyword">file</span>   <br></code></pre></td></tr></table></figure><h5 id="统计文件大小"><a href="#统计文件大小" class="headerlink" title="统计文件大小"></a>统计文件大小</h5><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim">du -<span class="hljs-keyword">sh</span> * 或者 du -h --<span class="hljs-built_in">max</span>-depth=<span class="hljs-number">1</span>  或 du -<span class="hljs-keyword">sh</span> * | <span class="hljs-keyword">grep</span> [GM] | <span class="hljs-keyword">sort</span> 提取G 和 M的文件并排序<br></code></pre></td></tr></table></figure><h5 id="计算reads数"><a href="#计算reads数" class="headerlink" title="计算reads数"></a>计算reads数</h5><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">expr <span class="hljs-constructor">$(<span class="hljs-params">wc</span> -<span class="hljs-params">l</span> &lt; <span class="hljs-operator">*</span>.<span class="hljs-params">fastq</span>)</span><span class="hljs-operator"> / </span><span class="hljs-number">4</span><br>expr <span class="hljs-constructor">$(<span class="hljs-params">zcat</span> <span class="hljs-params">test</span><span class="hljs-operator">/</span>1.R1.<span class="hljs-params">fq</span>.<span class="hljs-params">gz</span> | <span class="hljs-params">wc</span> -<span class="hljs-params">l</span>)</span><span class="hljs-operator"> / </span><span class="hljs-number">4</span><br></code></pre></td></tr></table></figure><h5 id="fastq-转-fasta"><a href="#fastq-转-fasta" class="headerlink" title="fastq 转 fasta"></a>fastq 转 fasta</h5><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">awk</span> &#x27;&#123;if(NR%<span class="hljs-number">4</span> == <span class="hljs-number">1</span>)&#123;print <span class="hljs-string">&quot;&gt;&quot;</span> substr($<span class="hljs-number">0</span>, <span class="hljs-number">2</span>)&#125;&#125;&#123;if(NR%<span class="hljs-number">4</span> == <span class="hljs-number">2</span>)&#123;print&#125;&#125;&#x27; xx.fastq &gt;xx.fasta<br><span class="hljs-attribute">awk</span> &#x27;&#123;if(NR%<span class="hljs-number">4</span> == <span class="hljs-number">1</span>)&#123;print <span class="hljs-string">&quot;&gt;&quot;</span> <span class="hljs-string">&quot;&#x27;$j&#x27;&quot;</span><span class="hljs-string">&quot;_&quot;</span>NR&#125;&#125;&#123;if(NR%<span class="hljs-number">4</span> == <span class="hljs-number">2</span>)&#123;print&#125;&#125;&#x27;    #   <span class="hljs-string">&quot;&#x27;$j&#x27;&quot;</span> awk中引用外部变量<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">sort</span> -k<span class="hljs-number">1</span>,<span class="hljs-number">1</span>V -k<span class="hljs-number">2</span>,<span class="hljs-number">2</span>n file   ## V 参数忽略第一列中的文本按数字排序<br><span class="hljs-attribute">awk</span> &#x27;$<span class="hljs-number">1</span> ~ /chr<span class="hljs-number">1</span>|chr<span class="hljs-number">3</span>/&#x27; file ## 第一列匹配chr<span class="hljs-number">1</span>或chr<span class="hljs-number">3</span><br><span class="hljs-attribute">awk</span> &#x27;NR &gt; <span class="hljs-number">3</span>&#x27; file ## 取出第四行以后<br><span class="hljs-attribute">sed</span> -n &#x27;<span class="hljs-number">20</span>,<span class="hljs-number">50</span>p&#x27; file # 取出<span class="hljs-number">20</span>到<span class="hljs-number">50</span>行<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">paste</span> file<span class="hljs-number">1</span> file<span class="hljs-number">2</span>  # 横向拼接文件，拼接前可用dos<span class="hljs-number">2</span>unix转换文件类型<br></code></pre></td></tr></table></figure><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs clean"><span class="hljs-string">&#x27;%&#x27;</span> 从后向前删除, <span class="hljs-string">&#x27;#&#x27;</span> 从前向后删除<br>sed 替换每行最后一个匹配的字符<br>sed <span class="hljs-string">&#x27;s/\(.*\)src_str\(.*\)/\1dst_str\2/&#x27;</span>  yourfile   ##  src_str：要匹配的字符  dst_str: 要替换的字符<br></code></pre></td></tr></table></figure><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">biom</span>=<span class="hljs-variable">$&#123;i##*/&#125;</span>    <span class="hljs-comment">#返回 / 后的字符</span><br><span class="hljs-attr">biom</span>=<span class="hljs-variable">$&#123;i%/*&#125;</span>     <span class="hljs-comment">#返回最后 / 前的字符</span><br></code></pre></td></tr></table></figure><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs avrasm">ls -<span class="hljs-keyword">ld</span> 列出文件全路径<br></code></pre></td></tr></table></figure><h5 id="使用-wget-完成批量下载"><a href="#使用-wget-完成批量下载" class="headerlink" title="使用 wget 完成批量下载"></a>使用 wget 完成批量下载</h5><p>如果想下载一个网站上目录中的所有文件, 我需要执行一长串wget命令, 但这样做会更好:</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">wget -nd -r -l1 --no-parent http:<span class="hljs-regexp">//</span>www.foo.com<span class="hljs-regexp">/mp3/</span><br></code></pre></td></tr></table></figure><p>这条命令可以执行的很好, 但有时会下载像 index.@xx 这样一些我不想要的文件. 如果你知道想要文件的格式, 可以用下面的命令来避免下载那些多余的文件:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">wget -nd -r -l1 --no-parent -<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">A</span>.</span></span>mp3 -<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">A</span>.</span></span>wma http:<span class="hljs-comment">//www.foo.com/mp3/</span><br></code></pre></td></tr></table></figure><p>我来简单的介绍一下命令中指定选项的作用.<br>-nd 不创建目录, wget默认会创建一个目录<br>-r 递归下载<br>-l1 (L one) 递归一层,只下载指定文件夹中的内容, 不下载下一级目录中的.<br>–no-parent 不下载父目录中的文件</p><h5 id="rsync可视化复制文件时的进度"><a href="#rsync可视化复制文件时的进度" class="headerlink" title="rsync可视化复制文件时的进度"></a>rsync可视化复制文件时的进度</h5><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">rsync</span> -avPh 源文件 目标文件<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>知网羊毛</title>
    <link href="/2021/08/12/%E7%9F%A5%E7%BD%91%E7%BE%8A%E6%AF%9B/"/>
    <url>/2021/08/12/%E7%9F%A5%E7%BD%91%E7%BE%8A%E6%AF%9B/</url>
    
    <content type="html"><![CDATA[<h2 id="薅社会主义羊毛-知网"><a href="#薅社会主义羊毛-知网" class="headerlink" title="薅社会主义羊毛-知网"></a>薅社会主义羊毛-知网</h2><span id="more"></span><p>工作中经常会受到同事、朋友的求助帮忙下载论文，中文的、外文的都遇到过。外文文献一般都在Sci_hub(<a href="https://tool.yovisun.com/scihub/">https://tool.yovisun.com/scihub/</a>) 上查找，当然也会有些找不到。中文的文献莫过于知网了，但是知网也是收费模式的。于是在网上挖了下，找到了两三个相对靠谱的可以薅知网羊毛的方法，但是也是有些限制的。</p><h5 id="iData-https-www-cn-ki-net"><a href="#iData-https-www-cn-ki-net" class="headerlink" title="iData(https://www.cn-ki.net/)"></a>iData(<a href="https://www.cn-ki.net/">https://www.cn-ki.net/</a>)</h5><p>直接注册便可使用，缺点就是每天只能下载几篇吧</p><h5 id="80图书馆-官网-http-www-80lib-com-知网-http-www-80lib-com-cnki"><a href="#80图书馆-官网-http-www-80lib-com-知网-http-www-80lib-com-cnki" class="headerlink" title="80图书馆(官网:http://www.80lib.com/ 知网:http://www.80lib.com/cnki/)"></a>80图书馆(官网:<a href="http://www.80lib.com/">http://www.80lib.com/</a> 知网:<a href="http://www.80lib.com/cnki/">http://www.80lib.com/cnki/</a>)</h5><p>优点是无限篇下载，缺点就是只有三天试用，不过应该可以换个邮箱再注册。还有个缺点就是使用相对麻烦，需要使用谷歌浏览器以及对应的插件，不过好在官网都提供了详细的步骤，这里不再赘述。</p><h5 id="科研通-www-ablesci-com"><a href="#科研通-www-ablesci-com" class="headerlink" title="科研通(www.ablesci.com)"></a>科研通(<a href="http://www.ablesci.com/">www.ablesci.com</a>)</h5><p>类似于百度文献互助</p><h5 id="百度文献互助-备用"><a href="#百度文献互助-备用" class="headerlink" title="百度文献互助(备用)"></a>百度文献互助(备用)</h5><p>缺点每天两篇，时间长，还不一定成功。</p>]]></content>
    
    
    <categories>
      
      <category>TroubleShoot</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TroubleShoot</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python 异步</title>
    <link href="/2021/08/11/Python_%E5%BC%82%E6%AD%A5/"/>
    <url>/2021/08/11/Python_%E5%BC%82%E6%AD%A5/</url>
    
    <content type="html"><![CDATA[<h2 id="Python中异步、同步、多进程及多线程的比较"><a href="#Python中异步、同步、多进程及多线程的比较" class="headerlink" title="Python中异步、同步、多进程及多线程的比较"></a>Python中异步、同步、多进程及多线程的比较</h2><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> urllib <span class="hljs-keyword">import</span> request<br><span class="hljs-keyword">from</span> urllib <span class="hljs-keyword">import</span> parse<br><span class="hljs-keyword">from</span> urllib.request <span class="hljs-keyword">import</span> urlopen<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-comment"># 用于多进程</span><br><span class="hljs-keyword">from</span> multiprocessing <span class="hljs-keyword">import</span> Process<br><span class="hljs-comment"># 用于多线程</span><br><span class="hljs-keyword">from</span> threading <span class="hljs-keyword">import</span> Thread<br><span class="hljs-comment"># 用于协程+异步</span><br><span class="hljs-keyword">import</span> aiohttp<br><span class="hljs-keyword">import</span> asyncio<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">aiohttp:异步发送POST请求</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">async</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">city_rule_asy</span>():</span><br>    data = &#123;<span class="hljs-string">&quot;key&quot;</span>: <span class="hljs-string">&quot;&quot;</span>&#125;<br>    myPostUrl = <span class="hljs-string">&quot;http://api.chinadatapay.com/government/traffic/2299&quot;</span><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> aiohttp.ClientSession() <span class="hljs-keyword">as</span> session:<br>        <span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> session.post(myPostUrl, data=data) <span class="hljs-keyword">as</span> res:<br>            <span class="hljs-comment"># print(res.status)</span><br>            <span class="hljs-keyword">return</span> json.loads(<span class="hljs-keyword">await</span> res.text())<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span>():</span><br>    tasks = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>        task = asyncio.ensure_future(city_rule_asy())<br>        tasks.append(task)<br>    loop = asyncio.get_event_loop()<br>    result = loop.run_until_complete(asyncio.gather(*tasks))<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;city_rules.txt&#x27;</span>, <span class="hljs-string">&#x27;a+&#x27;</span>) <span class="hljs-keyword">as</span> fw:<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> response[<span class="hljs-string">&#x27;data&#x27;</span>]:<br>           <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> i[<span class="hljs-string">&#x27;cities&#x27;</span>]:<br>                fw.write(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;j[<span class="hljs-string">&#x27;city&#x27;</span>]&#125;</span>\t<span class="hljs-subst">&#123;j[<span class="hljs-string">&#x27;engine&#x27;</span>]&#125;</span>\t<span class="hljs-subst">&#123;j[<span class="hljs-string">&#x27;prefix&#x27;</span>]&#125;</span>\t<span class="hljs-subst">&#123;j[<span class="hljs-string">&#x27;vin&#x27;</span>]&#125;</span>\t<span class="hljs-subst">&#123;j[<span class="hljs-string">&#x27;model&#x27;</span>]&#125;</span>\n&quot;</span>)<br><span class="hljs-comment">#### ============================================ ###</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">city_rule</span>():</span><br>    myPostUrl = <span class="hljs-string">&quot;http://api.chinadatapay.com/government/traffic/2299&quot;</span><br>    data = &#123;<span class="hljs-string">&quot;key&quot;</span>: <span class="hljs-string">&quot;&quot;</span>&#125;<br>    params = parse.urlencode(data).encode(<span class="hljs-string">&#x27;utf-8&#x27;</span>)  <span class="hljs-comment"># 提交类型不能为str，需要为byte类型</span><br>    req = request.Request(myPostUrl, params)<br>    response = json.loads(urlopen(req).read().decode())<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;city_rules.txt&#x27;</span>, <span class="hljs-string">&#x27;a+&#x27;</span>) <span class="hljs-keyword">as</span> fw:<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> response[<span class="hljs-string">&#x27;data&#x27;</span>]:<br>           <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> i[<span class="hljs-string">&#x27;cities&#x27;</span>]:<br>                fw.write(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;j[<span class="hljs-string">&#x27;city&#x27;</span>]&#125;</span>\t<span class="hljs-subst">&#123;j[<span class="hljs-string">&#x27;engine&#x27;</span>]&#125;</span>\t<span class="hljs-subst">&#123;j[<span class="hljs-string">&#x27;prefix&#x27;</span>]&#125;</span>\t<span class="hljs-subst">&#123;j[<span class="hljs-string">&#x27;vin&#x27;</span>]&#125;</span>\t<span class="hljs-subst">&#123;j[<span class="hljs-string">&#x27;model&#x27;</span>]&#125;</span>\n&quot;</span>)<br><span class="hljs-comment">## 单进程单线程同步</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">single_process</span>():</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>        city_rule()<br><span class="hljs-comment"># 多进程并行</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mul_process</span>():</span><br>    processes = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>        p = Process(target=city_rule, args=())     <span class="hljs-comment"># 一个参数 args=(prameter,)</span><br>        processes.append(p)<br>        p.start()<br>    <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> processes:<br>        p.join()<br><span class="hljs-comment"># 多线程并发</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mul_thead</span>():</span><br>    threads = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>        t = Thread(target=city_rule, args=())<br>        threads.append(t)<br>        t.start()<br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> threads:<br>        t.join()<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># 异步</span><br>    run()<br>    <span class="hljs-comment"># 同步</span><br>    single_process()<br>    <span class="hljs-comment"># 多进程</span><br>    mul_process()<br>    <span class="hljs-comment">#多线程</span><br>    mul_thead()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python Notes</title>
    <link href="/2021/02/02/Python-Notes/"/>
    <url>/2021/02/02/Python-Notes/</url>
    
    <content type="html"><![CDATA[<h2 id="Python学习随笔"><a href="#Python学习随笔" class="headerlink" title="Python学习随笔"></a>Python学习随笔</h2><span id="more"></span><h3 id="字典转dataframe"><a href="#字典转dataframe" class="headerlink" title="字典转dataframe"></a>字典转dataframe</h3><p>不定义列名时：pd.DataFrame.from_dict(data, orient=’index’)<br>定义列名时：pd.DataFrame.from_dict(data, orient=’index’, columns=[‘A’, ‘B’, ‘C’, ‘D’])</p><h3 id="pandas筛选"><a href="#pandas筛选" class="headerlink" title="pandas筛选"></a>pandas筛选</h3><h4 id="选取某列值等于某些值的行用-，不等于用-！-，data-loc-data-‘a’-‘one’"><a href="#选取某列值等于某些值的行用-，不等于用-！-，data-loc-data-‘a’-‘one’" class="headerlink" title="选取某列值等于某些值的行用 == ，不等于用 ！= ，data.loc[data[‘a’] == ‘one’]"></a>选取某列值等于某些值的行用 == ，不等于用 ！= ，data.loc[data[‘a’] == ‘one’]</h4><h4 id="选取某列值是否是某一类型的数值用-isin-取反用"><a href="#选取某列值是否是某一类型的数值用-isin-取反用" class="headerlink" title="选取某列值是否是某一类型的数值用 isin ,取反用 ~"></a>选取某列值是否是某一类型的数值用 isin ,取反用 ~</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs language">data.loc[data[&#x27;a&#x27;].isin([&#x27;one&#x27;, &#x27;two&#x27;])]<br>data.loc[~data[&#x27;a&#x27;].isin([&#x27;one&#x27;, &#x27;two&#x27;])]<br></code></pre></td></tr></table></figure><h4 id="多种条件的选取用-amp-data-loc-data-‘a’-‘one’-amp-data-‘b’-‘two’"><a href="#多种条件的选取用-amp-data-loc-data-‘a’-‘one’-amp-data-‘b’-‘two’" class="headerlink" title="多种条件的选取用 &amp; , data.loc[(data[‘a’] == ‘one’) &amp; (data[‘b’] == ‘two’)]"></a>多种条件的选取用 &amp; , data.loc[(data[‘a’] == ‘one’) &amp; (data[‘b’] == ‘two’)]</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs language">np.linspace(start, stop, num) ##参数为起点，终点，点数，num默认为50<br></code></pre></td></tr></table></figure><h3 id="把某列值设为index，df-set-index-‘columns’-df-reset-index-重置索引"><a href="#把某列值设为index，df-set-index-‘columns’-df-reset-index-重置索引" class="headerlink" title="把某列值设为index，df.set_index(‘columns’)  (df.reset_index()重置索引)"></a>把某列值设为index，df.set_index(‘columns’)  (df.reset_index()重置索引)</h3><p>df中merge函数按 键 合并，concat函数按 轴 合并</p><h3 id="按键-key-合并可以分「单键合并」和「多键合并」"><a href="#按键-key-合并可以分「单键合并」和「多键合并」" class="headerlink" title="按键 (key) 合并可以分「单键合并」和「多键合并」"></a>按键 (key) 合并可以分「单键合并」和「多键合并」</h3><h3 id="单键合并："><a href="#单键合并：" class="headerlink" title="单键合并："></a>单键合并：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs language">pd.merge(df1, df2, how=s, on=c ) ##c 是 df1 和 df2 共有的一栏，合并方式 (how=s) 有四种：<br></code></pre></td></tr></table></figure><h6 id="左连接-left-：合并之后显示-df1-的所有行"><a href="#左连接-left-：合并之后显示-df1-的所有行" class="headerlink" title="左连接 (left)：合并之后显示 df1 的所有行"></a>左连接 (left)：合并之后显示 df1 的所有行</h6><h6 id="右连接-right-：合并之后显示-df2-的所有行"><a href="#右连接-right-：合并之后显示-df2-的所有行" class="headerlink" title="右连接 (right)：合并之后显示 df2 的所有行"></a>右连接 (right)：合并之后显示 df2 的所有行</h6><h6 id="外连接-outer-：合并-df1-和-df2-共有的所有行"><a href="#外连接-outer-：合并-df1-和-df2-共有的所有行" class="headerlink" title="外连接 (outer)：合并 df1 和 df2 共有的所有行"></a>外连接 (outer)：合并 df1 和 df2 共有的所有行</h6><h6 id="内连接-inner-：只保留两个表中公共部分的信息-默认情况"><a href="#内连接-inner-：只保留两个表中公共部分的信息-默认情况" class="headerlink" title="内连接 (inner)：只保留两个表中公共部分的信息 (默认情况)"></a>内连接 (inner)：只保留两个表中公共部分的信息 (默认情况)</h6><h6 id="多键合并-俩组数据均有该列"><a href="#多键合并-俩组数据均有该列" class="headerlink" title="多键合并(俩组数据均有该列)"></a>多键合并(俩组数据均有该列)</h6><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs language">pd.merge( df1, df2, how=s, on=c )  ## c 是多栏（如一个列表<br></code></pre></td></tr></table></figure><h3 id="多键合并-两组数据不同的列名）"><a href="#多键合并-两组数据不同的列名）" class="headerlink" title="多键合并(两组数据不同的列名）"></a>多键合并(两组数据不同的列名）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs language">pd.merge(df1, df2, left_on = &#x27;key1&#x27;, right_on = &#x27;key2&#x27;)<br></code></pre></td></tr></table></figure><h3 id="插入列：除在最右侧插入用标签直接创建外，其他列用-insert-方法进行插入，比如table-insert-0-’date’-date"><a href="#插入列：除在最右侧插入用标签直接创建外，其他列用-insert-方法进行插入，比如table-insert-0-’date’-date" class="headerlink" title="插入列：除在最右侧插入用标签直接创建外，其他列用.insert()方法进行插入，比如table.insert(0,’date’,date)"></a>插入列：除在最右侧插入用标签直接创建外，其他列用.insert()方法进行插入，比如table.insert(0,’date’,date)</h3><p>当 df1 和 df2 有两个相同的列 (Asset 和 Instrument) 时，单单只对一列 (Asset) 做合并产出的 DataFrame 会有另一列 (Instrument) 重复的名称。<br>这时 merge 函数给重复的名称加个后缀 _x, _y，也可以设定 suffixes 来改后缀</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs language">pd.concat([df1,df2], axis=0, ignore_index=True)  # 默认axis=0（行连接）<br></code></pre></td></tr></table></figure><h3 id="列索引-→-行索引，用-stack-函数-行索引-→-列索引，用-unstack-函数"><a href="#列索引-→-行索引，用-stack-函数-行索引-→-列索引，用-unstack-函数" class="headerlink" title="列索引 → 行索引，用 stack 函数;行索引 → 列索引，用 unstack 函数"></a>列索引 → 行索引，用 stack 函数;行索引 → 列索引，用 unstack 函数</h3><h3 id="数据透视："><a href="#数据透视：" class="headerlink" title="数据透视："></a>数据透视：</h3><p>用 pivot 函数将「一张长表」变「多张宽表」，<br>用 melt 函数将「多张宽表」变「一张长表」  # 函数 melt 实际是将「源表」转化成 id-variable 类型的 DataFrame</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">data_pivot = data.pivot(index=<span class="hljs-string">&#x27;Date&#x27;</span>,columns=<span class="hljs-string">&#x27;Symbol&#x27;</span>,values=<span class="hljs-string">&#x27;Adj Close&#x27;</span>) <span class="hljs-comment">#若不设置value参数，剩下的列都用来透视</span><br>melted_data = pd.melt(data, id_vars=[<span class="hljs-string">&#x27;Date&#x27;</span>,<span class="hljs-string">&#x27;Symbol&#x27;</span>])<br><span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">set</span>(<span class="hljs-built_in">list</span>), key=<span class="hljs-built_in">list</span>.index)  <span class="hljs-comment">## 消除重复元素不改变原始数据顺序</span><br><span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">dict</span>.items(),key=<span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">1</span>],reverse=<span class="hljs-literal">True</span>)  <span class="hljs-comment">## 对字典按值反向排序（x[0]按键排序）</span><br></code></pre></td></tr></table></figure><h3 id="pandas-删除列"><a href="#pandas-删除列" class="headerlink" title="pandas 删除列"></a>pandas 删除列</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">df = df.drop([<span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>], <span class="hljs-attribute">axis</span>=1)<br><span class="hljs-comment">#或者</span><br>df.drop([<span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>], <span class="hljs-attribute">axis</span>=1, <span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h3 id="对行-z-score-标准化"><a href="#对行-z-score-标准化" class="headerlink" title="对行 z-score 标准化"></a>对行 z-score 标准化</h3><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs maxima">df.<span class="hljs-built_in">apply</span>(<span class="hljs-built_in">lambda</span> x: (x - <span class="hljs-built_in">np</span>.<span class="hljs-built_in">mean</span>(x)) / (<span class="hljs-built_in">np</span>.<span class="hljs-built_in">std</span>(x,ddof=<span class="hljs-number">1</span>)), axis=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><h3 id="对-Majority-protein-IDs-列转成多行"><a href="#对-Majority-protein-IDs-列转成多行" class="headerlink" title="对 Majority protein IDs 列转成多行"></a>对 Majority protein IDs 列转成多行</h3><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">df = df[~df[<span class="hljs-string">&#x27;Majority protein IDs&#x27;</span>].str.contains(<span class="hljs-string">&#x27;CON|REV&#x27;</span>, regex=<span class="hljs-keyword">True</span>)]<br>df = df.<span class="hljs-keyword">drop</span>(<span class="hljs-string">&#x27;Majority protein IDs&#x27;</span>, axis=<span class="hljs-number">1</span>).<span class="hljs-keyword">join</span>(df[<span class="hljs-string">&#x27;Majority protein IDs&#x27;</span>].str.split(<span class="hljs-string">&#x27;;&#x27;</span>, expand=<span class="hljs-keyword">True</span>).stack().reset_index(<span class="hljs-keyword">level</span>=<span class="hljs-number">1</span>, <span class="hljs-keyword">drop</span>=<span class="hljs-keyword">True</span>).<span class="hljs-keyword">rename</span>(<span class="hljs-string">&#x27;Majority protein IDs&#x27;</span>))<br><br>def ab(df): <br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;;&#x27;</span>.<span class="hljs-keyword">join</span>(df.<span class="hljs-keyword">values</span>)<br>newcolumns = df_merge.<span class="hljs-keyword">columns</span>.tolist()<br>newcolumns.remove(<span class="hljs-string">&#x27;Majority protein IDs&#x27;</span>)<br>newdf = df_merge.groupby(newcolumns)[<span class="hljs-string">&#x27;Majority protein IDs&#x27;</span>].apply(ab)   ## 多行合并一行<br></code></pre></td></tr></table></figure><h3 id="for、while循环中的else扩展用法"><a href="#for、while循环中的else扩展用法" class="headerlink" title="for、while循环中的else扩展用法"></a>for、while循环中的else扩展用法</h3><p>else中的程序只在一种条件下执行，即循环正常遍历所有内容或者由于条件不成立而结束循环，没有因break或者return而退出循环。continue对else没影响</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs language">for i in range(10):<br>    if i==5:<br>        break<br>    print(&#x27;i=&#x27;,i,end=&#x27;,&#x27;)<br>else:<br>    print(&#x27;success&#x27;)#不输出   在for循环中含有break时则直接终止循环，并不会执行else子句。<br><br>for i in range(10):<br>    if i==5:<br>        continue<br>    print(&#x27;i=&#x27;,i,end=&#x27;,&#x27;)<br>else:<br>    print(&#x27;success&#x27;)#输出<br></code></pre></td></tr></table></figure><h3 id="展平嵌套列表"><a href="#展平嵌套列表" class="headerlink" title="展平嵌套列表"></a>展平嵌套列表</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs language">newlist = [item for items in newlist for item in items]<br>#或者您可以像这样从chain中使用itertools<br>from itertools import chain<br>newlist = list(chain(*newlist))<br>#或者您可以使用chain.from_iterable，其中无需解压缩列表<br>from itertools import chain<br>newlist = list(chain.from_iterable(newlist)) #效率更高<br></code></pre></td></tr></table></figure><h3 id="生成requirements-txt"><a href="#生成requirements-txt" class="headerlink" title="生成requirements.txt"></a>生成requirements.txt</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs language">pipreqs ./ --encoding=utf-8 --force<br></code></pre></td></tr></table></figure><h3 id="单例"><a href="#单例" class="headerlink" title="单例"></a>单例</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs language">class Singleton(object):<br>    __instance = None<br><br>    def __new__(cls, age, name):<br>        if not cls.__instance:<br>            cls.__instance = object.__new__(cls)<br>        return cls.__instance<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
